{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Fair Representations, python implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as scipyopt\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Timing\n",
    "# http://pythoncentral.io/time-a-python-function/\n",
    "\n",
    "import timeit\n",
    "# timeit.timeit('\"-\".join(str(n) for n in range(100))', number=10000)\n",
    "# timeit.Timer('for i in range(10): oct(i)', 'gc.enable()').timeit() # with gc\n",
    "\n",
    "def wrapper(func, *args, **kwargs):\n",
    "    def wrapped():\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapped\n",
    "\n",
    "# >>> def costly_func(lst): \n",
    "# ...     return map(lambda x: x^2, lst) \n",
    "# ... \n",
    "# >>> short_list = range(10) \n",
    "# >>> wrapped = wrapper(costly_func, short_list)\n",
    "# >>> timeit.timeit(wrapped, number=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tst(a, b):\n",
    "    return a + b\n",
    "\n",
    "wrp_tst = functools.partial(tst, 1)\n",
    "wrp_tst(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the \"Adult\" dataset\n",
    "The Adult dataset is from [here](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/).\n",
    "In categorical data, missing data is handled as just another category. This data set does not contain NA-values in the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
      "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
      "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
      "       'classification'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "filename_orig_data = \"data/adult.data\"\n",
    "filename_orig_test = \"data/adult.test\"\n",
    "# NOTE: if the data set is very large, we should save the preprocessed data to avoid preprocessing cost.\n",
    "#filename_data = \"data/processed.adult.data\"\n",
    "#filename_test = \"data/processed.adult.test\"\n",
    "\n",
    "# missing_data_marker = \"?\" # can be a string\n",
    "\n",
    "# df_orig_data = pd.read_csv(filename_orig_data, sep=',',header=None)\n",
    "# df_orig_data = pd.read_csv(filename_orig_data, sep=',', na_values=['?'])\n",
    "df_orig_data = pd.read_csv(filename_orig_data, sep=',', na_values=['?'], skipinitialspace=True)\n",
    "df_orig_test = pd.read_csv(filename_orig_test, sep=',', na_values=['?'], skipinitialspace=True)\n",
    "# print(df_orig_data)\n",
    "print(df_orig_data.keys())\n",
    "\n",
    "# df_orig_data = readtable(filename_orig_data);\n",
    "# df_orig_test = readtable(filename_orig_test);\n",
    "\n",
    "# Not in use, let NA values go through as \"?\", handled as yet another category\n",
    "#df_orig_test = readtable(filename_orig_test, nastrings = [\"\", \"NA\", \"?\"]);\n",
    "\n",
    "# df_orig_test[ 1:3, :age ] # Example: selecting subset of rows (1 to 3), from a certain column\n",
    "\n",
    "# names(df_orig_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for our model\n",
    "Set $K$ to the number of prototypes. The paper does not discuss how this should be chosen. Here we use the dimension of the original data, when sensitive and classification features are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Columns to encode with OneHot encoding aka Dummy variables\n",
    "# Remember to not encode sensitive_column_name and classification_column_name\n",
    "# Has to be an array\n",
    "columns_to_encode = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country']\n",
    "\n",
    "# Sensitive column is \"gender\" as in the paper\n",
    "# NOTE: This column must be have only two possible values (be a binary variable)\n",
    "#       as the method in the paper assumes this (\"protected or not protected\").\n",
    "sensitive_column_name = 'sex'\n",
    "\n",
    "# NOTE: This column must be have only two possible values (be a binary variable)\n",
    "#       as the method in the paper assumes this (binary classification).\n",
    "classification_column_name = 'classification'\n",
    "\n",
    "# Size or data matrix. -1 for classification/target column and -1 for sensitive column\n",
    "K = df_orig_data.shape[1] - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OneHot encoding (dummy variables) for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data  (32561, 15) (16281, 15) (48842, 15) \n",
      "\n",
      "Encoded data  (32561, 105) (16280, 105) (48842, 105) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vertically concatenate to get the whole dataset\n",
    "df_orig_all = pd.concat([df_orig_data, df_orig_test])\n",
    "print(\"Original data \", df_orig_data.shape, df_orig_test.shape, df_orig_all.shape, \"\\n\")\n",
    "# Do the One-Hot-Encoding / Dummy variables conversion.\n",
    "# Missing values in categorical values are encoded simply as yet another category.\n",
    "# drop_first drops first column resulting from one-hot-encoding, so that the variables are not linearly dependent \n",
    "# df_all = pd.get_dummies(df_orig_all, prefix=columns_to_encode, drop_first=True, columns=columns_to_encode)\n",
    "df_all = pd.get_dummies(df_orig_all, prefix=columns_to_encode, columns=columns_to_encode)\n",
    "\n",
    "### Map sensitive and classification/target columns to appropriate types.\n",
    "\n",
    "# Map Sensitive column \"Male\"/\"Female\" to true/false.\n",
    "# Change this if you change the sensitive column\n",
    "# (maybe do something that automatically just picks one category to be true and other to be false)\n",
    "df_all[sensitive_column_name] = df_all[sensitive_column_name].map(lambda gender: gender == 'Female')\n",
    "# Map classification column values to 0 and 1\n",
    "df_all[classification_column_name] = df_all[classification_column_name].map(lambda label: 1 if label == '>50K' else 0)\n",
    "\n",
    "# Read out the converted data back to data and test sets.\n",
    "len_data = df_orig_data.shape[0]\n",
    "len_test = df_orig_test.shape[0]\n",
    "df_data = df_all[0:len_data]\n",
    "df_test = df_all[len_data+1:len_data+len_test]\n",
    "print(\"Encoded data \", df_data.shape, df_test.shape, df_all.shape, \"\\n\")\n",
    "# Julia:\n",
    "# Original data (32561,15)(16281,15)(48842,15)\n",
    "# Encoded data (32561,101)(16281,101)(48842,101)\n",
    "# Python: \n",
    "# Original data  (32561, 15) (16281, 15) (48842, 15)\n",
    "# Encoded data  (32560, 98) (16280, 98) (48842, 98)\n",
    "# TODO: investigate discrepency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Code for the model\n",
    "\n",
    "## Notation/Definitions in paper\n",
    "\n",
    "- $X$ denotes the entire data set of individuals. Each $x \\in X$ is a vector of length $D$ where each component of the vector describes some attribute of the person.\n",
    "- $S$ is a binary random variable representing whether or not a given individual is a member of the protected set; we assume the system has access to this attribute.\n",
    "- $X_0$ denotes the training set of individuals.\n",
    "- $X^+ \\subset X$, $X_0^+ \\subset X_0$ denotes the subset of individuals (from the whole set and the training set respectively) that are members of the protected set (i.e., $S = 1$), and $X^−$ and $X_0^−$ denotes the subsets that are not members of the protected set, i.e., $S = 0$.\n",
    "- $Z$ is a multinomial random variable, where each of the $K$ values represents one of the intermediate set of ”prototypes”. Associated with each prototype is a vector $\\mathbf{v}_k$ in the same space as the individuals $\\mathbf{x}$.\n",
    "- $Y$ is the binary random variable representing the classification decision for an individual, and $f : X \\rightarrow Y$ is the desired classification function.\n",
    "- $d$ is a distance measure on $X$, e.g., simple Euclidean distance: $d(\\mathbf{x}_n , \\mathbf{v}_k ) = \\Vert\\mathbf{x}_n − \\mathbf{v}_k \\Vert_2$.\n",
    "\n",
    "## Our changes and clarifications\n",
    "\n",
    "We will differ a bit from the definitions in the paper. The definitions we use are:\n",
    "- $\\mathbf{X}$ denotes the entire data set, a $(N \\times D)$ matrix. The rows of the matrix are the feature vectors $\\mathbf{x}_n$ representing attributes of an individual. $\\mathbf{X}$ contains neither the classification information (target) column, nor the sensitive column.\n",
    "- $S$ is a binary variable representing whether or not a given individual is a member of the \"protected group\". For the user of the algorithm, this is a decision that is done before running the algorithm by setting `sensitive_column_name` in the parameters.\n",
    "- $\\mathbf{X}_{train}$ denotes the training set.\n",
    "- $\\mathbf{X}_{test}$ denotes the test set.\n",
    "- $\\mathbf{X}^+$ denotes the subset of individuals that are members of the \"protected group\" i.e. individuals for whom $S=1$. Similarly $\\mathbf{X}^-$ denotes the subset of individuals for whom $S=0$. It's worthwhile to note that for the algorithm it actually doesn't matter if you flip the groups of who is \"protected\" and who is \"non-protected\", the result will be the same due to symmetry of statistical parity. So don't get too attached to the terminology.\n",
    "- Define $\\mathbf{X}_{train}^+$, $\\mathbf{X}_{train}^-$, $\\mathbf{X}_{test}^+$ and $\\mathbf{X}_{test}^-$ similarly as above.\n",
    "- $d$ is a distance measure on $\\mathbf{X}$ (e.g. euclidean distance).\n",
    "- $K$ is the number of prototypes.\n",
    "- $Z$ is a random integer from the set $\\left\\{1,\\dots,K\\right\\}$.\n",
    "- $Y$ is a binary variable representing the classification decision (we consider binary classification only).\n",
    "\n",
    "Let $Z$ be a random integer from the set $\\left\\{1,\\dots,K\\right\\}$. Now we can denote the probability that a datapoint $\\mathbf{x}$ maps to a particular prototype $k$ with $\\mathbb{P}(Z=k \\mid \\mathbf{x})$ i.e. given a datapoint $\\mathbf{x}$, the probability that $Z$, the index of the prototype for that data point, is $k$.\n",
    "\n",
    "## Definitions in code\n",
    "\n",
    "From the definitions above, we map some to code and also define additional stuff.\n",
    "\n",
    "Julia is Column-Majored, so our matrices will be altered accordingly.\n",
    "\n",
    "- $\\mathbf{X}$ is just `𝐗`, but $(D \\times N)$ instead of $(N \\times D)$.\n",
    "- $\\mathbf{X}_{train}$ is `𝐗train`, $\\mathbf{X}_{test}$ is `𝐗test`.\n",
    "- Grouped versions are `𝐗⁺train`, `𝐗⁻train`, `𝐗⁺test`, and `𝐗⁻test` respectively.\n",
    "- $S$ is defined as multiple vectors `S_<someset>`, each containing the sensitive column for `<someset>`, e.g. `S_𝐗`.\n",
    "- $d$ is defined as a lambda function `d` and plain function `de`.\n",
    "  - The functions implemented here have versions that default to euclidean distance, and versions that accept a user defined distance function.\n",
    "- $Z$ is replaced by $\\mathbf{Z}$, a matrix of probability vectors $\\mathbf{z}$.\n",
    "- The classification information is contained in `𝐲`, `𝐲train`, and `𝐲test`.\n",
    "\n",
    "Additionally:\n",
    "- Denote the tuple of prototypes $\\mathbf{V} = \\left(\\mathbf{v}_1,...,\\mathbf{v}_K\\right)$. Since a single prototype $\\mathbf{v}_k$ is a vector of length $D$, $\\mathbf{V}$ can be expressed as a ($D \\times K$) matrix. This is our optimization variable `𝐕`.\n",
    "- `A` contains the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Sensitive indices for training and test data\n",
    "S_Xtrain_df = df_data[sensitive_column_name]\n",
    "S_Xtest_df = df_test[sensitive_column_name]\n",
    "\n",
    "# Classification vectors for training and test data\n",
    "ytrain_df = df_data[classification_column_name]\n",
    "ytest_df = df_test[classification_column_name]\n",
    "\n",
    "# Drop sensitive and classification columns, set difference\n",
    "columns_to_drop = set([sensitive_column_name, classification_column_name])\n",
    "features_left = [x for x in df_data.keys() if x not in columns_to_drop]\n",
    "\n",
    "# Training and test sets\n",
    "Xtrain_df = df_data[features_left]\n",
    "Xtest_df = df_test[features_left]\n",
    "\n",
    "\n",
    "# Standardize the features to have mean 0 variance 1\n",
    "# But only standardize non-one-hot-encoded features\n",
    "\n",
    "# Features that are not one-hot-encoded\n",
    "non_encoded_features = [x for x in features_left if x.split('_')[0] not in columns_to_encode]\n",
    "# Find indices of those features\n",
    "# non_encoded_features = find(symbol -> in(symbol, non_encoded), names(df_data[idxs_left]))\n",
    "\n",
    "# Calculate mean and var from training set\n",
    "train_mean_df = Xtrain_df.mean()\n",
    "train_var_df = Xtrain_df.var()\n",
    "\n",
    "# Only standardize non-one-hot-encoded features\n",
    "for ne_feat in non_encoded_features:\n",
    "    Xtrain_df = Xtrain_df.assign(**{ne_feat: (Xtrain_df.loc[:, ne_feat] - train_mean_df[ne_feat]) / train_var_df[ne_feat]})\n",
    "    # We need to use the same variance and mean for our test set as in the training set,\n",
    "    # otherwise they would not be comparable.\n",
    "    Xtest_df = Xtest_df.assign(**{ne_feat: (Xtest_df.loc[:, ne_feat] - train_mean_df[ne_feat]) / train_var_df[ne_feat]})\n",
    "\n",
    "\n",
    "# Reconstruct full dataset\n",
    "X_df = pd.concat([Xtrain_df, Xtest_df])\n",
    "S_X_df = pd.concat([S_Xtrain_df, S_Xtest_df])\n",
    "y_df = pd.concat([ytrain_df, ytest_df])\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From DataFrames to matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "### Dimensions\n",
    "N = X_df.shape[0]\n",
    "D = X_df.shape[1]\n",
    "Ntrain = Xtrain_df.shape[0]\n",
    "Ntest = Xtest_df.shape[0]\n",
    "\n",
    "### Data matrices\n",
    "X         = X_df.values\n",
    "Xtrain    = Xtrain_df.values\n",
    "Xtest     = Xtest_df.values\n",
    "\n",
    "S_X       = S_X_df.values\n",
    "S_Xtrain  = S_Xtrain_df.values\n",
    "S_Xtest   = S_Xtest_df.values\n",
    "\n",
    "y         = y_df.values\n",
    "ytrain    = ytrain_df.values\n",
    "ytest     = ytest_df.values\n",
    "\n",
    "### Optimization variables\n",
    "# Main optimization variable, matrix holding the prototype vectors.\n",
    "# Initialized to random Float64 matrix, normal distribution 0 mean 1 variance\n",
    "V = np.random.randn(K, D)\n",
    "# Weights or \"prototype label predictions\" (probabilities)\n",
    "w = np.random.rand(K) # floats in [0,1)\n",
    "# Alphas\n",
    "alpha_pos = np.random.rand(D) # 𝛂⁺\n",
    "alpha_neg = np.random.rand(D) # 𝛂⁻\n",
    "\n",
    "### Hyperparameters\n",
    "A = dict = {'z': 1000, 'x': 0.0001, 'y': 0.1};\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " -1.3719338843612912,\n",
       " 1.0,\n",
       " -1.3719338843612912,\n",
       " 3.5096356112243225,\n",
       " -3.188600743274669)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that normalization got us values around 0\n",
    "Xtrain.max().max(), Xtrain.min().min(), Xtest.max().max(), Xtest.min().min(), V.max(), V.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (10771, 103) (21790, 103) Test: (5421, 103) (10859, 103)\n"
     ]
    }
   ],
   "source": [
    "Xtrain_pos = Xtrain[S_Xtrain]\n",
    "Xtrain_neg = Xtrain[S_Xtrain==False]\n",
    "Xtest_pos = Xtest[S_Xtest]\n",
    "Xtest_neg = Xtest[S_Xtest==False]\n",
    "Xtrain.shape, Xtrain_pos.shape, Xtrain_neg.shape, S_Xtrain.shape, Xtrain_pos.shape[0]+Xtrain_neg.shape[0]\n",
    "Xtest.shape, Xtest_pos.shape, Xtest_neg.shape, S_Xtest.shape, Xtest_pos.shape[0]+Xtest_neg.shape[0]\n",
    "print(\"Training:\", Xtrain_pos.shape, Xtrain_neg.shape, \"Test:\", Xtest_pos.shape, Xtest_neg.shape)\n",
    "# Julia: (\"Training:\",(99,10771),(99,21790),\"Test:\",(99,5421),(99,10860))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide training and test sets to groups according to whether the individuals are \"protected\" or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping $\\mathbf{X} \\rightarrow \\mathbf{Z}$\n",
    "Now we can define a mapping from the original dataset $\\mathbf{X}$ to probabilities via the *softmax function*. [Wikipedia](https://en.wikipedia.org/wiki/Softmax_function):\n",
    "> Softmax function \"squashes\" a $K$-dimensional vector $\\mathbf{z}$ of arbitrary real values to a $K$-dimensional vector $\\sigma(\\mathbf{z})$ of real values in the range $[0, 1]$ that add up to 1.\n",
    "\n",
    "Most notably `softmax` returns a probability vector. We will define a modified version that maps a $D$-dimensional vector $\\mathbf{x}$ to a $K$-dimensional vector $\\sigma(\\mathbf{x})$, i.e. the mapping won't necessarily preserve the dimensionality of $\\mathbf{x}$.\n",
    "\n",
    "Also from [Wikipedia](https://en.wikipedia.org/wiki/Multinomial_logistic_regression):\n",
    "> $$\\operatorname{softmax}(k,x_1,\\ldots,x_n) = \\frac{e^{x_k}}{\\sum_{i=1}^n e^{x_i}}$$\n",
    "> is referred to as the [*softmax function*](https://en.wikipedia.org/wiki/Softmax_function).  The reason is that the effect of exponentiating the values $x_1,\\ldots,x_n$ is to exaggerate the differences between them.  As a result, $\\operatorname{softmax}(k,x_1,\\ldots,x_n)$ will return a value close to 0 whenever $x_k$ is significantly less than the maximum of all the values, and will return a value close to 1 when applied to the maximum value, unless it is extremely close to the next-largest value.  Thus, the softmax function can be used to construct a weighted average that behaves as a smooth function (which can be conveniently differentiated, etc.) and which approximates the [indicator function](https://en.wikipedia.org/wiki/Indicator_function).\n",
    "\n",
    "So we define, as in the paper equation (2), $$\\mathbb{P}(Z=k \\mid \\mathbf{x}) = \\frac{e^{-d(\\mathbf{x}, \\mathbf{v}_k)}}{\\sum_{j=1}^K e^{-d(\\mathbf{x}, \\mathbf{v}_j)}}$$\n",
    "where\n",
    "- $\\mathbb{P}(Z=k \\mid \\mathbf{x})$ is described in [definitions](#Definitions)\n",
    "- $\\mathbf{x}$ is the datapoint\n",
    "- $\\mathbf{v}_k$ is a vector associated with the $k$th prototype\n",
    "- $d$ is a distance measure between $\\mathbf{x}$ and $\\mathbf{v}_k$ (e.g. the euclidean distance)\n",
    "\n",
    "This means that since we have replaced $x_k$ in the softmax with the negative distance between $\\mathbf{x}$ and prototype $\\mathbf{v}_k$, the softmax returns a value close to 0 whenever the distance from $\\mathbf{x}$ to the prototype $\\mathbf{v}_k$ is significantly higher than $\\min_{j\\in\\{1,\\dots,K\\}}\\, d(\\mathbf{x}, \\mathbf{v}_i)$, and close to 1 when applied to the minimum value.\n",
    "\n",
    "\"Mapping from X to Z\" in the paper means mapping the vector $\\mathbf{x}$ to a probability vector $\\mathbf{z}$ of length $K$ via the softmax function. These probability vectors are then used directly for training the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In code** this means that $\\mathbb{P}(Z=k \\mid \\mathbf{x})$ is represented by a function taking\n",
    "- the data point $\\mathbf{x}$ which is a `Vector` of length $D$\n",
    "- $(D \\times K)$ `Matrix` of prototypes $\\mathbf{V}$ containing all $K$ prototypes $\\mathbf{v}_k$ i.e. each prototype is a `Vector` (remember, Column-Major order on matrices)\n",
    "- a distance measure on $\\mathbf{X}$\n",
    "\n",
    "and returning\n",
    "- a `Vector` $\\mathbf{z}$ of length $K$ representing a [probability vector](https://en.wikipedia.org/wiki/Probability_vector), where each value $z_k$ of the probability vector $\\mathbf{z}$ tells how probable it is that $\\mathbf{x}$ maps to $\\mathbf{v}_k$. Since $\\mathbf{z}$ is a probability vector, $\\sum_{i=k}^K z_k = 1$.\n",
    "\n",
    "We will name this function `softmax` and define it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13, 103), (48841, 103))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape, X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances are calculated with Eq (12):\n",
    "$$d(\\mathbf{x}_n, \\mathbf{v}_k, \\alpha) = \\sum_{d=1}^D \\mathbf{\\alpha}_d (\\mathbf{x}_{nd}-\\mathbf{v}_{kd})^2$$\n",
    "So the distance matrix is defined by `distances[n, k] = sum(alphas * (X[n,:] - V[k,:])^2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.118001469400888"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(alpha_pos * np.power(X[1, :] - V[1, :], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate distance matrix\n",
    "@jit(nopython=True)\n",
    "def distances(X, V, alpha):\n",
    "    # X is a (N x D) matrix of datapoints\n",
    "    # V is a (K x D) matrix of prototypes\n",
    "    # alpha is a (1 x D) vector of weights\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    K = V.shape[0]\n",
    "    distances = np.zeros((N, K))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            distances[n, k] = np.sum(alpha * np.power(X[n, :] - V[k, :], 2))\n",
    "    \n",
    "    return distances\n",
    "\n",
    "\n",
    "import concurrent.futures\n",
    "import functools\n",
    "# Calculate distance matrix\n",
    "@jit(nopython=True, nogil=True)\n",
    "def distances_threaded_job(distances, X, V, alpha, N, k):\n",
    "    for n in range(N):\n",
    "        distances[n, k] = np.sum(alpha * np.power(X[n, :] - V[k, :], 2))\n",
    "\n",
    "def distances_threaded(X, V, alpha):\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    K = V.shape[0]\n",
    "    distances = np.zeros((N, K))\n",
    "    fun = functools.partial(distances_threaded_job, distances, X, V, alpha, N)\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = executor.map(fun, (k for k in range(K)))\n",
    "    return distances\n",
    "\n",
    "\n",
    "def distances_threaded_old(X, V, alpha=None):\n",
    "    # X is a (N x D) matrix of datapoints\n",
    "    # V is a (K x D) matrix of prototypes\n",
    "    # alpha is a (1 x D) vector of weights\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    K = V.shape[0]\n",
    "    distances = np.empty((N, K))\n",
    "    \n",
    "    def inside_job(k):\n",
    "        for n in range(N):\n",
    "            distances[n, k] = np.sum(alpha * np.power(X[n, :] - V[k, :], 2))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        fs = {executor.submit(inside_job, k): k for k in range(K)}\n",
    "        concurrent.futures.wait(fs)\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # distances(Xtrain, V, alpha_pos)\n",
    "# wrapped_distances = wrapper(distances, Xtrain, V, alpha_pos)\n",
    "# timeit.timeit(wrapped_distances, number=5)\n",
    "# # timeit.Timer(wrapped_distances, 'gc.enable()').timeit(5) # with gc\n",
    "# # 21.99340252300317, 21.508173273992725\n",
    "# # numba jit: 7.959764497005381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wrapped_distances_threaded = wrapper(distances_threaded, Xtrain, V, alpha_pos)\n",
    "# timeit.timeit(wrapped_distances_threaded, number=10)\n",
    "# # 3.146800766000524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dists = distances_threaded(Xtrain, V, alpha_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: For avoiding numerical infinity problems in softmax, see John D. Cook's advice\n",
    "# http://www.johndcook.com/blog/2010/01/20/how-to-compute-the-soft-maximum/\n",
    "# http://cs231n.github.io/linear-classify/#softmax\n",
    "\n",
    "# Returns (N x K) matrix, a probability vector of length K for each datapoint\n",
    "\n",
    "@jit(nopython=True,nogil=True)\n",
    "def softmax_jit_loop(result, distances, N, K):\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            # Negation of distance was done already\n",
    "            result[n,k] = np.exp(distances[n,k])\n",
    "        # stability\n",
    "        result[n,:] -= np.max(result[n,:])\n",
    "        # result\n",
    "        result[n,:] = result[n,:] * (1/np.sum(result[n,:]))\n",
    "\n",
    "@jit(nopython=True,nogil=True)\n",
    "def softmax_jit_loop_stable(result, distances, N):\n",
    "    for n in range(N):\n",
    "        # minus np.max for numerical stability\n",
    "        tmp = np.exp(distances[n,:] - np.max(distances[n,:]))\n",
    "        # Negation of distance was done already\n",
    "        result[n,:] = tmp / np.sum(tmp)\n",
    "        \n",
    "def softmax(X, V, alpha):\n",
    "    # Calculate the distances, negate already\n",
    "    distances = -distances_threaded(X, V, alpha)\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    K = V.shape[0]\n",
    "    \n",
    "    result = np.zeros((N, K))\n",
    "#     softmax_jit_loop(result, distances, N, K)\n",
    "    softmax_jit_loop_stable(result, distances, N)\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "@jit(nopython=True,nogil=True)\n",
    "def softmax_threaded_job_fixed(result, distances, N, k):\n",
    "    for n in range(N):\n",
    "        # Negation of distance was done already\n",
    "        result[n,k] = np.exp(distances[n,k] - np.max(distances[n,:]))\n",
    "\n",
    "@jit(nopython=True)\n",
    "def sfix(result, N):\n",
    "    for n in range(N):\n",
    "        total = np.sum(result[n,:])\n",
    "        if total:\n",
    "            div = total\n",
    "        else:\n",
    "            div = 1e-9\n",
    "        result[n,:] = result[n,:] / div\n",
    "\n",
    "def softmax_threaded_fixed(X, V, alpha):\n",
    "    # Calculate the distances, negate already\n",
    "    distances = -distances_threaded(X, V, alpha)\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    K = V.shape[0]\n",
    "    result = np.empty((N, K))\n",
    "    \n",
    "    fun = functools.partial(softmax_threaded_job_fixed, result, distances, N)\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = executor.map(fun, (k for k in range(K)))\n",
    "    \n",
    "    sfix(result, N)\n",
    "    return result\n",
    "\n",
    "# for n in range(N):\n",
    "#     for k in range(K):\n",
    "#         # Negation of distance was done already\n",
    "#         result[n,k] = np.exp(distances[n,k])\n",
    "#     result[n,:] = result[n,:] * (1/np.sum(result[n,:]))\n",
    "\n",
    "# @jit(nopython=True,nogil=True)\n",
    "# def softmax_threaded_joined_job(result, X, V, alpha, N, K):\n",
    "#     for n in range(N):\n",
    "#         for k in range(K):\n",
    "#             # Negation of distance was done already\n",
    "#             dm = np.sum(alpha * np.power(X[n, :] - V[k, :], 2))\n",
    "#             result[n,k] = np.exp(-dm)\n",
    "#         result[n,:] = result[n,:] * (1/np.sum(result[n,:]))\n",
    "\n",
    "@jit(nopython=True,nogil=True)\n",
    "def softmax_threaded_joined_job(result, X, V, alpha, N, k):\n",
    "    for n in range(N):\n",
    "        # Negation of distance was done already\n",
    "        dm = -np.sum(alpha * np.power(X[n, :] - V[k, :], 2))\n",
    "        result[n,k] = np.exp(dm)\n",
    "        # minus np.max for numerical stability\n",
    "        tmp = np.exp(distances[n,:] - np.max(distances[n,:]))\n",
    "\n",
    "def softmax_threaded_joined(X, V, alpha):    \n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    K = V.shape[0]\n",
    "    result = np.empty((N, K))\n",
    "    \n",
    "    fun = functools.partial(softmax_threaded_joined_job, result, X, V, alpha, N)\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = executor.map(fun, (k for k in range(K)))\n",
    "    \n",
    "    sfix(result, N)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32561.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo1=softmax(Xtrain, V, alpha_pos)\n",
    "np.sum(foo1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32561.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo2=softmax_threaded_fixed(Xtrain, V, alpha_pos)\n",
    "np.sum(foo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32561.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo3=softmax_threaded_joined(Xtrain, V, alpha_pos)\n",
    "np.sum(foo3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32561, 13), (32561, 13), (32561, 13))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo1.shape,foo2.shape,foo3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561.0, 32561.0, 32561.0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(foo1),np.sum(foo2),np.sum(foo3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify\n",
    "foo1.shape[0] == round(np.sum(foo1)), foo2.shape[0] == round(np.sum(foo2)), foo3.shape[0] == round(np.sum(foo3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.07579768000869"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_softmax = wrapper(softmax, Xtrain, V, alpha_pos)\n",
    "timeit.timeit(wrapped_softmax, number=50)\n",
    "# 14.974193882997497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.313272532992414"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_softmax_threaded_fixed = wrapper(softmax_threaded_fixed, Xtrain, V, alpha_pos)\n",
    "timeit.timeit(wrapped_softmax_threaded_fixed, number=50)\n",
    "# 15=5.419921508990228\n",
    "# 50=18.265479806999792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.90795697500289"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_softmax_threaded_joined = wrapper(softmax_threaded_joined, Xtrain, V, alpha_pos)\n",
    "timeit.timeit(wrapped_softmax_threaded_joined, number=50)\n",
    "# 15=5.419921508990228\n",
    "# 50=18.265479806999792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(foo1-foo3),np.min(foo1-foo3),np.max(foo1-foo2),np.min(foo1-foo2),np.max(foo2-foo3),np.min(foo2-foo3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(foo1-foo3),np.sum(foo1-foo2),np.sum(foo2-foo3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.6691817583990904e-11, 2.6691817583990904e-11, 2.6691817583990904e-11)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo1[5,5],foo2[5,5],foo3[5,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-129-940f30bfb79d>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-129-940f30bfb79d>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    if (X.shape[1] == 1) return softmax_vec(X, V, distanceFun, alphas)\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TODO: clean up duplicate code\n",
    "\n",
    "### FOR VECTORS; give in 𝐱, get 𝐳\n",
    "\n",
    "# This is same as Eq (2) in paper.\n",
    "function softmax_dist{T<:Number}(𝐱::Vector{T}, 𝐕::Matrix{T}, distanceMeasure::Function)\n",
    "    K = size(𝐕, 2)\n",
    "    res = Vector{Float64}(K)\n",
    "    denominator = Float64(0.0)\n",
    "    # Use one loop to calculate both numerator and denominator\n",
    "    @inbounds for k in 1:K\n",
    "        res[k] = exp(- distanceMeasure(𝐱, 𝐕[:,k]) )\n",
    "        denominator += res[k]\n",
    "    end\n",
    "    denom = inv(denominator)\n",
    "    res .* denom\n",
    "end\n",
    "\n",
    "function softmax_euclidean{T<:Number}(𝐱::Vector{T}, 𝐕::Matrix{T})\n",
    "    K = size(𝐕, 2)\n",
    "    res = Vector{Float64}(K)\n",
    "    denominator = Float64(0.0)\n",
    "    # Use one loop to calculate both numerator and denominator\n",
    "    @inbounds for k in 1:K\n",
    "        res[k] = exp(- vecnorm(𝐱 - 𝐕[:,k]) )\n",
    "        denominator += res[k]\n",
    "    end\n",
    "    denom = inv(denominator)\n",
    "    res .* denom\n",
    "end\n",
    "\n",
    "### FOR MATRICES; give in 𝐗, get 𝐙\n",
    "\n",
    "function softmax_dist{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T}, distanceMeasure::Function)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = Matrix{Float64}(K, N)\n",
    "    @fastmath @inbounds for n in 1:N\n",
    "        res[:,n] = softmax_dist(𝐗[:,n], 𝐕, distanceMeasure)\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "# Parallel version\n",
    "function softmax_dist_par{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T}, distanceMeasure::Function)\n",
    "    #nprocs()==CPU_CORES || addprocs(CPU_CORES-1)    \n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = SharedArray(Float64, (K, N))\n",
    "    @fastmath @sync @parallel for n in 1:N\n",
    "        for k in 1:K\n",
    "            res[k,n] = exp(- distanceMeasure(𝐗[:,n], 𝐕[:,k]) )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "function softmax_euclidean{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = Matrix{Float64}(K, N)\n",
    "    @fastmath @inbounds for n in 1:N\n",
    "        denominator = Float64(0.0)\n",
    "        # Use one loop to calculate both numerator and denominator\n",
    "        for k in 1:K\n",
    "            res[k,n] = exp(- vecnorm(𝐗[:,n] - 𝐕[:,k]) )\n",
    "            denominator += res[k,n]\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(denominator)\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "# Parallel version\n",
    "#function softmax_euclidean_par{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "function softmax_euclidean_par(𝐗::Matrix{Float64}, 𝐕::Matrix{Float64})\n",
    "    #nprocs()==CPU_CORES || addprocs(CPU_CORES-1)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = SharedArray(Float64, (K, N))\n",
    "    @fastmath @sync @parallel for n in 1:N\n",
    "        for k in 1:K\n",
    "            res[k,n] = exp(- vecnorm(𝐗[:,n] - 𝐕[:,k]) )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "@everywhere function softmax_alpha(𝐗::SharedArray{Float64,2}, 𝐕::SharedArray{Float64,2}, 𝛂::SharedArray{Float64,1})\n",
    "    return softmax_alpha(sdata(𝐗), sdata(𝐕), sdata(𝛂))\n",
    "end\n",
    "\n",
    "# Version that accepts alphas, uses the distance function defined in the paper as Eq (12)\n",
    "@everywhere function softmax_alpha(𝐗::Matrix{Float64}, 𝐕::Matrix{Float64}, 𝛂::Vector{Float64})\n",
    "    localD = length(𝛂)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = Matrix{Float64}(K, N)\n",
    "    @fastmath @inbounds @simd for n in 1:N\n",
    "        for k in 1:K\n",
    "            dm::Float64 = zero(Float64)\n",
    "            for i in 1:localD # Eq (12)\n",
    "                dm += 𝛂[i]*((𝐗[i,n]-𝐕[i,k])^2)\n",
    "            end\n",
    "            res[k,n] = exp(- dm )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "    # Slower alternatives for distance calculation, replacing the 1:localD loop:\n",
    "    # return 𝛂 ⋅ ((𝐚-𝐛).^2) # Eq (12)\n",
    "    # return sum(i -> 𝛂[i]*(𝐚[i]-𝐛[i])^2, 1:localD) # Eq (12)\n",
    "end\n",
    "\n",
    "@everywhere function softmax_alpha_s(𝐗::SharedArray{Float64,2}, 𝐕::SharedArray{Float64,2}, 𝛂::SharedArray{Float64,1})\n",
    "    return softmax_alpha_s(sdata(𝐗), sdata(𝐕), sdata(𝛂))\n",
    "end\n",
    "\n",
    "# Version that accepts alphas, uses the distance function defined in the paper as Eq (12)\n",
    "@everywhere function softmax_alpha_s(𝐗::Matrix{Float64}, 𝐕::Matrix{Float64}, 𝛂::Vector{Float64})\n",
    "    localD = length(𝛂)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix as SharedArray, no need to zero it\n",
    "    res = SharedArray(Float64, (K, N))\n",
    "    @fastmath @inbounds @simd for n in 1:N\n",
    "        for k in 1:K\n",
    "            dm::Float64 = zero(Float64)\n",
    "            for i in 1:localD # Eq (12)\n",
    "                dm += 𝛂[i]*((𝐗[i,n]-𝐕[i,k])^2)\n",
    "            end\n",
    "            res[k,n] = exp(- dm )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "    # Slower alternatives for distance calculation, replacing the 1:localD loop:\n",
    "    # return 𝛂 ⋅ ((𝐚-𝐛).^2) # Eq (12)\n",
    "    # return sum(i -> 𝛂[i]*(𝐚[i]-𝐛[i])^2, 1:localD) # Eq (12)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    }
   ],
   "source": [
    "@time for i in 1:25\n",
    "    softmax_alpha(𝐗train, 𝐕, 𝛂⁺);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For development\n",
    "Zpre = softmax(Xtrain, V)\n",
    "Zpre_pos = softmax_euclidean_par(Xtrain_pos, V)\n",
    "Zpre_neg = softmax_euclidean_par(Xtrain_neg, V);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.342045 seconds (8.08 M allocations: 576.790 MB, 11.21% gc time)\n"
     ]
    }
   ],
   "source": [
    "# For development\n",
    "𝐙pre = softmax_euclidean_par(𝐗train, 𝐕)\n",
    "𝐙pre⁺ = softmax_euclidean_par(𝐗⁺train, 𝐕)\n",
    "𝐙pre⁻ = softmax_euclidean_par(𝐗⁻train, 𝐕);\n",
    "\n",
    "𝐙preu = sdata(𝐙pre)\n",
    "𝐙preu⁺ = sdata(𝐙pre⁺)\n",
    "𝐙preu⁻ = sdata(𝐙pre⁻);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is akin to using a funky [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) to \"predict the prototype\" (category) where a data point $\\mathbf{x}$ maps to.\n",
    "Wikipedia:\n",
    "> These are all statistical classification problems. They all have in common a dependent variable to be predicted that comes from one of a limited set of items which cannot be meaningfully ordered, as well as a set of independent variables (also known as features, explanators, etc.), which are used to predict the dependent variable. Multinomial logit regression is a particular solution to the classification problem that assumes that a linear combination of the observed features and some problem-specific parameters can be used to determine the probability of each particular outcome of the dependent variable. The best values of the parameters for a given problem are usually determined from some training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of the optimization objective\n",
    "\n",
    "### $L_z$ &mdash; statistical parity\n",
    "In code, we will denote $L_z$ with function `Lz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### \"NAIVE\" VERSION FOR UNDERSTANDING THE IMPLEMENTATION\n",
    "\n",
    "function LzNaive{T<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T}, dist::Function)\n",
    "    # Operate on matrices and take mean from sample dimension N\n",
    "    meanp = mean( softmax_dist_par(𝐗⁺, 𝐕, dist), 2 ) # Eq (6)\n",
    "    meann = mean( softmax_dist_par(𝐗⁻, 𝐕, dist), 2 ) # Similarly for M_k^-\n",
    "    sum(abs(meanp - meann)) # Eq (7), sum is from k=1 to K\n",
    "end\n",
    "\n",
    "### VERSIONS TAKING IN THE DATA SET AND PROTOTYPES\n",
    "# Optionally a distance measure function can be passed as an argument\n",
    "\n",
    "function Lz{T<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T})\n",
    "    return Lz(sdata(softmax_euclidean_par(𝐗⁺, 𝐕)), sdata(softmax_euclidean_par(𝐗⁻, 𝐕)))\n",
    "end\n",
    "\n",
    "function Lz{T<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T}, dist::Function)\n",
    "    return Lz(sdata(softmax_dist_par(𝐗⁺, 𝐕, dist)), sdata(softmax_dist_par(𝐗⁻, 𝐕, dist)))\n",
    "end\n",
    "\n",
    "# # TODO: use parallel version of softmax_dist_alpha?\n",
    "# function Lz{T<:Number,U<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T}, dist::Function, 𝛂::Vector{U})\n",
    "#     return Lz(softmax_dist_alpha(𝐗⁺, 𝐕, dist, 𝛂), softmax_dist_alpha(𝐗⁻, 𝐕, dist, 𝛂))\n",
    "# end\n",
    "\n",
    "### VERSION FOR PRECALCULATED 𝐙⁺ and 𝐙⁻\n",
    "# Note we have only a version for matrices. This is because during performance\n",
    "# testing I noticed that\n",
    "#\n",
    "#   ZZZp = sdata(𝐙shared⁺)\n",
    "#   ZZZn = sdata(𝐙shared⁻)\n",
    "#   Lz(ZZZp, ZZZn)\n",
    "#\n",
    "# is faster than\n",
    "#\n",
    "#   Lz(𝐙shared⁺, 𝐙shared⁻)\n",
    "#\n",
    "# So use the Matrix version always and if necessary lift the matrices out\n",
    "# of the SharedArray with sdata().\n",
    "#\n",
    "# TODO: is there way to make a faster parallel version?\n",
    "#function Lz{T<:Number}(𝐙⁺::SharedArray{T,2}, 𝐙⁻::SharedArray{T,2})\n",
    "@everywhere function Lz(𝐙⁺::SharedArray{Float64,2}, 𝐙⁻::SharedArray{Float64,2})\n",
    "    Lz(sdata(𝐙⁺), sdata(𝐙⁻))\n",
    "end\n",
    "\n",
    "#function Lz{T<:Number}(𝐙⁺::Matrix{T}, 𝐙⁻::Matrix{T})\n",
    "@everywhere function Lz(𝐙⁺::Matrix{Float64}, 𝐙⁻::Matrix{Float64})\n",
    "    # Operate on matrices and take mean from sample dimension N\n",
    "    meanp = mean( 𝐙⁺, 2 ) # Eq (6)\n",
    "    meann = mean( 𝐙⁻, 2 ) # Similarly for M_k^-\n",
    "    sum(abs(meanp - meann)) # Eq (7), sum is from k=1 to K\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05031480867026994,0.051942562238902076,0.051942562238902076)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test (e.g. somewhere between 0.02 and 0.07, depending on 𝐕 randomization)\n",
    "LZtrain1 = Lz(𝐗⁺test, 𝐗⁻test, 𝐕)\n",
    "LZtrain2 = Lz(𝐙pre⁺, 𝐙pre⁻)\n",
    "LZtrain3 = Lz(𝐙preu⁺, 𝐙preu⁻)\n",
    "(LZtrain1, LZtrain2, LZtrain3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    }
   ],
   "source": [
    "@time for i in 1:5000\n",
    "    Lz(𝐙pre⁺, 𝐙pre⁻)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_x$ &mdash; information loss\n",
    "In code, we will denote $L_x$ with function `Lx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Symbols: 𝐗 ⁺ ⁻ ∑ 𝐕 𝐱 𝐲 𝐙 𝐳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note .* elementwise multiplication of softmax_dist() and V, there is no \\cdot in the paper in Eq (9), dot product would return a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### NAIVE VERSION FOR UNDERSTANDING THE IMPLEMENTATION\n",
    "\n",
    "function LxNaive{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{U}, dist::Function)\n",
    "    D = size(𝐗, 1)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    𝐗hat = zeros(Float64, (D,N))\n",
    "    sum = Float64(0.0)\n",
    "    for n in 1:N\n",
    "        𝐳_n = softmax_dist(𝐗[:,n], 𝐕, dist) # prob that x_n maps to protos (Array{13,1})\n",
    "        for k in 1:K # Eq (9)\n",
    "            𝐗hat[:,n] = 𝐗hat[:,n] + (𝐳_n[k] * 𝐕[:,k])\n",
    "        end\n",
    "        sum += (𝐗[:,n] - 𝐗hat[:,n]) ⋅ (𝐗[:,n] - 𝐗hat[:,n]) # Eq (8)\n",
    "    end\n",
    "    return sum, 𝐗hat\n",
    "end\n",
    "\n",
    "### VERSIONS TAKING IN THE DATA SET AND PROTOTYPES\n",
    "# Optionally a distance measure function can be passed as an argument\n",
    "\n",
    "function Lx{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{U})\n",
    "    return Lx(softmax_euclidean_par(𝐗, 𝐕), 𝐗, 𝐕)\n",
    "end\n",
    "\n",
    "function Lx{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{U}, dist::Function)\n",
    "    return Lx(softmax_dist_par(𝐗, 𝐕, dist), 𝐗, 𝐕)\n",
    "end\n",
    "\n",
    "### VERSION FOR PRECALCULATED 𝐙\n",
    "\n",
    "#function Lx{T<:Number}(𝐙::SharedArray{T,2}, 𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "@everywhere function Lx(𝐙::SharedArray{Float64,2}, 𝐗::Matrix{Float64}, 𝐕::Matrix{Float64})\n",
    "    Lx(sdata(𝐙), 𝐗, 𝐕)\n",
    "end\n",
    "\n",
    "#function Lx{T<:Number}(𝐙::Matrix{T}, 𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "@everywhere function Lx(𝐙::Matrix{Float64}, 𝐗::Matrix{Float64}, 𝐕::Matrix{Float64})\n",
    "    𝐗minus𝐗hat = 𝐗 - (𝐕 * 𝐙) # Eq (8) and (9) combined\n",
    "    return vecdot(𝐗minus𝐗hat, 𝐗minus𝐗hat) # \"simple squared error\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.298525 seconds (110.12 k allocations: 7.714 MB, 0.32% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(438406.07313762454,438406.07313762454,438406.07313762454)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test (e.g. 423475.8479283692)\n",
    "LXtrain1 = Lx(𝐗train, 𝐕, d)\n",
    "LXtrain2 = Lx(𝐙pre, 𝐗train, 𝐕)\n",
    "LXtrain3 = Lx(𝐙preu, 𝐗train, 𝐕)\n",
    "(LXtrain1, LXtrain2, LXtrain3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    }
   ],
   "source": [
    "@time for i in 1:50\n",
    "    Lx(𝐙pre, 𝐗train, 𝐕)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_y$ &mdash; prediction accuracy\n",
    "In code, we will denote $L_y$ with function `Ly`.\n",
    "\n",
    "Essentially here we are letting the optimization pick both the prototypes (i.e. feature vectors) and their predictions (i.e. labels), and the predictions don't have to be discrete 0 and 1, but can be from the range $[0,1]$ and thus themselves can be viewed as probabilities. E.g. let's say that for prototype $v_k$ it's prediction $w_k = 0.82$, then \"there is a 82% chance prototype $\\mathbf{v}_k$ gets label 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### NAIVE VERSION TO HELP UNDERSTAND THE IMPLEMENTATION\n",
    "\n",
    "function LyNaive{T1<:Number,T2<:Number,T3<:Number}(\n",
    "        𝐗::Matrix{T1}, 𝐕::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}, dist::Function\n",
    "    )\n",
    "    D = size(𝐗, 1)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    𝐲hat = zeros(Float64, N)\n",
    "    sum = Float64(0.0)\n",
    "    # Replace 𝐲hat in Eq (10) with Eq (11), then you get this for loop\n",
    "    for n in 1:N\n",
    "        𝐙_n = softmax_dist(𝐗[:,n], 𝐕, dist) # Vector of length K\n",
    "        for k in 1:K\n",
    "            𝐲hat[n] = 𝐲hat[n] + (𝐙_n[k] * 𝐰[k])\n",
    "        end\n",
    "        # The following line could be replaced with\n",
    "        # if 𝐲[n] == 1\n",
    "        #    sum -= log(𝐲hat[n])\n",
    "        # else # 𝐲[n] == 0\n",
    "        #    sum -= log(1 - 𝐲hat[n])\n",
    "        # end\n",
    "        sum += -𝐲[n] * log(𝐲hat[n])  -  (1 - 𝐲[n]) * log(1 - 𝐲hat[n])\n",
    "    end\n",
    "    #return sum, 𝐲hat\n",
    "    return sum\n",
    "end\n",
    "\n",
    "### VERSIONS TAKING IN THE DATA SET AND PROTOTYPES\n",
    "# Optionally a distance measure function can be passed as an argument\n",
    "\n",
    "function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "        𝐗::Matrix{T1}, 𝐕::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}\n",
    "    )\n",
    "    # 𝐙 = softmax_euclidean_par(𝐗, 𝐕)\n",
    "    return Ly(softmax_euclidean_par(𝐗, 𝐕), 𝐲, 𝐰)\n",
    "end\n",
    "\n",
    "function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "        𝐗::Matrix{T1}, 𝐕::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}, dist::Function\n",
    "    )\n",
    "    # 𝐙 = softmax_dist_par(𝐗, 𝐕, dist)\n",
    "    return Ly(softmax_dist_par(𝐗, 𝐕, dist), 𝐲, 𝐰)\n",
    "end\n",
    "\n",
    "### VERSION FOR PRECALCULATED 𝐙\n",
    "\n",
    "# function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "#         𝐙::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}\n",
    "#     )\n",
    "#     # Copy to shared memory\n",
    "#     𝐙shared = convert(SharedArray{T1, 2}, 𝐙)\n",
    "#     return Ly(𝐙shared, 𝐲, 𝐰)\n",
    "# end\n",
    "\n",
    "# Slower:\n",
    "# function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "#         𝐙::SharedArray{T1,2}, 𝐲::Vector{T2}, 𝐰::Vector{T3}\n",
    "#     )\n",
    "#     N = size(𝐙, 2)\n",
    "#     # Keep 𝐙 as SharedArray, will be faster than taking sdata() when fed to the following @parallel loop\n",
    "#     sum = @parallel (+) for n in 1:N # Eq (10)\n",
    "#         yhat_n = 𝐙[:,n] ⋅ 𝐰 # Eq (11)\n",
    "#         - 𝐲[n] * log(yhat_n) - (1 - 𝐲[n]) * log(1 - yhat_n)\n",
    "#     end\n",
    "#     return sum\n",
    "# end\n",
    "\n",
    "#function Ly{T1<:Number,T2<:Number,T3<:Number}(𝐙::SharedArray{T1,2}, 𝐲::Vector{T2}, 𝐰::Vector{T3})\n",
    "@everywhere function Ly(𝐙::SharedArray{Float64,2}, 𝐲::Vector{Int}, 𝐰::SharedArray{Float64,1})\n",
    "    Ly(sdata(𝐙), 𝐲, sdata(𝐰))\n",
    "end\n",
    "\n",
    "@everywhere function Ly(𝐙::SharedArray{Float64,2}, 𝐲::Vector{Int}, 𝐰::Vector{Float64})\n",
    "    Ly(sdata(𝐙), 𝐲, 𝐰)\n",
    "end\n",
    "\n",
    "#function Ly{T1<:Number,T2<:Number,T3<:Number}(𝐙::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3})\n",
    "@everywhere function Ly(𝐙::Matrix{Float64}, 𝐲::Vector{Int}, 𝐰::Vector{Float64})\n",
    "    N = size(𝐙, 2)\n",
    "    sum = zero(Float64)\n",
    "    𝐲hat = 𝐰' * 𝐙 # Eq (11)\n",
    "    𝐲hat = reshape(𝐲hat, length(𝐲hat))\n",
    "    for n in 1:N # Eq (10)\n",
    "        sum += - 𝐲[n] * log(𝐲hat[n]) - (1 - 𝐲[n]) * log(1 - 𝐲hat[n])\n",
    "    end\n",
    "    return sum\n",
    "end\n",
    "\n",
    "# Slower:\n",
    "# function Ly2{T1<:Number,T2<:Number,T3<:Number}(𝐙::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3})\n",
    "#     tsup = (𝐰' * 𝐙) # # Eq (10) and (11)\n",
    "#     tsup = reshape(tsup, length(tsup))\n",
    "#     return -dot(𝐲, log(tsup)) + -dot(1 - 𝐲, log(1 - tsup))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.236009 seconds (450 allocations: 2.402 GB, 26.95% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19855.692771914662,19855.692771914662,19855.692771914662)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test (e.g. 20572.03833866735)\n",
    "LYtrain1 = Ly(𝐗train, 𝐕, 𝐲train, 𝐰)\n",
    "LYtrain2 = Ly(𝐙pre, 𝐲train, 𝐰)\n",
    "LYtrain3 = Ly(𝐙preu, 𝐲train, 𝐰)\n",
    "(LYtrain1, LYtrain2, LYtrain3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    }
   ],
   "source": [
    "@time for i in 1:200\n",
    "    Ly(𝐙pre, 𝐲train, 𝐰)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective_pre_alpha_s (generic function with 2 methods)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall objective function\n",
    "objective_euclidean(𝐗, 𝐗⁺, 𝐗⁻, 𝐕, 𝐲, 𝐰, A) = A[:z]*Lz(𝐗⁺, 𝐗⁻, 𝐕) + A[:x]*Lx(𝐗, 𝐕) + A[:y]*Ly(𝐗, 𝐕, 𝐲, 𝐰)\n",
    "objective_dist(𝐗, 𝐗⁺, 𝐗⁻, 𝐕, 𝐲, 𝐰, A, dist) = A[:z]*Lz(𝐗⁺, 𝐗⁻, 𝐕, dist) + A[:x]*Lx(𝐗, 𝐕, dist) + A[:y]*Ly(𝐗, 𝐕, 𝐲, 𝐰, dist)\n",
    "function objective_pre(𝐗::Matrix, S::Vector{Bool}, 𝐕::Matrix, 𝐲::Vector, 𝐰::Vector, A::Dict)\n",
    "    # Calculate 𝐙 and partitions once\n",
    "    𝐙 = softmax_euclidean_par(𝐗, 𝐕)\n",
    "    (𝐙⁺, 𝐙⁻) = partition(𝐙, S)\n",
    "    # Use functions that accept precalculated 𝐙\n",
    "    return A[:z]*Lz(𝐙⁺, 𝐙⁻) + A[:x]*Lx(𝐙, 𝐗, 𝐕) + A[:y]*Ly(𝐙, 𝐲, 𝐰)\n",
    "end\n",
    "function objective_pre_alpha(𝐗::Matrix{Float64}, S::Vector{Bool}, 𝐕::Matrix{Float64}, 𝐲::Vector{Int}, 𝐰::Vector{Float64}, A::Dict{Symbol, Float64}, 𝛂⁺::Vector{Float64}, 𝛂⁻::Vector{Float64})\n",
    "    # Calculate 𝐙 and partitions\n",
    "    (𝐗⁺, 𝐗⁻) = partition(𝐗, S)\n",
    "    (𝐲⁺, 𝐲⁻) = partition(𝐲, S)\n",
    "    𝐙⁺ = softmax_alpha_s(𝐗⁺, 𝐕, 𝛂⁺)\n",
    "    𝐙⁻ = softmax_alpha_s(𝐗⁻, 𝐕, 𝛂⁻)\n",
    "    # Use functions that accept precalculated 𝐙\n",
    "    return A[:z]*Lz(𝐙⁺, 𝐙⁻) + A[:x]*Lx(𝐙⁺, 𝐗⁺, 𝐕) + A[:x]*Lx(𝐙⁻, 𝐗⁻, 𝐕) + A[:y]*Ly(𝐙⁺, 𝐲⁺, 𝐰) + A[:y]*Ly(𝐙⁻, 𝐲⁻, 𝐰)\n",
    "end\n",
    "function objective_pre_alpha_s(𝐗::SharedArray{Float64,2}, S::SharedArray{Bool,1}, 𝐕::Matrix{Float64}, 𝐲::SharedArray{Int,1}, 𝐰::Vector{Float64}, A::Dict{Symbol, Float64}, 𝛂⁺::Vector{Float64}, 𝛂⁻::Vector{Float64})\n",
    "    # Calculate 𝐙 and partitions\n",
    "    (𝐗⁺, 𝐗⁻) = partition(𝐗, S)\n",
    "    (𝐲⁺, 𝐲⁻) = partition(𝐲, S)\n",
    "    @sync begin\n",
    "        𝐙⁺ref = @spawn softmax_alpha_s(𝐗⁺, 𝐕, 𝛂⁺)\n",
    "        𝐙⁻ref = @spawn softmax_alpha_s(𝐗⁻, 𝐕, 𝛂⁻)\n",
    "    end\n",
    "    𝐙⁺ = fetch(𝐙⁺ref)\n",
    "    𝐙⁻ = fetch(𝐙⁻ref)\n",
    "    @sync begin\n",
    "        rlz = @spawn Lz(𝐙⁺, 𝐙⁻)\n",
    "        rlxp = @spawn Lx(𝐙⁺, 𝐗⁺, 𝐕)\n",
    "        rlxn = @spawn Lx(𝐙⁻, 𝐗⁻, 𝐕)\n",
    "        rlyp = @spawn Ly(𝐙⁺, 𝐲⁺, 𝐰)\n",
    "        rlyn = @spawn Ly(𝐙⁻, 𝐲⁻, 𝐰)\n",
    "    end\n",
    "    # Use functions that accept precalculated 𝐙\n",
    "    #return A[:z]*Lz(𝐙⁺, 𝐙⁻) + A[:x]*Lx(𝐙⁺, 𝐗⁺, 𝐕) + A[:x]*Lx(𝐙⁻, 𝐗⁻, 𝐕) + A[:y]*Ly(𝐙⁺, 𝐲⁺, 𝐰) + A[:y]*Ly(𝐙⁻, 𝐲⁻, 𝐰)\n",
    "    return A[:z]*fetch(rlz) + A[:x]*fetch(rlxp) + A[:x]*fetch(rlxn) + A[:y]*fetch(rlyp) + A[:y]*fetch(rlyn)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.328048 seconds (1.66 k allocations: 49.763 MB, 3.73% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2081.352446744131"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_euclidean(𝐗train, 𝐗⁺train, 𝐗⁻train, 𝐕, 𝐲train, 𝐰, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2081.352446744131"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_pre(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2951.8650964392987"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_pre_alpha(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "𝐗train_s = convert(SharedArray{Float64, 2}, 𝐗train)\n",
    "S_𝐗train_s = convert(SharedArray{Bool, 1}, S_𝐗train)\n",
    "𝐲train_s = convert(SharedArray{Int, 1}, 𝐲train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2951.8650964392987"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_pre_alpha_s(𝐗train_s, S_𝐗train_s, 𝐕, 𝐲train_s, 𝐰, A, 𝛂⁺, 𝛂⁻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1220.3472494405382"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_euclidean(𝐗test, 𝐗⁺test, 𝐗⁻test, 𝐕, 𝐲test, 𝐰, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.755369 seconds (831.70 k allocations: 558.355 MB, 1.60% gc time)\n",
      "  2.124663 seconds (210.88 k allocations: 557.520 MB, 3.58% gc time)\n",
      "  1.753765 seconds (13.20 M allocations: 1.100 GB, 35.45% gc time)\n",
      "  1.303245 seconds (84.00 k allocations: 283.905 MB, 2.54% gc time)\n"
     ]
    }
   ],
   "source": [
    "# @time for i in 1:10\n",
    "#     objective_euclidean(𝐗train, 𝐗⁺train, 𝐗⁻train, 𝐕, 𝐲train, 𝐰, A)\n",
    "# end\n",
    "# @time for i in 1:10\n",
    "#     objective_pre(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A)\n",
    "# end\n",
    "# @time for i in 1:10\n",
    "#     objective_pre_alpha(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)\n",
    "# end\n",
    "# @time for i in 1:10\n",
    "#     objective_pre_alpha_s(𝐗train_s, S_𝐗train_s, 𝐕, 𝐲train_s, 𝐰, A, 𝛂⁺, 𝛂⁻)\n",
    "# end\n",
    "#   5.755369 seconds (831.70 k allocations: 558.355 MB, 1.60% gc time)\n",
    "#   2.124663 seconds (210.88 k allocations: 557.520 MB, 3.58% gc time)\n",
    "#   1.753765 seconds (13.20 M allocations: 1.100 GB, 35.45% gc time)\n",
    "#   1.303245 seconds (84.00 k allocations: 283.905 MB, 2.54% gc time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17.722520 seconds (132.00 M allocations: 10.997 GB, 34.11% gc time)\n",
      " 13.390848 seconds (858.29 k allocations: 2.773 GB, 2.72% gc time)\n"
     ]
    }
   ],
   "source": [
    "# @time for i in 1:100\n",
    "#     objective_pre_alpha(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)\n",
    "# end\n",
    "# @time for i in 1:100\n",
    "#     objective_pre_alpha_s(𝐗train_s, S_𝐗train_s, 𝐕, 𝐲train_s, 𝐰, A, 𝛂⁺, 𝛂⁻)\n",
    "# end\n",
    "#  17.722520 seconds (132.00 M allocations: 10.997 GB, 34.11% gc time)\n",
    "#  13.390848 seconds (858.29 k allocations: 2.773 GB, 2.72% gc time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benchmark (generic function with 1 method)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fun_to_profile(n::Int)\n",
    "   for i in 1:n\n",
    "        objective_pre_alpha(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Profiling\n",
    "profiling_logfile = \"profile.bin\"\n",
    "function benchmark()\n",
    "    # Any setup code goes here.\n",
    "\n",
    "    # Run once, to force compilation.\n",
    "    println(\"======================= First run:\")\n",
    "    srand(666)\n",
    "    @time fun_to_profile(1)\n",
    "\n",
    "    # Run a second time, with profiling.\n",
    "    println(\"\\n\\n======================= Second run:\")\n",
    "    srand(666)\n",
    "    Profile.init(delay=0.01)\n",
    "    Profile.clear()\n",
    "    Profile.clear_malloc_data()\n",
    "    @profile @time fun_to_profile(50)\n",
    "    \n",
    "    Profile.print()\n",
    "\n",
    "#     # Write profile results to profile.bin.\n",
    "#     r = Profile.retrieve()\n",
    "#     f = open(profiling_logfile, \"w\")\n",
    "#     serialize(f, r)\n",
    "#     close(f)\n",
    "end\n",
    "\n",
    "# function show_profiling()\n",
    "#     f = open(profiling_logfile)\n",
    "#     r = deserialize(f);\n",
    "#     ProfileView.view(r[1], lidict=r[2])\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show_profiling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test optimization run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj_func (generic function with 1 method)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0 # keep track of # function evaluations\n",
    "#function obj_func(x::Vector, grad::Vector)\n",
    "function obj_func(x::Vector{Float64})\n",
    "#function obj_func(x::Vector)\n",
    "#     return sum(x)\n",
    "    # Increase count\n",
    "    global count\n",
    "    count::Int += 1\n",
    "    \n",
    "    # Print progress\n",
    "    if count % 250 == 0\n",
    "        @printf(\"round %d, \", count)\n",
    "    end\n",
    "    \n",
    "    # Get 𝐕, 𝐰, 𝛂⁺, 𝛂⁻\n",
    "    loc𝐕 = reshape(x[1:length(𝐕)], size(𝐕)) # Uses the global 𝐕 for size\n",
    "    cursor = length(𝐕)\n",
    "    loc𝛂⁺ = x[cursor+1:cursor+length(𝛂⁺)]\n",
    "    cursor += length(𝛂⁺)\n",
    "    loc𝛂⁻ = x[cursor+1:cursor+length(𝛂⁻)]\n",
    "    cursor += length(𝛂⁻)\n",
    "    loc𝐰 = x[cursor+1:cursor+length(𝐰)]\n",
    "    #return size(loc𝐕), size(loc𝛂⁺), size(loc𝛂⁻), size(loc𝐰)\n",
    "#     return objective_pre_alpha(𝐗train, S_𝐗train, loc𝐕, 𝐲train, loc𝐰, A, loc𝛂⁺, loc𝛂⁻)\n",
    "    return objective_pre_alpha_s(𝐗train_s, S_𝐗train_s, loc𝐕, 𝐲train_s, loc𝐰, A, loc𝛂⁺, loc𝛂⁻)\n",
    "#     return objective_pre_alpha(\n",
    "#         𝐗train::Matrix{Float64},\n",
    "#         S_𝐗train::Vector{Bool},\n",
    "#         loc𝐕::Vector{Float64},\n",
    "#         𝐲train::Vector{Int},\n",
    "#         loc𝐰::Vector{Float64},\n",
    "#         A::Dict{Symbol, Float64},\n",
    "#         loc𝛂⁺::Vector{Float64},\n",
    "#         loc𝛂⁻::Vector{Float64}\n",
    "#     )\n",
    "#     if length(grad) > 0:\n",
    "#         ...set grad to gradient, in-place...\n",
    "#     return ...value of f(x)...\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: "
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "reshape(A, dims)\n",
       "\\end{verbatim}\n",
       "Create an array with the same data as the given array, but with different dimensions. An implementation for a particular type of array may choose whether the data is copied or shared.\n"
      ],
      "text/markdown": [
       "```\n",
       "reshape(A, dims)\n",
       "```\n",
       "\n",
       "Create an array with the same data as the given array, but with different dimensions. An implementation for a particular type of array may choose whether the data is copied or shared.\n"
      ],
      "text/plain": [
       "```\n",
       "reshape(A, dims)\n",
       "```\n",
       "\n",
       "Create an array with the same data as the given array, but with different dimensions. An implementation for a particular type of array may choose whether the data is copied or shared.\n"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape promote_shape\n",
      "\n"
     ]
    }
   ],
   "source": [
    "?reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99,13),(1,13),(99,),(99,))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(𝐕),size(𝐰'),size(𝛂⁺),size(𝛂⁻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optVarInit = vcat(reshape(𝐕, length(𝐕)), reshape(𝛂⁺, length(𝛂⁺)), reshape(𝛂⁻, length(𝛂⁻)), reshape(𝐰, length(𝐰)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Int64,1}:\n",
       " 0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros(Int,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bounds_lower = vcat(fill(-Inf, length(𝐕)), zeros(length(𝛂⁺) + length(𝛂⁻) + length(𝐰)));\n",
    "bounds_upper = vcat(fill(+Inf, length(𝐕)), ones(length(𝛂⁺) + length(𝛂⁻) + length(𝐰)));\n",
    "length(optVarInit), length(bounds_lower), length(bounds_upper)\n",
    "bounds = collect(zip(bounds_lower, bounds_upper));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Tuple{Float64,Float64},1}:\n",
       " (-Inf,Inf)\n",
       " (-Inf,Inf)\n",
       " (-Inf,Inf)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bounds = collect(zip(zeros(3), ones(3)))\n",
    "bounds[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 250, round 500, round 750, round 1000, round 1250, "
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: InterruptException:\nwhile loading In[177], in expression starting on line 2",
     "output_type": "error",
     "traceback": [
      "LoadError: InterruptException:\nwhile loading In[177], in expression starting on line 2",
      "",
      " in pycall at /Users/kuuranne/.julia/v0.4/PyCall/src/PyCall.jl:383",
      " in call at /Users/kuuranne/.julia/v0.4/PyCall/src/PyCall.jl:388"
     ]
    }
   ],
   "source": [
    "count::Int = 0\n",
    "res = scipyopt.fmin_l_bfgs_b(\n",
    "    obj_func,\n",
    "    x0=optVarInit,\n",
    "    #x0=optVarInit[1:3],\n",
    "    epsilon=1e-5,\n",
    "#     args=(\n",
    "#         training_sensitive, training_nonsensitive, \n",
    "#         ytrain_sensitive, ytrain_nonsensitive, k, 1e-4,\n",
    "#         0.1, 1000, 0\n",
    "#     ),\n",
    "    bounds=bounds,\n",
    "    #bounds=bounds,\n",
    "    approx_grad=true,\n",
    "    maxfun=500,\n",
    "    maxiter=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count::Int = 0\n",
    "# # # Call L-BFGS\n",
    "# # res = optimize(obj_func,\n",
    "# #     optVarInit,\n",
    "# #     method = :l_bfgs,\n",
    "# #     xtol = 1e-4,\n",
    "# #     grtol = 1e-12,\n",
    "# #     iterations = 1000,\n",
    "# #     store_trace = true,\n",
    "# #     show_trace = false)\n",
    "\n",
    "# # TODO: Can we use upper and lower limits for 𝐕? Will it speed up the optimizer?\n",
    "# #       The prototypes need to lie inside the smallest hypercube that contains all original datapoints, yes?\n",
    "\n",
    "# d1 = DifferentiableFunction(obj_func)\n",
    "# # # Note that d1 above will use central finite differencing to approximate the gradient.\n",
    "\n",
    "# res = fminbox(d1,\n",
    "#     optVarInit,\n",
    "#     bounds_lower,\n",
    "#     bounds_upper,\n",
    "#     xtol = 1e-4,\n",
    "#     grtol = 1e-12,\n",
    "#     iterations = 100,\n",
    "#     store_trace = true,\n",
    "#     show_trace = false)\n",
    "# # In tutorial: @elapsed fminbox(d4, x0, l, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #opt = Opt(:LD_MMA, 2)\n",
    "# opt = Opt(:LN_SBPLX, length(optVarInit)) # Use a derivative-free optimization algorithm (instead of L-BFGS)\n",
    "\n",
    "# # Lower and upper bounds for alphas and 𝐰\n",
    "# lower_bounds!(opt, bounds_lower)\n",
    "# upper_bounds!(opt, bounds_upper)\n",
    "\n",
    "# # Tolerance\n",
    "# xtol_rel!(opt, 1e-4)\n",
    "\n",
    "# # Stop when the number of function evaluations exceeds the second argument.\n",
    "# #(0 or negative for no limit.)\n",
    "# maxeval!(opt, 200)\n",
    "\n",
    "# # Stop when the optimization time (in seconds) exceeds the second argument.\n",
    "# #(0 or negative for no limit.)\n",
    "# maxtime!(opt, 60*2)\n",
    "\n",
    "# # Minimize\n",
    "# min_objective!(opt, obj_func)\n",
    "\n",
    "# (minf,minx,ret) = NLopt.optimize(opt, optVarInit)\n",
    "# println(\"got $minf at $minx after $count iterations (returned $ret)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization, running the algorithm\n",
    "Hyperparameters for the objective function.\n",
    "In the paper they use grid search to find the parameters. The sets defined here are the same as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sets of hyperparameters as in paper, for grid search\n",
    "gridA = Dict(:z => Set([0.1, 0.5, 1.0, 5.0, 10.0]), :x => Set([0, 0.01]), :y => Set([0.1, 0.5, 1.0, 5.0, 10.0]))\n",
    "# An example of selected hyperparameters, for development\n",
    "A = Dict(:z => 0.01, :x => 0.5, :y => 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Overall process with pictures\n",
    "- Some analysis e.g. check the alphas if they give the same result\n",
    "- Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems/Cons/Notes:\n",
    "- Nothing said in the paper about choosing the number of prototypes $K$\n",
    "- Let's say that there is a column/feature \"Religion\" in the dataset.\n",
    "- Now this paper says we can only say that \"Is a member of protected group\" or \"Is not a member of protected group\".\n",
    "- You have to decide what is the \"protected\" group, and what is the \"normal/non protected\" group. You have to decide based on some external criteria who are discriminated against and who are not.\n",
    "- Let's say we have a dataset with a feature \"Religion\" and we have 5 different religions represented.\n",
    "- Now we have to choose which ones are protected and which ones are not.\n",
    "- The problem of course is that some might be in general discriminated against more than others. There is not necessary even split between the different groups that are discriminated against.\n",
    "\n",
    "- What we would like to say is that \"Religion\" is a sensitive feature, and we should not infer _anything_ from it, regardless what it is.\n",
    "\n",
    "- Does running the algo multiple times help, changing the binary classification each time? Can we extend it so that $S \\in {1,...,C}$ where $C$ is the number of categories in the sensitive column.\n",
    "  - We can extend, just split $L_z$ to multiple cases and the optimization is done to all of them. There will be $c = \\frac{(C-1)C}{2} \\approx O(C^2)$ pairs. Whether this is computationally still feasible is another question. In the objective function $L_z$ is replaced by $A_{z_1} \\cdot L_{z_1} + A_{z_2} \\cdot L_{z_2} + \\dots + A_{z_c} \\cdot L_{z_c}$.\n",
    "\n",
    "- On the current case where $S \\in \\left\\{0,1\\right\\}$ once we have set for which rows $S=1$ and $S=0$, we can flip them around without changing anything. This is because we are using statistical parity. This means that from the algorithm's perspective saying that group0 is non-protected and groups 1..4 are protected is the same as saying group1 is protected and other non-protected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problems with Julia / libraries\n",
    "- Optim.jl and NLOpt.jl don't work for some reason on my Mac. Need to check if the problem persists on Linux. This caused maybe 20 hours of extra work, until I figured out I can use PyCall to call SciPy's L-BFGS implementation.\n",
    "- I've spent hours optimizing the code but it is still too slow. Should go through Julia's performance manual again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
