{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Nothing to be done\n",
      "INFO: Nothing to be done\n",
      "INFO: Nothing to be done\n"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Dataframes\")\n",
    "Pkg.add(\"Optim\") # For L-BFGS <https://github.com/JuliaOpt/Optim.jl#basic-api-introduction>\n",
    "#Pkg.add(\"NLopt\")\n",
    "#Pkg.add(\"Orchestra\")\n",
    "Pkg.add(\"ProfileView\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pkg.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using DataArrays, DataFrames\n",
    "using Optim\n",
    "#using NLopt # Nonlinear optimization library http://ab-initio.mit.edu/wiki/index.php/NLopt\n",
    "using ProfileView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parallel processing cores\n",
    "nprocs()==CPU_CORES || addprocs(CPU_CORES-1)\n",
    "nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the \"Adult\" dataset\n",
    "The Adult dataset is from [here](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/).\n",
    "In categorical data, missing data is handled as just another category. This data set does not contain NA-values in the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{Symbol,1}:\n",
       " :age           \n",
       " :workclass     \n",
       " :fnlwgt        \n",
       " :education     \n",
       " :education_num \n",
       " :marital_status\n",
       " :occupation    \n",
       " :relationship  \n",
       " :race          \n",
       " :sex           \n",
       " :capital_gain  \n",
       " :capital_loss  \n",
       " :hours_per_week\n",
       " :native_country\n",
       " :classification"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_orig_data = \"data/adult.data\"\n",
    "filename_orig_test = \"data/adult.test\"\n",
    "# NOTE: if the data set is very large, we should save the preprocessed data to avoid preprocessing cost.\n",
    "#filename_data = \"data/processed.adult.data\"\n",
    "#filename_test = \"data/processed.adult.test\"\n",
    "\n",
    "# missing_data_marker = \"?\" # can be a string\n",
    "\n",
    "df_orig_data = readtable(filename_orig_data);\n",
    "df_orig_test = readtable(filename_orig_test);\n",
    "\n",
    "# Not in use, let NA values go through as \"?\", handled as yet another category\n",
    "#df_orig_test = readtable(filename_orig_test, nastrings = [\"\", \"NA\", \"?\"]);\n",
    "\n",
    "# df_orig_test[ 1:3, :age ] # Example: selecting subset of rows (1 to 3), from a certain column\n",
    "\n",
    "names(df_orig_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for our model\n",
    "Set $K$ to the number of prototypes. The paper does not discuss how this should be chosen. Here we use the dimension of the original data, when sensitive and classification features are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to encode with OneHot encoding aka Dummy variables\n",
    "# Remember to not encode sensitive_column_name and classification_column_name\n",
    "# Has to be an array\n",
    "columns_to_encode = [:workclass, :education, :marital_status, :occupation, :relationship, :race, :native_country]\n",
    "\n",
    "# Sensitive column is \"gender\" as in the paper\n",
    "# NOTE: This column must be have only two possible values (be a binary variable)\n",
    "#       as the method in the paper assumes this (\"protected or not protected\").\n",
    "sensitive_column_name = :sex\n",
    "\n",
    "# NOTE: This column must be have only two possible values (be a binary variable)\n",
    "#       as the method in the paper assumes this (binary classification).\n",
    "classification_column_name = :classification\n",
    "\n",
    "# Size or data matrix. -1 for classification/target column and -1 for sensitive column\n",
    "K = size(df_orig_data, 2) - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partition (generic function with 6 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling the categorical variables, using OneHot encoding\n",
    "# https://groups.google.com/forum/#!topic/julia-users/7-Vtpi8w4YI\n",
    "# Would be nice to use DataFrames pool, but couldn't figure out how to get the OneHot-encoded ModelMatrix out nicely.\n",
    "# http://dataframesjl.readthedocs.org/en/latest/pooling.html\n",
    "# http://stackoverflow.com/questions/29158626/dummy-variables-in-julia\n",
    "\n",
    "function getdummy{R}(df::DataFrame, cname::Symbol, ::Type{R})\n",
    "    darr = df[cname]\n",
    "    vals = sort(levels(darr))[2:end]\n",
    "    #namedict = Dict(vals, 1:length(vals))\n",
    "    namedict = Dict(zip(vals,1:length(vals)))\n",
    "    arr = zeros(R, length(darr), length(namedict))\n",
    "    for i=1:length(darr)\n",
    "        if haskey(namedict, darr[i])\n",
    "            arr[i, namedict[darr[i]]] = 1\n",
    "        end\n",
    "    end\n",
    "    newdf = convert(DataFrame, arr)\n",
    "    names!(newdf, [symbol(\"$(cname)_$k\") for k in vals])\n",
    "    return newdf\n",
    "end\n",
    "\n",
    "# Conversion to dummy variables / OneHot encoding\n",
    "function convertdummy{R}(df::DataFrame, cnames::Array{Symbol}, ::Type{R})\n",
    "    # consider every variable from cnames as categorical\n",
    "    # and convert them into set of dummy variables,\n",
    "    # return new dataframe\n",
    "    newdf = DataFrame()\n",
    "    for cname in names(df)\n",
    "        if !in(cname, cnames)\n",
    "            newdf[cname] = df[cname]\n",
    "        else\n",
    "            dummydf = getdummy(df, cname, R)\n",
    "            for dummyname in names(dummydf)\n",
    "                newdf[dummyname] = dummydf[dummyname]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return newdf\n",
    "end\n",
    "\n",
    "# Two parameter version\n",
    "convertdummy(df::DataFrame, cnames::Array{Symbol}) = convertdummy(df, cnames, Int32)\n",
    "\n",
    "\n",
    "# A nice Unicode named summation function. Not really necessary, just something Julia can do.\n",
    "#∑(from::Integer, to::Integer, inner::Function) = sum(inner, colon(from,to))\n",
    "# Test:\n",
    "#f(x, k) = x*k\n",
    "#∑(1, 3, (k) -> f(1, k))\n",
    "\n",
    "# Partition a matrix two, according to given indices or indicator vector.\n",
    "# Your matrices need to be column-major, as this is the Julia memory layout.\n",
    "function partition{T<:Integer}(x::Vector, indices::Vector{T})\n",
    "    return x[ indices ], x[ setdiff(1:length(x), indices) ]\n",
    "end\n",
    "function partition{T<:Integer}(X::Matrix, indices::Vector{T})\n",
    "    return X[ :, indices ], X[ :, setdiff(1:size(X,2), indices) ]\n",
    "end\n",
    "function partition{T<:Integer,U<:Any}(X::SharedArray{U,2}, indices::Vector{T})\n",
    "    return X[ :, indices ], X[ :, setdiff(1:size(X,2), indices) ]\n",
    "end\n",
    "function partition{T<:Bool}(x::Vector, indicator::Vector{T})\n",
    "    return partition(x, find(indicator))\n",
    "end\n",
    "function partition{T<:Bool}(X::Matrix, indicator::Vector{T})\n",
    "    return partition(X, find(indicator))\n",
    "end\n",
    "function partition{T<:Bool,U<:Any}(X::SharedArray{U,2}, indicator::Vector{T})\n",
    "    return partition(X, find(indicator))\n",
    "end\n",
    "# Test:\n",
    "# @which partition([:first, :second, :third, :fourth], [true, false, true, false])\n",
    "# @which partition([:first, :second, :third, :fourth], [1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OneHot encoding (dummy variables) for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data (32561,15)(16281,15)(48842,15)\n",
      "Encoded data ("
     ]
    }
   ],
   "source": [
    "# Vertically concatenate to get the whole dataset\n",
    "df_orig_all = vcat(df_orig_data, df_orig_test)\n",
    "print(\"Original data \", size(df_orig_data), size(df_orig_test), size(df_orig_all), \"\\n\")\n",
    "# Do the One-Hot-Encoding / Dummy variables conversion.\n",
    "df_all = convertdummy(df_orig_all, columns_to_encode)\n",
    "\n",
    "### Map sensitive and classification/target columns to appropriate types.\n",
    "\n",
    "# Map Sensitive column \"Male\"/\"Female\" to true/false.\n",
    "# Change this if you change the sensitive column\n",
    "# (maybe do something that automatically just picks one category to be true and other to be false)\n",
    "df_all[ sensitive_column_name ] = map(gender -> gender == \"Female\" ? true : false, df_all[ sensitive_column_name ])\n",
    "df_all[ sensitive_column_name ] = convert(DataArrays.DataArray{Bool,1}, df_all[ sensitive_column_name ])\n",
    "# Map classification column values to 0 and 1\n",
    "df_all[ classification_column_name ] = map(class -> class == \">50K\" ? 1 : 0, df_all[ classification_column_name ])\n",
    "df_all[ classification_column_name ] = convert(DataArrays.DataArray{Integer,1}, df_all[ classification_column_name ])\n",
    "\n",
    "# Read out the converted data back to data and test sets.\n",
    "len_data = size(df_orig_data, 1)\n",
    "len_test = size(df_orig_test, 1)\n",
    "df_data = df_all[1:len_data, :]\n",
    "df_test = df_all[len_data+1:len_data+len_test, :]\n",
    "print(\"Encoded data \", size(df_data), size(df_test), size(df_all), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Code for the model\n",
    "\n",
    "## Notation/Definitions in paper\n",
    "\n",
    "- $X$ denotes the entire data set of individuals. Each $x \\in X$ is a vector of length $D$ where each component of the vector describes some attribute of the person.\n",
    "- $S$ is a binary random variable representing whether or not a given individual is a member of the protected set; we assume the system has access to this attribute.\n",
    "- $X_0$ denotes the training set of individuals.\n",
    "- $X^+ \\subset X$, $X_0^+ \\subset X_0$ denotes the subset of individuals (from the whole set and the training set respectively) that are members of the protected set (i.e., $S = 1$), and $X^−$ and $X_0^−$ denotes the subsets that are not members of the protected set, i.e., $S = 0$.\n",
    "- $Z$ is a multinomial random variable, where each of the $K$ values represents one of the intermediate set of ”prototypes”. Associated with each prototype is a vector $\\mathbf{v}_k$ in the same space as the individuals $\\mathbf{x}$.\n",
    "- $Y$ is the binary random variable representing the classification decision for an individual, and $f : X \\rightarrow Y$ is the desired classification function.\n",
    "- $d$ is a distance measure on $X$, e.g., simple Euclidean distance: $d(\\mathbf{x}_n , \\mathbf{v}_k ) = \\Vert\\mathbf{x}_n − \\mathbf{v}_k \\Vert_2$.\n",
    "\n",
    "## Our changes and clarifications\n",
    "\n",
    "We will differ a bit from the definitions in the paper. The definitions we use are:\n",
    "- $\\mathbf{X}$ denotes the entire data set, a $(N \\times D)$ matrix. The rows of the matrix are the feature vectors $\\mathbf{x}_n$ representing attributes of an individual. $\\mathbf{X}$ contains neither the classification information (target) column, nor the sensitive column.\n",
    "- $S$ is a binary variable representing whether or not a given individual is a member of the \"protected group\". For the user of the algorithm, this is a decision that is done before running the algorithm by setting `sensitive_column_name` in the parameters.\n",
    "- $\\mathbf{X}_{train}$ denotes the training set.\n",
    "- $\\mathbf{X}_{test}$ denotes the test set.\n",
    "- $\\mathbf{X}^+$ denotes the subset of individuals that are members of the \"protected group\" i.e. individuals for whom $S=1$. Similarly $\\mathbf{X}^-$ denotes the subset of individuals for whom $S=0$. It's worthwhile to note that for the algorithm it actually doesn't matter if you flip the groups of who is \"protected\" and who is \"non-protected\", the result will be the same due to symmetry of statistical parity. So don't get too attached to the terminology.\n",
    "- Define $\\mathbf{X}_{train}^+$, $\\mathbf{X}_{train}^-$, $\\mathbf{X}_{test}^+$ and $\\mathbf{X}_{test}^-$ similarly as above.\n",
    "- $d$ is a distance measure on $\\mathbf{X}$ (e.g. euclidean distance).\n",
    "- $K$ is the number of prototypes.\n",
    "- $Z$ is a random integer from the set $\\left\\{1,\\dots,K\\right\\}$.\n",
    "- $Y$ is a binary variable representing the classification decision (we consider binary classification only).\n",
    "\n",
    "Let $Z$ be a random integer from the set $\\left\\{1,\\dots,K\\right\\}$. Now we can denote the probability that a datapoint $\\mathbf{x}$ maps to a particular prototype $k$ with $\\mathbb{P}(Z=k \\mid \\mathbf{x})$ i.e. given a datapoint $\\mathbf{x}$, the probability that $Z$, the index of the prototype for that data point, is $k$.\n",
    "\n",
    "## Definitions in code\n",
    "\n",
    "From the definitions above, we map some to code and also define additional stuff.\n",
    "\n",
    "Julia is Column-Majored, so our matrices will be altered accordingly.\n",
    "\n",
    "- $\\mathbf{X}$ is just `𝐗`, but $(D \\times N)$ instead of $(N \\times D)$.\n",
    "- $\\mathbf{X}_{train}$ is `𝐗train`, $\\mathbf{X}_{test}$ is `𝐗test`.\n",
    "- Grouped versions are `𝐗⁺train`, `𝐗⁻train`, `𝐗⁺test`, and `𝐗⁻test` respectively.\n",
    "- $S$ is defined as multiple vectors `S_<someset>`, each containing the sensitive column for `<someset>`, e.g. `S_𝐗`.\n",
    "- $d$ is defined as a lambda function `d` and plain function `de`.\n",
    "  - The functions implemented here have versions that default to euclidean distance, and versions that accept a user defined distance function.\n",
    "- $Z$ is replaced by $\\mathbf{Z}$, a matrix of probability vectors $\\mathbf{z}$.\n",
    "- The classification information is contained in `𝐲`, `𝐲train`, and `𝐲test`.\n",
    "\n",
    "Additionally:\n",
    "- Denote the tuple of prototypes $\\mathbf{V} = \\left(\\mathbf{v}_1,...,\\mathbf{v}_K\\right)$. Since a single prototype $\\mathbf{v}_k$ is a vector of length $D$, $\\mathbf{V}$ can be expressed as a ($D \\times K$) matrix. This is our optimization variable `𝐕`.\n",
    "- `A` contains the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32561,101)(16281,101)(48842,101)\n",
      "Done."
     ]
    }
   ],
   "source": [
    "# Sensitive indices for training and test data\n",
    "S_𝐗train = convert(Array{Bool}, df_data[ sensitive_column_name ])\n",
    "S_𝐗test = convert(Array{Bool}, df_test[ sensitive_column_name ])\n",
    "\n",
    "# Classification vectors for training and test data\n",
    "𝐲train = convert(Array, df_data[ classification_column_name ])\n",
    "𝐲test = convert(Array, df_test[ classification_column_name ])\n",
    "\n",
    "# Drop sensitive and classification columns\n",
    "idxs_left = setdiff(names(df_data), [sensitive_column_name, classification_column_name])\n",
    "𝐗train = transpose(convert(Matrix{Float64}, df_data[idxs_left]))\n",
    "𝐗test = transpose(convert(Matrix{Float64}, df_test[idxs_left]))\n",
    "# 𝐗train = transpose(convert(Matrix, df_data[idxs_left]))\n",
    "# 𝐗test = transpose(convert(Matrix, df_test[idxs_left]))\n",
    "\n",
    "\n",
    "# Standardize the features with mean 0 variance 1\n",
    "# Otherwise exponentiation gets quickly out of hand, e^-800 is already NaN on Float64\n",
    "# But only standardize non-one-hot-encoded features\n",
    "\n",
    "# Features that are not one-hot-encoded\n",
    "non_encoded = setdiff(names(df_orig_data), [sensitive_column_name, classification_column_name])\n",
    "non_encoded = setdiff(non_encoded, columns_to_encode)\n",
    "# Find indices of those features\n",
    "non_encoded_idxs = find(symbol -> in(symbol, non_encoded), names(df_data[idxs_left]))\n",
    "# Calculate mean and var from training set\n",
    "train_mean = mean(𝐗train,2)[:]\n",
    "train_var = var(𝐗train,2)[:]\n",
    "\n",
    "# # Standardize everything\n",
    "# 𝐗train = 𝐗train .- train_mean # substraction of vector\n",
    "# 𝐗train = 𝐗train ./ train_var # division by vector\n",
    "# # We need to use the same variance and mean for our test set as in the training set,\n",
    "# # otherwise they would not be comparable.\n",
    "# 𝐗test = 𝐗test .- train_mean # same mean as in training, on purpose\n",
    "# 𝐗test = 𝐗test ./ train_var # same variance as in training, on purpose\n",
    "\n",
    "# Only standardize non-one-hot-encoded features\n",
    "for n_idx in non_encoded_idxs\n",
    "    𝐗train[n_idx,:] = 𝐗train[n_idx,:] .- train_mean[n_idx] # substraction\n",
    "    𝐗train[n_idx,:] = 𝐗train[n_idx,:] ./ train_var[n_idx] # division\n",
    "    # We need to use the same variance and mean for our test set as in the training set,\n",
    "    # otherwise they would not be comparable.\n",
    "    𝐗test[n_idx,:] = 𝐗test[n_idx,:] .- train_mean[n_idx] # same mean as in training, on purpose\n",
    "    𝐗test[n_idx,:] = 𝐗test[n_idx,:] ./ train_var[n_idx] # same variance as in training, on purpose\n",
    "end\n",
    "\n",
    "# Reconstruct full dataset\n",
    "𝐗 = hcat(𝐗train, 𝐗test)\n",
    "S_𝐗 = vcat(S_𝐗train, S_𝐗test)\n",
    "𝐲 = vcat(𝐲train, 𝐲test)\n",
    "\n",
    "# Dimensions\n",
    "D = size(𝐗, 1)\n",
    "N = size(𝐗, 2)\n",
    "Ntrain = size(𝐗train, 2)\n",
    "Ntest = size(𝐗test, 2)\n",
    "\n",
    "### Distance function\n",
    "# Lambda\n",
    "d = (𝐚::Vector, 𝐛::Vector) -> vecnorm(𝐚 - 𝐛) # Euclidean distance\n",
    "# Non-lambda is slightly faster for calculations, but has to be defined\n",
    "# for all processes with @everywhere\n",
    "@everywhere de(𝐚::Vector{Float64}, 𝐛::Vector{Float64}) = vecnorm(𝐚 - 𝐛)\n",
    "# With alphas\n",
    "#@everywhere dalpha(𝐚::Vector{Float64}, 𝐛::Vector{Float64}, 𝛂::Vector{Float64}) = sum(i -> 𝛂[i]*(𝐚[i]-𝐛[i])^2, 1:D) vecnorm(𝐚 - 𝐛)\n",
    "@everywhere function dalpha(𝐚::Vector{Float64}, 𝐛::Vector{Float64}, 𝛂::Vector{Float64})\n",
    "    # Does this function bind us to use K=D? See Eq. 12. Can't think straight right now.\n",
    "    localD = length(𝛂)\n",
    "    sum = 0.0\n",
    "    @inbounds for i in 1:localD  # (Eq. 12)\n",
    "        sum += 𝛂[i]*((𝐚[i]-𝐛[i])^2)\n",
    "    end\n",
    "    return sum\n",
    "    # Slower alternatives:\n",
    "    # return 𝛂 ⋅ ((𝐚-𝐛).^2) # (Eq. 12)\n",
    "    # return sum(i -> 𝛂[i]*(𝐚[i]-𝐛[i])^2, 1:localD) # (Eq. 12)\n",
    "end\n",
    "@everywhere function dalpha_test(𝐚::Vector{Float64}, 𝐛::Vector{Float64}, 𝛂::Vector{Float64})\n",
    "    # For testing purposes, ignores alphas and returns vector norm\n",
    "    return vecnorm(𝐚 - 𝐛)\n",
    "end\n",
    "\n",
    "### Optimization variables\n",
    "# Main optimization variable, matrix holding the prototype vectors.\n",
    "# Initialized to random Float64 matrix, normal distribution 0 mean 1 variance\n",
    "𝐕 = randn(D, K)\n",
    "# # Weights or \"prototype label predictions\" (probabilities)\n",
    "𝐰 = rand(K) # floats in [0,1)\n",
    "# # Alphas\n",
    "𝛂⁺ = rand(D)\n",
    "𝛂⁻ = rand(D)\n",
    "\n",
    "### Hyperparameters\n",
    "A = Dict(:z=> 1000, :x=> 0.0001, :y=> 0.1)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,-1.3719338843612912,1.0,-1.3719338843612912,3.2262304410946294,-3.336553142375075)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum(𝐗train), minimum(𝐗train), maximum(𝐗test), minimum(𝐗test), maximum(𝐕), minimum(𝐕)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide training and test sets to groups according to whether the individuals are \"protected\" or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Training:\",(99,10771),(99,21790),\"Test:\",(99,5421),(99,10860))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(𝐗⁺train, 𝐗⁻train) = partition(𝐗train, S_𝐗train)\n",
    "(𝐗⁺test, 𝐗⁻test) = partition(𝐗test, S_𝐗test)\n",
    "\"Training:\",size(𝐗⁺train), size(𝐗⁻train), \"Test:\", size(𝐗⁺test), size(𝐗⁻test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping $\\mathbf{X} \\rightarrow \\mathbf{Z}$\n",
    "Now we can define a mapping from the original dataset $\\mathbf{X}$ to probabilities via the *softmax function*. [Wikipedia](https://en.wikipedia.org/wiki/Softmax_function):\n",
    "> Softmax function \"squashes\" a $K$-dimensional vector $\\mathbf{z}$ of arbitrary real values to a $K$-dimensional vector $\\sigma(\\mathbf{z})$ of real values in the range $[0, 1]$ that add up to 1.\n",
    "\n",
    "Most notably `softmax` returns a probability vector. We will define a modified version that maps a $D$-dimensional vector $\\mathbf{x}$ to a $K$-dimensional vector $\\sigma(\\mathbf{x})$, i.e. the mapping won't necessarily preserve the dimensionality of $\\mathbf{x}$.\n",
    "\n",
    "Also from [Wikipedia](https://en.wikipedia.org/wiki/Multinomial_logistic_regression):\n",
    "> $$\\operatorname{softmax}(k,x_1,\\ldots,x_n) = \\frac{e^{x_k}}{\\sum_{i=1}^n e^{x_i}}$$\n",
    "> is referred to as the [*softmax function*](https://en.wikipedia.org/wiki/Softmax_function).  The reason is that the effect of exponentiating the values $x_1,\\ldots,x_n$ is to exaggerate the differences between them.  As a result, $\\operatorname{softmax}(k,x_1,\\ldots,x_n)$ will return a value close to 0 whenever $x_k$ is significantly less than the maximum of all the values, and will return a value close to 1 when applied to the maximum value, unless it is extremely close to the next-largest value.  Thus, the softmax function can be used to construct a weighted average that behaves as a smooth function (which can be conveniently differentiated, etc.) and which approximates the [indicator function](https://en.wikipedia.org/wiki/Indicator_function).\n",
    "\n",
    "So we define, as in the paper equation (2), $$\\mathbb{P}(Z=k \\mid \\mathbf{x}) = \\frac{e^{-d(\\mathbf{x}, \\mathbf{v}_k)}}{\\sum_{j=1}^K e^{-d(\\mathbf{x}, \\mathbf{v}_j)}}$$\n",
    "where\n",
    "- $\\mathbb{P}(Z=k \\mid \\mathbf{x})$ is described in [definitions](#Definitions)\n",
    "- $\\mathbf{x}$ is the datapoint\n",
    "- $\\mathbf{v}_k$ is a vector associated with the $k$th prototype\n",
    "- $d$ is a distance measure between $\\mathbf{x}$ and $\\mathbf{v}_k$ (e.g. the euclidean distance)\n",
    "\n",
    "This means that since we have replaced $x_k$ in the softmax with the negative distance between $\\mathbf{x}$ and prototype $\\mathbf{v}_k$, the softmax returns a value close to 0 whenever the distance from $\\mathbf{x}$ to the prototype $\\mathbf{v}_k$ is significantly higher than $\\min_{j\\in\\{1,\\dots,K\\}}\\, d(\\mathbf{x}, \\mathbf{v}_i)$, and close to 1 when applied to the minimum value.\n",
    "\n",
    "\"Mapping from X to Z\" in the paper means mapping the vector $\\mathbf{x}$ to a probability vector $\\mathbf{z}$ of length $K$ via the softmax function. These probability vectors are then used directly for training the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In code** this means that $\\mathbb{P}(Z=k \\mid \\mathbf{x})$ is represented by a function taking\n",
    "- the data point $\\mathbf{x}$ which is a `Vector` of length $D$\n",
    "- $(D \\times K)$ `Matrix` of prototypes $\\mathbf{V}$ containing all $K$ prototypes $\\mathbf{v}_k$ i.e. each prototype is a `Vector` (remember, Column-Major order on matrices)\n",
    "- a distance measure on $\\mathbf{X}$\n",
    "\n",
    "and returning\n",
    "- a `Vector` $\\mathbf{z}$ of length $K$ representing a [probability vector](https://en.wikipedia.org/wiki/Probability_vector), where each value $z_k$ of the probability vector $\\mathbf{z}$ tells how probable it is that $\\mathbf{x}$ maps to $\\mathbf{v}_k$. Since $\\mathbf{z}$ is a probability vector, $\\sum_{i=k}^K z_k = 1$.\n",
    "\n",
    "We will name this function `softmax` and define it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_euclidean_par (generic function with 1 method)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: clean up duplicate code\n",
    "\n",
    "### FOR VECTORS; give in 𝐱, get 𝐳\n",
    "\n",
    "# This is same as Eq (2) in paper.\n",
    "function softmax_dist{T<:Number}(𝐱::Vector{T}, 𝐕::Matrix{T}, distanceMeasure::Function)\n",
    "    K = size(𝐕, 2)\n",
    "    res = Vector{Float64}(K)\n",
    "    denominator = Float64(0.0)\n",
    "    # Use one loop to calculate both numerator and denominator\n",
    "    @inbounds for k in 1:K\n",
    "        res[k] = exp(- distanceMeasure(𝐱, 𝐕[:,k]) )\n",
    "        denominator += res[k]\n",
    "    end\n",
    "    denom = inv(denominator)\n",
    "    res .* denom\n",
    "end\n",
    "\n",
    "function softmax_euclidean{T<:Number}(𝐱::Vector{T}, 𝐕::Matrix{T})\n",
    "    K = size(𝐕, 2)\n",
    "    res = Vector{Float64}(K)\n",
    "    denominator = Float64(0.0)\n",
    "    # Use one loop to calculate both numerator and denominator\n",
    "    @inbounds for k in 1:K\n",
    "        res[k] = exp(- vecnorm(𝐱 - 𝐕[:,k]) )\n",
    "        denominator += res[k]\n",
    "    end\n",
    "    denom = inv(denominator)\n",
    "    res .* denom\n",
    "end\n",
    "\n",
    "### FOR MATRICES; give in 𝐗, get 𝐙\n",
    "\n",
    "function softmax_dist{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T}, distanceMeasure::Function)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = Matrix{Float64}(K, N)\n",
    "    @inbounds for n in 1:N\n",
    "        res[:,n] = softmax_dist(𝐗[:,n], 𝐕, distanceMeasure)\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "# Version that accepts alphas for distance function\n",
    "function softmax_dist_alpha{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T}, distanceMeasure::Function, 𝛂::Vector{U})\n",
    "    #nprocs()==CPU_CORES || addprocs(CPU_CORES-1)    \n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = Matrix{Float64}(K, N)\n",
    "    @inbounds @simd for n in 1:N\n",
    "        @inbounds @simd for k in 1:K\n",
    "            res[k,n] = exp(- distanceMeasure(𝐗[:,n], 𝐕[:,k], 𝛂) )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "# Parallel version\n",
    "function softmax_dist_par{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T}, distanceMeasure::Function)\n",
    "    #nprocs()==CPU_CORES || addprocs(CPU_CORES-1)    \n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = SharedArray(Float64, (K, N))\n",
    "    @sync @parallel for n in 1:N\n",
    "        for k in 1:K\n",
    "            res[k,n] = exp(- distanceMeasure(𝐗[:,n], 𝐕[:,k]) )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "# Parallel version that accepts alphas for distance function\n",
    "function softmax_dist_alpha_par{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T}, distanceMeasure::Function, 𝛂::Vector{U})\n",
    "    #nprocs()==CPU_CORES || addprocs(CPU_CORES-1)    \n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = SharedArray(Float64, (K, N))\n",
    "    @sync @parallel for n in 1:N\n",
    "        for k in 1:K\n",
    "            res[k,n] = exp(- distanceMeasure(𝐗[:,n], 𝐕[:,k], 𝛂) )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "function softmax_euclidean{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = Matrix{Float64}(K, N)\n",
    "    @inbounds for n in 1:N\n",
    "        denominator = Float64(0.0)\n",
    "        # Use one loop to calculate both numerator and denominator\n",
    "        for k in 1:K\n",
    "            res[k,n] = exp(- vecnorm(𝐗[:,n] - 𝐕[:,k]) )\n",
    "            denominator += res[k,n]\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(denominator)\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "# Parallel version\n",
    "function softmax_euclidean_par{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "    #nprocs()==CPU_CORES || addprocs(CPU_CORES-1)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = SharedArray(Float64, (K, N))\n",
    "    @sync @parallel for n in 1:N\n",
    "        for k in 1:K\n",
    "            res[k,n] = exp(- vecnorm(𝐗[:,n] - 𝐕[:,k]) )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For development\n",
    "𝐙pre = softmax_euclidean_par(𝐗train, 𝐕)\n",
    "𝐙pre⁺ = softmax_euclidean_par(𝐗⁺train, 𝐕)\n",
    "𝐙pre⁻ = softmax_euclidean_par(𝐗⁻train, 𝐕);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is akin to using a funky [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) to \"predict the prototype\" (category) where a data point $\\mathbf{x}$ maps to.\n",
    "Wikipedia:\n",
    "> These are all statistical classification problems. They all have in common a dependent variable to be predicted that comes from one of a limited set of items which cannot be meaningfully ordered, as well as a set of independent variables (also known as features, explanators, etc.), which are used to predict the dependent variable. Multinomial logit regression is a particular solution to the classification problem that assumes that a linear combination of the observed features and some problem-specific parameters can be used to determine the probability of each particular outcome of the dependent variable. The best values of the parameters for a given problem are usually determined from some training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of the optimization objective\n",
    "\n",
    "### $L_z$ &mdash; statistical parity\n",
    "In code, we will denote $L_z$ with function `Lz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lz (generic function with 3 methods)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### \"NAIVE\" VERSION FOR UNDERSTANDING THE IMPLEMENTATION\n",
    "\n",
    "function LzNaive{T<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T}, dist::Function)\n",
    "    # Operate on matrices and take mean from sample dimension N\n",
    "    meanp = mean( softmax_dist_par(𝐗⁺, 𝐕, dist), 2 ) # Eq (6)\n",
    "    meann = mean( softmax_dist_par(𝐗⁻, 𝐕, dist), 2 ) # Similarly for M_k^-\n",
    "    sum(abs(meanp - meann)) # Eq (7), sum is from k=1 to K\n",
    "end\n",
    "\n",
    "### VERSIONS TAKING IN THE DATA SET AND PROTOTYPES\n",
    "# Optionally a distance measure function can be passed as an argument\n",
    "\n",
    "function Lz{T<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T})\n",
    "    return Lz(sdata(softmax_euclidean_par(𝐗⁺, 𝐕)), sdata(softmax_euclidean_par(𝐗⁻, 𝐕)))\n",
    "end\n",
    "\n",
    "function Lz{T<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T}, dist::Function)\n",
    "    return Lz(sdata(softmax_dist_par(𝐗⁺, 𝐕, dist)), sdata(softmax_dist_par(𝐗⁻, 𝐕, dist)))\n",
    "end\n",
    "\n",
    "# # TODO: use parallel version of softmax_dist_alpha?\n",
    "# function Lz{T<:Number,U<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T}, dist::Function, 𝛂::Vector{U})\n",
    "#     return Lz(softmax_dist_alpha(𝐗⁺, 𝐕, dist, 𝛂), softmax_dist_alpha(𝐗⁻, 𝐕, dist, 𝛂))\n",
    "# end\n",
    "\n",
    "### VERSION FOR PRECALCULATED 𝐙⁺ and 𝐙⁻\n",
    "# Note we have only a version for matrices. This is because during performance\n",
    "# testing I noticed that\n",
    "#\n",
    "#   ZZZp = sdata(𝐙shared⁺)\n",
    "#   ZZZn = sdata(𝐙shared⁻)\n",
    "#   Lz(ZZZp, ZZZn)\n",
    "#\n",
    "# is faster than\n",
    "#\n",
    "#   Lz(𝐙shared⁺, 𝐙shared⁻)\n",
    "#\n",
    "# So use the Matrix version always and if necessary lift the matrices out\n",
    "# of the SharedArray with sdata().\n",
    "#\n",
    "# TODO: is there way to make a faster parallel version?\n",
    "\n",
    "function Lz{T<:Number}(𝐙⁺::Matrix{T}, 𝐙⁻::Matrix{T})\n",
    "    # Operate on matrices and take mean from sample dimension N\n",
    "    meanp = mean( 𝐙⁺, 2 )[:] # Eq (6)\n",
    "    meann = mean( 𝐙⁻, 2 )[:] # Similarly for M_k^-\n",
    "    sum(abs(meanp - meann)) # Eq (7), sum is from k=1 to K\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05724699945304177"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test (e.g. somewhere between 0.02 and 0.06, depending on 𝐕 randomization)\n",
    "LZtrain = Lz(𝐗⁺test, 𝐗⁻test, 𝐕)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_x$ &mdash; information loss\n",
    "In code, we will denote $L_x$ with function `Lx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Symbols: 𝐗 ⁺ ⁻ ∑ 𝐕 𝐱 𝐲 𝐙 𝐳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note .* elementwise multiplication of softmax_dist() and V, there is no \\cdot in the paper in Eq (9), dot product would return a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lx (generic function with 4 methods)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### NAIVE VERSION FOR UNDERSTANDING THE IMPLEMENTATION\n",
    "\n",
    "function LxNaive{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{U}, dist::Function)\n",
    "    D = size(𝐗, 1)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    𝐗hat = zeros(Float64, (D,N))\n",
    "    sum = Float64(0.0)\n",
    "    for n in 1:N\n",
    "        𝐳_n = softmax_dist(𝐗[:,n], 𝐕, dist) # prob that x_n maps to protos (Array{13,1})\n",
    "        for k in 1:K # Eq (9)\n",
    "            𝐗hat[:,n] = 𝐗hat[:,n] + (𝐳_n[k] * 𝐕[:,k])\n",
    "        end\n",
    "        sum += (𝐗[:,n] - 𝐗hat[:,n]) ⋅ (𝐗[:,n] - 𝐗hat[:,n]) # Eq (8)\n",
    "    end\n",
    "    return sum, 𝐗hat\n",
    "end\n",
    "\n",
    "### VERSIONS TAKING IN THE DATA SET AND PROTOTYPES\n",
    "# Optionally a distance measure function can be passed as an argument\n",
    "\n",
    "function Lx{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{U})\n",
    "    return Lx(softmax_euclidean_par(𝐗, 𝐕), 𝐗, 𝐕)\n",
    "end\n",
    "\n",
    "function Lx{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{U}, dist::Function)\n",
    "    return Lx(softmax_dist_par(𝐗, 𝐕, dist), 𝐗, 𝐕)\n",
    "end\n",
    "\n",
    "### VERSION FOR PRECALCULATED 𝐙\n",
    "\n",
    "function Lx{T<:Number}(𝐙::SharedArray{T,2}, 𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "    D = size(𝐕, 1)\n",
    "    K = size(𝐕, 2)\n",
    "    N = size(𝐙, 2)\n",
    "    # Keep 𝐙 as SharedArray, will be faster than taking sdata() when fed to the following @parallel loop\n",
    "    sum = @parallel (+) for n in 1:N # Eq (8)\n",
    "        𝐱hat_n = zeros(Float64, D)\n",
    "        for k in 1:K # Eq (9)\n",
    "            𝐱hat_n = 𝐱hat_n + (𝐙[k,n] * 𝐕[:,k]) # We are constructing a vector of length D\n",
    "        end\n",
    "        (𝐗[:,n] - 𝐱hat_n) ⋅ (𝐗[:,n] - 𝐱hat_n) # \"simple squared error\"\n",
    "    end\n",
    "    return sum\n",
    "end\n",
    "\n",
    "# TODO parallel?\n",
    "function Lx{T<:Number}(𝐙::Matrix{T}, 𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "    D = size(𝐕, 1)\n",
    "    K = size(𝐕, 2)\n",
    "    N = size(𝐙, 2)\n",
    "#    # Keep 𝐙 as SharedArray, will be faster than taking sdata() when fed to the following @parallel loop\n",
    "    sum::Float64 = 0.0\n",
    "    @inbounds @simd for n in 1:N # Eq (8)\n",
    "        𝐱hat_n = zeros(Float64, D)\n",
    "        for k in 1:K # Eq (9)\n",
    "            𝐱hat_n = 𝐱hat_n + (𝐙[k,n] * 𝐕[:,k]) # We are constructing a vector of length D\n",
    "        end\n",
    "        sum += (𝐗[:,n] - 𝐱hat_n) ⋅ (𝐗[:,n] - 𝐱hat_n) # \"simple squared error\"\n",
    "    end\n",
    "    return sum\n",
    "end\n",
    "\n",
    "# TODO: test if making V into sharedarray increases performance, probably not since V is usually small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611049.2235646215"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test (e.g. 423475.8479283692)\n",
    "LXtrain = Lx(𝐗train, 𝐕, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_y$ &mdash; prediction accuracy\n",
    "In code, we will denote $L_y$ with function `Ly`.\n",
    "\n",
    "Essentially here we are letting the optimization pick both the prototypes (i.e. feature vectors) and their predictions (i.e. labels), and the predictions don't have to be discrete 0 and 1, but can be from the range $[0,1]$ and thus themselves can be viewed as probabilities. E.g. let's say that for prototype $v_k$ it's prediction $w_k = 0.82$, then \"there is a 82% chance prototype $\\mathbf{v}_k$ gets label 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ly (generic function with 4 methods)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### NAIVE VERSION TO HELP UNDERSTAND THE IMPLEMENTATION\n",
    "\n",
    "function LyNaive{T1<:Number,T2<:Number,T3<:Number}(\n",
    "        𝐗::Matrix{T1}, 𝐕::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}, dist::Function\n",
    "    )\n",
    "    D = size(𝐗, 1)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    𝐲hat = zeros(Float64, N)\n",
    "    sum = Float64(0.0)\n",
    "    # Replace 𝐲hat in Eq (10) with Eq (11), then you get this for loop\n",
    "    for n in 1:N\n",
    "        𝐙_n = softmax_dist(𝐗[:,n], 𝐕, dist) # Vector of length K\n",
    "        for k in 1:K\n",
    "            𝐲hat[n] = 𝐲hat[n] + (𝐙_n[k] * 𝐰[k])\n",
    "        end\n",
    "        # The following line could be replaced with\n",
    "        # if 𝐲[n] == 1\n",
    "        #    sum -= log(𝐲hat[n])\n",
    "        # else # 𝐲[n] == 0\n",
    "        #    sum -= log(1 - 𝐲hat[n])\n",
    "        # end\n",
    "        sum += -𝐲[n] * log(𝐲hat[n])  -  (1 - 𝐲[n]) * log(1 - 𝐲hat[n])\n",
    "    end\n",
    "    #return sum, 𝐲hat\n",
    "    return sum\n",
    "end\n",
    "\n",
    "### VERSIONS TAKING IN THE DATA SET AND PROTOTYPES\n",
    "# Optionally a distance measure function can be passed as an argument\n",
    "\n",
    "function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "        𝐗::Matrix{T1}, 𝐕::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}\n",
    "    )\n",
    "    # 𝐙 = softmax_euclidean_par(𝐗, 𝐕)\n",
    "    return Ly(softmax_euclidean_par(𝐗, 𝐕), 𝐲, 𝐰)\n",
    "end\n",
    "\n",
    "function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "        𝐗::Matrix{T1}, 𝐕::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}, dist::Function\n",
    "    )\n",
    "    # 𝐙 = softmax_dist_par(𝐗, 𝐕, dist)\n",
    "    return Ly(softmax_dist_par(𝐗, 𝐕, dist), 𝐲, 𝐰)\n",
    "end\n",
    "\n",
    "### VERSION FOR PRECALCULATED 𝐙\n",
    "\n",
    "# function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "#         𝐙::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}\n",
    "#     )\n",
    "#     # Copy to shared memory\n",
    "#     𝐙shared = convert(SharedArray{T1, 2}, 𝐙)\n",
    "#     return Ly(𝐙shared, 𝐲, 𝐰)\n",
    "# end\n",
    "\n",
    "function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "        𝐙::SharedArray{T1,2}, 𝐲::Vector{T2}, 𝐰::Vector{T3}\n",
    "    )\n",
    "    N = size(𝐙, 2)\n",
    "    # Keep 𝐙 as SharedArray, will be faster than taking sdata() when fed to the following @parallel loop\n",
    "    sum = @parallel (+) for n in 1:N # Eq (10)\n",
    "        yhat_n = 𝐙[:,n] ⋅ 𝐰 # Eq (11)\n",
    "        - 𝐲[n] * log(yhat_n) - (1 - 𝐲[n]) * log(1 - yhat_n)\n",
    "    end\n",
    "    return sum\n",
    "end\n",
    "\n",
    "function Ly{T1<:Number,T2<:Number,T3<:Number}(𝐙::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3})\n",
    "    N = size(𝐙, 2)\n",
    "#     # Keep 𝐙 as SharedArray, will be faster than taking sdata() when fed to the following @parallel loop\n",
    "    sum::Float64 = 0.0\n",
    "    @inbounds @simd for n in 1:N # Eq (10)\n",
    "        yhat_n = 𝐙[:,n] ⋅ 𝐰 # Eq (11)\n",
    "        sum += - 𝐲[n] * log(yhat_n) - (1 - 𝐲[n]) * log(1 - yhat_n)\n",
    "    end\n",
    "    return sum\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20547.574072108637"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test (e.g. 20572.03833866735)\n",
    "LYtrain = Ly(𝐗train, 𝐕, 𝐲train, 𝐰)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective_pre_alphadist_nonpar (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall objective function\n",
    "objective_euclidean(𝐗, 𝐗⁺, 𝐗⁻, 𝐕, 𝐲, 𝐰, A) = A[:z]*Lz(𝐗⁺, 𝐗⁻, 𝐕) + A[:x]*Lx(𝐗, 𝐕) + A[:y]*Ly(𝐗, 𝐕, 𝐲, 𝐰)\n",
    "objective_dist(𝐗, 𝐗⁺, 𝐗⁻, 𝐕, 𝐲, 𝐰, A, dist) = A[:z]*Lz(𝐗⁺, 𝐗⁻, 𝐕, dist) + A[:x]*Lx(𝐗, 𝐕, dist) + A[:y]*Ly(𝐗, 𝐕, 𝐲, 𝐰, dist)\n",
    "function objective_pre(𝐗::Matrix, S::Vector{Bool}, 𝐕::Matrix, 𝐲::Vector, 𝐰::Vector, A::Dict)\n",
    "    # Calculate 𝐙 and partitions once\n",
    "    𝐙 = softmax_euclidean_par(𝐗, 𝐕)\n",
    "    (𝐙⁺, 𝐙⁻) = partition(𝐙, S)\n",
    "    # Use functions that accept precalculated 𝐙\n",
    "    return A[:z]*Lz(𝐙⁺, 𝐙⁻) + A[:x]*Lx(𝐙, 𝐗, 𝐕) + A[:y]*Ly(𝐙, 𝐲, 𝐰)\n",
    "end\n",
    "function objective_pre_alphadist(𝐗::Matrix, S::Vector{Bool}, 𝐕::Matrix, 𝐲::Vector, 𝐰::Vector, A::Dict, 𝛂⁺::Vector, 𝛂⁻::Vector)\n",
    "    # Calculate 𝐙 and partitions\n",
    "    (𝐗⁺, 𝐗⁻) = partition(𝐗, S)\n",
    "    (𝐲⁺, 𝐲⁻) = partition(𝐲, S)\n",
    "    𝐙⁺ = softmax_dist_alpha_par(𝐗⁺, 𝐕, dalpha, 𝛂⁺) # Can put dalpha_test to make sure result is same as objective_pre\n",
    "    𝐙⁻ = softmax_dist_alpha_par(𝐗⁻, 𝐕, dalpha, 𝛂⁻) # Can put dalpha_test to make sure result is same as objective_pre\n",
    "    # Use functions that accept precalculated 𝐙\n",
    "    return A[:z]*Lz(sdata(𝐙⁺), sdata(𝐙⁻)) + A[:x]*Lx(𝐙⁺, 𝐗⁺, 𝐕) + A[:x]*Lx(𝐙⁻, 𝐗⁻, 𝐕) + A[:y]*Ly(𝐙⁺, 𝐲⁺, 𝐰) + A[:y]*Ly(𝐙⁻, 𝐲⁻, 𝐰)\n",
    "end\n",
    "function objective_pre_alphadist_nonpar(𝐗::Matrix, S::Vector{Bool}, 𝐕::Matrix, 𝐲::Vector, 𝐰::Vector, A::Dict, 𝛂⁺::Vector, 𝛂⁻::Vector)\n",
    "    # Calculate 𝐙 and partitions\n",
    "    (𝐗⁺, 𝐗⁻) = partition(𝐗, S)\n",
    "    (𝐲⁺, 𝐲⁻) = partition(𝐲, S)\n",
    "    𝐙⁺ = softmax_dist_alpha(𝐗⁺, 𝐕, dalpha, 𝛂⁺) # Can put dalpha_test to make sure result is same as objective_pre\n",
    "    𝐙⁻ = softmax_dist_alpha(𝐗⁻, 𝐕, dalpha, 𝛂⁻) # Can put dalpha_test to make sure result is same as objective_pre\n",
    "    # Use functions that accept precalculated 𝐙\n",
    "    return A[:z]*Lz(𝐙⁺, 𝐙⁻) + A[:x]*Lx(𝐙⁺, 𝐗⁺, 𝐕) + A[:x]*Lx(𝐙⁻, 𝐗⁻, 𝐕) + A[:y]*Ly(𝐙⁺, 𝐲⁺, 𝐰) + A[:y]*Ly(𝐙⁻, 𝐲⁻, 𝐰)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2173.422892653416"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_euclidean(𝐗train, 𝐗⁺train, 𝐗⁻train, 𝐕, 𝐲train, 𝐰, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2173.422892653416"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_pre(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3067.9043622279164"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_pre_alphadist(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3067.904362227916"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_pre_alphadist_nonpar(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show_profiling (generic function with 1 method)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function fun_to_profile()\n",
    "    objective_pre_alphadist_nonpar(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)\n",
    "end\n",
    "\n",
    "function fun_to_profile_loop()\n",
    "   for i in 1:50\n",
    "        fun_to_profile()\n",
    "    end\n",
    "end\n",
    "\n",
    "# Profiling\n",
    "profiling_logfile = \"profile.bin\"\n",
    "function benchmark()\n",
    "    # Any setup code goes here.\n",
    "\n",
    "    # Run once, to force compilation.\n",
    "    println(\"======================= First run:\")\n",
    "    srand(666)\n",
    "    @time fun_to_profile()\n",
    "\n",
    "    # Run a second time, with profiling.\n",
    "    println(\"\\n\\n======================= Second run:\")\n",
    "    srand(666)\n",
    "    Profile.init(delay=0.01)\n",
    "    Profile.clear()\n",
    "    Profile.clear_malloc_data()\n",
    "    @profile @time fun_to_profile_loop()\n",
    "\n",
    "    # Write profile results to profile.bin.\n",
    "    r = Profile.retrieve()\n",
    "    f = open(profiling_logfile, \"w\")\n",
    "    serialize(f, r)\n",
    "    close(f)\n",
    "end\n",
    "\n",
    "function show_profiling()\n",
    "    f = open(profiling_logfile)\n",
    "    r = deserialize(f);\n",
    "    ProfileView.view(r[1], lidict=r[2])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= First run:\n",
      "  "
     ]
    }
   ],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show_profiling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7.008553 seconds (74.51 M allocations: 20.287 GB, 33.03% gc time)\n"
     ]
    }
   ],
   "source": [
    "# @time for i in 1:10\n",
    "#     objective_euclidean(𝐗train, 𝐗⁺train, 𝐗⁻train, 𝐕, 𝐲train, 𝐰, A)\n",
    "# end\n",
    "# @time for i in 1:10\n",
    "#     objective_pre(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A)\n",
    "# end\n",
    "# @time for i in 1:10\n",
    "#     objective_pre_alphadist(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)\n",
    "# end\n",
    "# @time for i in 1:10\n",
    "#     objective_pre_alphadist_nonpar(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)\n",
    "# end\n",
    "#   7.533158 seconds (5.70 M allocations: 301.744 MB, 1.88% gc time)\n",
    "#   4.280320 seconds (5.09 M allocations: 301.643 MB, 1.56% gc time)\n",
    "#   4.707826 seconds (5.64 M allocations: 577.336 MB, 2.26% gc time)\n",
    "#   7.008553 seconds (74.51 M allocations: 20.287 GB, 33.03% gc time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objective_euclidean(𝐗test, 𝐗⁺test, 𝐗⁻test, 𝐕, 𝐲test, 𝐰, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test optimization run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0 # keep track of # function evaluations\n",
    "#function obj_func(x::Vector, grad::Vector)\n",
    "function obj_func(x::Vector)\n",
    "    # Increase count\n",
    "    global count\n",
    "    count::Int += 1\n",
    "    \n",
    "    # Print progress\n",
    "    if count % 50 == 0\n",
    "        @printf(\"Round %d\\n\", count)\n",
    "    end\n",
    "    \n",
    "    # Get 𝐕, 𝐰, 𝛂⁺, 𝛂⁻\n",
    "    loc𝐕 = reshape(x[1:length(𝐕)], size(𝐕)) # Uses the global 𝐕 for size\n",
    "    cursor = length(𝐕)\n",
    "    loc𝛂⁺ = x[cursor+1:cursor+length(𝛂⁺)]\n",
    "    cursor += length(𝛂⁺)\n",
    "    loc𝛂⁻ = x[cursor+1:cursor+length(𝛂⁻)]\n",
    "    cursor += length(𝛂⁻)\n",
    "    loc𝐰 = x[cursor+1:cursor+length(𝐰)]\n",
    "    #return size(loc𝐕), size(loc𝛂⁺), size(loc𝛂⁻), size(loc𝐰)\n",
    "    return objective_pre_alphadist(𝐗train, S_𝐗train, loc𝐕, 𝐲train, loc𝐰, A, loc𝛂⁺, loc𝛂⁻)\n",
    "#     if length(grad) > 0:\n",
    "#         ...set grad to gradient, in-place...\n",
    "#     return ...value of f(x)...\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size(𝐕),size(𝐰'),size(𝛂⁺),size(𝛂⁻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optVarInit = vcat(reshape(𝐕, length(𝐕)), reshape(𝛂⁺, length(𝛂⁺)), reshape(𝛂⁻, length(𝛂⁻)), reshape(𝐰, length(𝐰)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bounds_lower = vcat(fill(-Inf, length(𝐕)), zeros(length(𝛂⁺) + length(𝛂⁻) + length(𝐰)));\n",
    "bounds_upper = vcat(fill(+Inf, length(𝐕)), ones(length(𝛂⁺) + length(𝛂⁻) + length(𝐰)));\n",
    "length(optVarInit), length(bounds_lower), length(bounds_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Call L-BFGS\n",
    "res = optimize(obj_func,\n",
    "    optVarInit,\n",
    "    method = :l_bfgs,\n",
    "    xtol = 1e-4,\n",
    "    grtol = 1e-12,\n",
    "    iterations = 15000,\n",
    "    store_trace = true,\n",
    "    show_trace = false)\n",
    "\n",
    "# TODO: Can we use upper and lower limits for 𝐕? Will it speed up the optimizer?\n",
    "#       The prototypes need to lie inside the smallest hypercube that contains all original datapoints, yes?\n",
    "\n",
    "# d1 = DifferentiableFunction(obj_func)\n",
    "# # Note that d1 above will use central finite differencing to approximate the gradient.\n",
    "\n",
    "# @elapsed res = fminbox(d1,\n",
    "#     optVarInit,\n",
    "#     bounds_lower,\n",
    "#     bounds_upper,\n",
    "#     method = :l_bfgs,\n",
    "#     xtol = 1e-4,\n",
    "#     grtol = 1e-12,\n",
    "#     iterations = 15000,\n",
    "#     store_trace = true,\n",
    "#     show_trace = false)\n",
    "# #fminbox(d4, x0, l, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#opt = Opt(:LD_MMA, 2)\n",
    "opt = Opt(:LN_SBPLX, length(optVarInit)) # Use a derivative-free optimization algorithm (instead of L-BFGS)\n",
    "\n",
    "# Lower and upper bounds for alphas and 𝐰\n",
    "lower_bounds!(opt, bounds_lower)\n",
    "upper_bounds!(opt, bounds_upper)\n",
    "\n",
    "# Tolerance\n",
    "xtol_rel!(opt,1e-4)\n",
    "\n",
    "# Stop when the number of function evaluations exceeds the second argument.\n",
    "#(0 or negative for no limit.)\n",
    "maxeval!(opt, 15000)\n",
    "\n",
    "# Stop when the optimization time (in seconds) exceeds the second argument.\n",
    "#(0 or negative for no limit.)\n",
    "maxtime!(opt, 60*2)\n",
    "\n",
    "# Minimize\n",
    "min_objective!(opt, obj_func)\n",
    "\n",
    "(minf,minx,ret) = optimize(opt, optVarInit)\n",
    "println(\"got $minf at $minx after $count iterations (returned $ret)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization, running the algorithm\n",
    "Hyperparameters for the objective function.\n",
    "In the paper they use grid search to find the parameters. The sets defined here are the same as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sets of hyperparameters as in paper, for grid search\n",
    "gridA = Dict(:z => Set([0.1, 0.5, 1.0, 5.0, 10.0]), :x => Set([0, 0.01]), :y => Set([0.1, 0.5, 1.0, 5.0, 10.0]))\n",
    "# An example of selected hyperparameters, for development\n",
    "A = Dict(:z => 0.01, :x => 0.5, :y => 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Overall process with pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems/Cons/Notes:\n",
    "- Let's say that there is a column/feature \"Religion\" in the dataset.\n",
    "- Now this paper says we can only say that \"Is a member of protected group\" or \"Is not a member of protected group\".\n",
    "- You have to decide what is the \"protected\" group, and what is the \"normal/non protected\" group. You have to decide based on some external criteria who are discriminated against and who are not.\n",
    "- Let's say we have a dataset with a feature \"Religion\" and we have 5 different religions represented.\n",
    "- Now we have to choose which ones are protected and which ones are not.\n",
    "- The problem of course is that some might be in general discriminated against more than others. There is not necessary even split between the different groups that are discriminated against.\n",
    "\n",
    "- What we would like to say is that \"Religion\" is a sensitive feature, and we should not infer _anything_ from it, regardless what it is.\n",
    "\n",
    "- Does running the algo multiple times help, changing the binary classification each time? Can we extend it so that $S \\in {1,...,C}$ where $C$ is the number of categories in the sensitive column.\n",
    "  - We can extend, just split $L_z$ to multiple cases and the optimization is done to all of them. There will be $c = \\frac{(C-1)C}{2} \\approx O(C^2)$ pairs. Whether this is computationally still feasible is another question. In the objective function $L_z$ is replaced by $A_{z_1} \\cdot L_{z_1} + A_{z_2} \\cdot L_{z_2} + \\dots + A_{z_c} \\cdot L_{z_c}$.\n",
    "\n",
    "- On the current case where $S \\in \\left\\{0,1\\right\\}$ once we have set for which rows $S=1$ and $S=0$, we can flip them around without changing anything. This is because we are using statistical parity. This means that from the algorithm's perspective saying that group0 is non-protected and groups 1..4 are protected is the same as saying group1 is protected and other non-protected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
