{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Nothing to be done\n"
     ]
    }
   ],
   "source": [
    "#Pkg.add(\"Dataframes\")\n",
    "Pkg.add(\"Optim\") # For L-BFGS <https://github.com/JuliaOpt/Optim.jl#basic-api-introduction>\n",
    "#Pkg.add(\"NLopt\")\n",
    "#Pkg.add(\"Orchestra\")\n",
    "#Pkg.add(\"ProfileView\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pkg.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using DataArrays, DataFrames\n",
    "using Optim\n",
    "#using NLopt # Nonlinear optimization library http://ab-initio.mit.edu/wiki/index.php/NLopt\n",
    "#using ProfileView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parallel processing cores\n",
    "nprocs()==CPU_CORES || addprocs(CPU_CORES-1)\n",
    "nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the \"Adult\" dataset\n",
    "The Adult dataset is from [here](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/).\n",
    "In categorical data, missing data is handled as just another category. This data set does not contain NA-values in the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{Symbol,1}:\n",
       " :age           \n",
       " :workclass     \n",
       " :fnlwgt        \n",
       " :education     \n",
       " :education_num \n",
       " :marital_status\n",
       " :occupation    \n",
       " :relationship  \n",
       " :race          \n",
       " :sex           \n",
       " :capital_gain  \n",
       " :capital_loss  \n",
       " :hours_per_week\n",
       " :native_country\n",
       " :classification"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_orig_data = \"data/adult.data\"\n",
    "filename_orig_test = \"data/adult.test\"\n",
    "# NOTE: if the data set is very large, we should save the preprocessed data to avoid preprocessing cost.\n",
    "#filename_data = \"data/processed.adult.data\"\n",
    "#filename_test = \"data/processed.adult.test\"\n",
    "\n",
    "# missing_data_marker = \"?\" # can be a string\n",
    "\n",
    "df_orig_data = readtable(filename_orig_data);\n",
    "df_orig_test = readtable(filename_orig_test);\n",
    "\n",
    "# Not in use, let NA values go through as \"?\", handled as yet another category\n",
    "#df_orig_test = readtable(filename_orig_test, nastrings = [\"\", \"NA\", \"?\"]);\n",
    "\n",
    "# df_orig_test[ 1:3, :age ] # Example: selecting subset of rows (1 to 3), from a certain column\n",
    "\n",
    "names(df_orig_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for our model\n",
    "Set $K$ to the number of prototypes. The paper does not discuss how this should be chosen. Here we use the dimension of the original data, when sensitive and classification features are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to encode with OneHot encoding aka Dummy variables\n",
    "# Remember to not encode sensitive_column_name and classification_column_name\n",
    "# Has to be an array\n",
    "columns_to_encode = [:workclass, :education, :marital_status, :occupation, :relationship, :race, :native_country]\n",
    "\n",
    "# Sensitive column is \"gender\" as in the paper\n",
    "# NOTE: This column must be have only two possible values (be a binary variable)\n",
    "#       as the method in the paper assumes this (\"protected or not protected\").\n",
    "sensitive_column_name = :sex\n",
    "\n",
    "# NOTE: This column must be have only two possible values (be a binary variable)\n",
    "#       as the method in the paper assumes this (binary classification).\n",
    "classification_column_name = :classification\n",
    "\n",
    "# Size or data matrix. -1 for classification/target column and -1 for sensitive column\n",
    "K = size(df_orig_data, 2) - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partition (generic function with 6 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling the categorical variables, using OneHot encoding\n",
    "# https://groups.google.com/forum/#!topic/julia-users/7-Vtpi8w4YI\n",
    "# Would be nice to use DataFrames pool, but couldn't figure out how to get the OneHot-encoded ModelMatrix out nicely.\n",
    "# http://dataframesjl.readthedocs.org/en/latest/pooling.html\n",
    "# http://stackoverflow.com/questions/29158626/dummy-variables-in-julia\n",
    "\n",
    "function getdummy{R}(df::DataFrame, cname::Symbol, ::Type{R})\n",
    "    darr = df[cname]\n",
    "    vals = sort(levels(darr))[2:end]\n",
    "    #namedict = Dict(vals, 1:length(vals))\n",
    "    namedict = Dict(zip(vals,1:length(vals)))\n",
    "    arr = zeros(R, length(darr), length(namedict))\n",
    "    for i=1:length(darr)\n",
    "        if haskey(namedict, darr[i])\n",
    "            arr[i, namedict[darr[i]]] = 1\n",
    "        end\n",
    "    end\n",
    "    newdf = convert(DataFrame, arr)\n",
    "    names!(newdf, [symbol(\"$(cname)_$k\") for k in vals])\n",
    "    return newdf\n",
    "end\n",
    "\n",
    "# Conversion to dummy variables / OneHot encoding\n",
    "function convertdummy{R}(df::DataFrame, cnames::Array{Symbol}, ::Type{R})\n",
    "    # consider every variable from cnames as categorical\n",
    "    # and convert them into set of dummy variables,\n",
    "    # return new dataframe\n",
    "    newdf = DataFrame()\n",
    "    for cname in names(df)\n",
    "        if !in(cname, cnames)\n",
    "            newdf[cname] = df[cname]\n",
    "        else\n",
    "            dummydf = getdummy(df, cname, R)\n",
    "            for dummyname in names(dummydf)\n",
    "                newdf[dummyname] = dummydf[dummyname]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return newdf\n",
    "end\n",
    "\n",
    "# Two parameter version\n",
    "convertdummy(df::DataFrame, cnames::Array{Symbol}) = convertdummy(df, cnames, Int32)\n",
    "\n",
    "\n",
    "# A nice Unicode named summation function. Not really necessary, just something Julia can do.\n",
    "#∑(from::Integer, to::Integer, inner::Function) = sum(inner, colon(from,to))\n",
    "# Test:\n",
    "#f(x, k) = x*k\n",
    "#∑(1, 3, (k) -> f(1, k))\n",
    "\n",
    "# Partition a matrix two, according to given indices or indicator vector.\n",
    "# Your matrices need to be column-major, as this is the Julia memory layout.\n",
    "function partition{T<:Integer}(x::Vector, indices::Vector{T})\n",
    "    return x[ indices ], x[ setdiff(1:length(x), indices) ]\n",
    "end\n",
    "function partition{T<:Integer}(X::Matrix, indices::Vector{T})\n",
    "    return X[ :, indices ], X[ :, setdiff(1:size(X,2), indices) ]\n",
    "end\n",
    "function partition{T<:Integer,U<:Any}(X::SharedArray{U,2}, indices::Vector{T})\n",
    "    return X[ :, indices ], X[ :, setdiff(1:size(X,2), indices) ]\n",
    "end\n",
    "function partition{T<:Bool}(x::Vector, indicator::Vector{T})\n",
    "    return partition(x, find(indicator))\n",
    "end\n",
    "function partition{T<:Bool}(X::Matrix, indicator::Vector{T})\n",
    "    return partition(X, find(indicator))\n",
    "end\n",
    "function partition{T<:Bool,U<:Any}(X::SharedArray{U,2}, indicator::Vector{T})\n",
    "    return partition(X, find(indicator))\n",
    "end\n",
    "# Test:\n",
    "# @which partition([:first, :second, :third, :fourth], [true, false, true, false])\n",
    "# @which partition([:first, :second, :third, :fourth], [1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OneHot encoding (dummy variables) for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Profile.clear()\n",
    "# @profile convertdummy(df_orig_all, columns_to_encode)\n",
    "# Profile.print()\n",
    "# Profile.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data (32561,15)(16281,15)(48842,15)\n",
      "Encoded data (32561,101)(16281,101)(48842,101)\n"
     ]
    }
   ],
   "source": [
    "# Vertically concatenate to get the whole dataset\n",
    "df_orig_all = vcat(df_orig_data, df_orig_test)\n",
    "print(\"Original data \", size(df_orig_data), size(df_orig_test), size(df_orig_all), \"\\n\")\n",
    "# Do the One-Hot-Encoding / Dummy variables conversion.\n",
    "df_all = convertdummy(df_orig_all, columns_to_encode)\n",
    "\n",
    "### Map sensitive and classification/target columns to appropriate types.\n",
    "\n",
    "# Map Sensitive column \"Male\"/\"Female\" to true/false.\n",
    "# Change this if you change the sensitive column\n",
    "# (maybe do something that automatically just picks one category to be true and other to be false)\n",
    "df_all[ sensitive_column_name ] = map(gender -> gender == \"Female\" ? true : false, df_all[ sensitive_column_name ])\n",
    "df_all[ sensitive_column_name ] = convert(DataArrays.DataArray{Bool,1}, df_all[ sensitive_column_name ])\n",
    "# Map classification column values to 0 and 1\n",
    "df_all[ classification_column_name ] = map(class -> class == \">50K\" ? 1 : 0, df_all[ classification_column_name ])\n",
    "df_all[ classification_column_name ] = convert(DataArrays.DataArray{Integer,1}, df_all[ classification_column_name ])\n",
    "\n",
    "# Read out the converted data back to data and test sets.\n",
    "len_data = size(df_orig_data, 1)\n",
    "len_test = size(df_orig_test, 1)\n",
    "df_data = df_all[1:len_data, :]\n",
    "df_test = df_all[len_data+1:len_data+len_test, :]\n",
    "print(\"Encoded data \", size(df_data), size(df_test), size(df_all), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Code for the model\n",
    "\n",
    "## Notation/Definitions in paper\n",
    "\n",
    "- $X$ denotes the entire data set of individuals. Each $x \\in X$ is a vector of length $D$ where each component of the vector describes some attribute of the person.\n",
    "- $S$ is a binary random variable representing whether or not a given individual is a member of the protected set; we assume the system has access to this attribute.\n",
    "- $X_0$ denotes the training set of individuals.\n",
    "- $X^+ \\subset X$, $X_0^+ \\subset X_0$ denotes the subset of individuals (from the whole set and the training set respectively) that are members of the protected set (i.e., $S = 1$), and $X^−$ and $X_0^−$ denotes the subsets that are not members of the protected set, i.e., $S = 0$.\n",
    "- $Z$ is a multinomial random variable, where each of the $K$ values represents one of the intermediate set of ”prototypes”. Associated with each prototype is a vector $\\mathbf{v}_k$ in the same space as the individuals $\\mathbf{x}$.\n",
    "- $Y$ is the binary random variable representing the classification decision for an individual, and $f : X \\rightarrow Y$ is the desired classification function.\n",
    "- $d$ is a distance measure on $X$, e.g., simple Euclidean distance: $d(\\mathbf{x}_n , \\mathbf{v}_k ) = \\Vert\\mathbf{x}_n − \\mathbf{v}_k \\Vert_2$.\n",
    "\n",
    "## Our changes and clarifications\n",
    "\n",
    "We will differ a bit from the definitions in the paper. The definitions we use are:\n",
    "- $\\mathbf{X}$ denotes the entire data set, a $(N \\times D)$ matrix. The rows of the matrix are the feature vectors $\\mathbf{x}_n$ representing attributes of an individual. $\\mathbf{X}$ contains neither the classification information (target) column, nor the sensitive column.\n",
    "- $S$ is a binary variable representing whether or not a given individual is a member of the \"protected group\". For the user of the algorithm, this is a decision that is done before running the algorithm by setting `sensitive_column_name` in the parameters.\n",
    "- $\\mathbf{X}_{train}$ denotes the training set.\n",
    "- $\\mathbf{X}_{test}$ denotes the test set.\n",
    "- $\\mathbf{X}^+$ denotes the subset of individuals that are members of the \"protected group\" i.e. individuals for whom $S=1$. Similarly $\\mathbf{X}^-$ denotes the subset of individuals for whom $S=0$. It's worthwhile to note that for the algorithm it actually doesn't matter if you flip the groups of who is \"protected\" and who is \"non-protected\", the result will be the same due to symmetry of statistical parity. So don't get too attached to the terminology.\n",
    "- Define $\\mathbf{X}_{train}^+$, $\\mathbf{X}_{train}^-$, $\\mathbf{X}_{test}^+$ and $\\mathbf{X}_{test}^-$ similarly as above.\n",
    "- $d$ is a distance measure on $\\mathbf{X}$ (e.g. euclidean distance).\n",
    "- $K$ is the number of prototypes.\n",
    "- $Z$ is a random integer from the set $\\left\\{1,\\dots,K\\right\\}$.\n",
    "- $Y$ is a binary variable representing the classification decision (we consider binary classification only).\n",
    "\n",
    "Let $Z$ be a random integer from the set $\\left\\{1,\\dots,K\\right\\}$. Now we can denote the probability that a datapoint $\\mathbf{x}$ maps to a particular prototype $k$ with $\\mathbb{P}(Z=k \\mid \\mathbf{x})$ i.e. given a datapoint $\\mathbf{x}$, the probability that $Z$, the index of the prototype for that data point, is $k$.\n",
    "\n",
    "## Definitions in code\n",
    "\n",
    "From the definitions above, we map some to code and also define additional stuff.\n",
    "\n",
    "Julia is Column-Majored, so our matrices will be altered accordingly.\n",
    "\n",
    "- $\\mathbf{X}$ is just `𝐗`, but $(D \\times N)$ instead of $(N \\times D)$.\n",
    "- $\\mathbf{X}_{train}$ is `𝐗train`, $\\mathbf{X}_{test}$ is `𝐗test`.\n",
    "- Grouped versions are `𝐗⁺train`, `𝐗⁻train`, `𝐗⁺test`, and `𝐗⁻test` respectively.\n",
    "- $S$ is defined as multiple vectors `S_<someset>`, each containing the sensitive column for `<someset>`, e.g. `S_𝐗`.\n",
    "- $d$ is defined as a lambda function `d` and plain function `de`.\n",
    "  - The functions implemented here have versions that default to euclidean distance, and versions that accept a user defined distance function.\n",
    "- $Z$ is replaced by $\\mathbf{Z}$, a matrix of probability vectors $\\mathbf{z}$.\n",
    "- The classification information is contained in `𝐲`, `𝐲train`, and `𝐲test`.\n",
    "\n",
    "Additionally:\n",
    "- Denote the tuple of prototypes $\\mathbf{V} = \\left(\\mathbf{v}_1,...,\\mathbf{v}_K\\right)$. Since a single prototype $\\mathbf{v}_k$ is a vector of length $D$, $\\mathbf{V}$ can be expressed as a ($D \\times K$) matrix. This is our optimization variable `𝐕`.\n",
    "- `A` contains the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done."
     ]
    }
   ],
   "source": [
    "# Sensitive indices for training and test data\n",
    "S_𝐗train = convert(Array{Bool}, df_data[ sensitive_column_name ])\n",
    "S_𝐗test = convert(Array{Bool}, df_test[ sensitive_column_name ])\n",
    "\n",
    "# Classification vectors for training and test data\n",
    "𝐲train = convert(Array, df_data[ classification_column_name ])\n",
    "𝐲test = convert(Array, df_test[ classification_column_name ])\n",
    "\n",
    "# Drop sensitive and classification columns\n",
    "idxs_left = setdiff(names(df_data), [sensitive_column_name, classification_column_name])\n",
    "𝐗train = transpose(convert(Matrix{Float64}, df_data[idxs_left]))\n",
    "𝐗test = transpose(convert(Matrix{Float64}, df_test[idxs_left]))\n",
    "# 𝐗train = transpose(convert(Matrix, df_data[idxs_left]))\n",
    "# 𝐗test = transpose(convert(Matrix, df_test[idxs_left]))\n",
    "\n",
    "\n",
    "# Standardize the features with mean 0 variance 1\n",
    "# Otherwise exponentiation gets quickly out of hand, e^-800 is already NaN on Float64\n",
    "# But only standardize non-one-hot-encoded features\n",
    "\n",
    "# Features that are not one-hot-encoded\n",
    "non_encoded = setdiff(names(df_orig_data), [sensitive_column_name, classification_column_name])\n",
    "non_encoded = setdiff(non_encoded, columns_to_encode)\n",
    "# Find indices of those features\n",
    "non_encoded_idxs = find(symbol -> in(symbol, non_encoded), names(df_data[idxs_left]))\n",
    "# Calculate mean and var from training set\n",
    "train_mean = mean(𝐗train,2)[:]\n",
    "train_var = var(𝐗train,2)[:]\n",
    "\n",
    "# # Standardize everything\n",
    "# 𝐗train = 𝐗train .- train_mean # substraction of vector\n",
    "# 𝐗train = 𝐗train ./ train_var # division by vector\n",
    "# # We need to use the same variance and mean for our test set as in the training set,\n",
    "# # otherwise they would not be comparable.\n",
    "# 𝐗test = 𝐗test .- train_mean # same mean as in training, on purpose\n",
    "# 𝐗test = 𝐗test ./ train_var # same variance as in training, on purpose\n",
    "\n",
    "# Only standardize non-one-hot-encoded features\n",
    "for n_idx in non_encoded_idxs\n",
    "    𝐗train[n_idx,:] = 𝐗train[n_idx,:] .- train_mean[n_idx] # substraction\n",
    "    𝐗train[n_idx,:] = 𝐗train[n_idx,:] ./ train_var[n_idx] # division\n",
    "    # We need to use the same variance and mean for our test set as in the training set,\n",
    "    # otherwise they would not be comparable.\n",
    "    𝐗test[n_idx,:] = 𝐗test[n_idx,:] .- train_mean[n_idx] # same mean as in training, on purpose\n",
    "    𝐗test[n_idx,:] = 𝐗test[n_idx,:] ./ train_var[n_idx] # same variance as in training, on purpose\n",
    "end\n",
    "\n",
    "# Reconstruct full dataset\n",
    "𝐗 = hcat(𝐗train, 𝐗test)\n",
    "S_𝐗 = vcat(S_𝐗train, S_𝐗test)\n",
    "𝐲 = vcat(𝐲train, 𝐲test)\n",
    "\n",
    "# Dimensions\n",
    "D = size(𝐗, 1)\n",
    "N = size(𝐗, 2)\n",
    "Ntrain = size(𝐗train, 2)\n",
    "Ntest = size(𝐗test, 2)\n",
    "\n",
    "### Distance function\n",
    "# Lambda\n",
    "d = (𝐚::Vector, 𝐛::Vector) -> vecnorm(𝐚 - 𝐛) # Euclidean distance\n",
    "# Non-lambda is slightly faster for calculations, but has to be defined\n",
    "# for all processes with @everywhere\n",
    "@everywhere de(𝐚::Vector{Float64}, 𝐛::Vector{Float64}) = vecnorm(𝐚 - 𝐛)\n",
    "\n",
    "### Optimization variables\n",
    "# Main optimization variable, matrix holding the prototype vectors.\n",
    "# Initialized to random Float64 matrix, normal distribution 0 mean 1 variance\n",
    "𝐕 = randn(D, K)\n",
    "# # Weights or \"prototype label predictions\" (probabilities)\n",
    "𝐰 = rand(K) # floats in [0,1)\n",
    "# # Alphas\n",
    "𝛂⁺ = rand(D)\n",
    "𝛂⁻ = rand(D)\n",
    "\n",
    "### Hyperparameters\n",
    "A = Dict(:z=> 1000, :x=> 0.0001, :y=> 0.1)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,-1.3719338843612912,1.0,-1.3719338843612912,2.9171705356545155,-3.3222297674591674)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum(𝐗train), minimum(𝐗train), maximum(𝐗test), minimum(𝐗test), maximum(𝐕), minimum(𝐕)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide training and test sets to groups according to whether the individuals are \"protected\" or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Training:\",(99,10771),(99,21790),\"Test:\",(99,5421),(99,10860))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(𝐗⁺train, 𝐗⁻train) = partition(𝐗train, S_𝐗train)\n",
    "(𝐗⁺test, 𝐗⁻test) = partition(𝐗test, S_𝐗test)\n",
    "\"Training:\",size(𝐗⁺train), size(𝐗⁻train), \"Test:\", size(𝐗⁺test), size(𝐗⁻test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping $\\mathbf{X} \\rightarrow \\mathbf{Z}$\n",
    "Now we can define a mapping from the original dataset $\\mathbf{X}$ to probabilities via the *softmax function*. [Wikipedia](https://en.wikipedia.org/wiki/Softmax_function):\n",
    "> Softmax function \"squashes\" a $K$-dimensional vector $\\mathbf{z}$ of arbitrary real values to a $K$-dimensional vector $\\sigma(\\mathbf{z})$ of real values in the range $[0, 1]$ that add up to 1.\n",
    "\n",
    "Most notably `softmax` returns a probability vector. We will define a modified version that maps a $D$-dimensional vector $\\mathbf{x}$ to a $K$-dimensional vector $\\sigma(\\mathbf{x})$, i.e. the mapping won't necessarily preserve the dimensionality of $\\mathbf{x}$.\n",
    "\n",
    "Also from [Wikipedia](https://en.wikipedia.org/wiki/Multinomial_logistic_regression):\n",
    "> $$\\operatorname{softmax}(k,x_1,\\ldots,x_n) = \\frac{e^{x_k}}{\\sum_{i=1}^n e^{x_i}}$$\n",
    "> is referred to as the [*softmax function*](https://en.wikipedia.org/wiki/Softmax_function).  The reason is that the effect of exponentiating the values $x_1,\\ldots,x_n$ is to exaggerate the differences between them.  As a result, $\\operatorname{softmax}(k,x_1,\\ldots,x_n)$ will return a value close to 0 whenever $x_k$ is significantly less than the maximum of all the values, and will return a value close to 1 when applied to the maximum value, unless it is extremely close to the next-largest value.  Thus, the softmax function can be used to construct a weighted average that behaves as a smooth function (which can be conveniently differentiated, etc.) and which approximates the [indicator function](https://en.wikipedia.org/wiki/Indicator_function).\n",
    "\n",
    "So we define, as in the paper equation (2), $$\\mathbb{P}(Z=k \\mid \\mathbf{x}) = \\frac{e^{-d(\\mathbf{x}, \\mathbf{v}_k)}}{\\sum_{j=1}^K e^{-d(\\mathbf{x}, \\mathbf{v}_j)}}$$\n",
    "where\n",
    "- $\\mathbb{P}(Z=k \\mid \\mathbf{x})$ is described in [definitions](#Definitions)\n",
    "- $\\mathbf{x}$ is the datapoint\n",
    "- $\\mathbf{v}_k$ is a vector associated with the $k$th prototype\n",
    "- $d$ is a distance measure between $\\mathbf{x}$ and $\\mathbf{v}_k$ (e.g. the euclidean distance)\n",
    "\n",
    "This means that since we have replaced $x_k$ in the softmax with the negative distance between $\\mathbf{x}$ and prototype $\\mathbf{v}_k$, the softmax returns a value close to 0 whenever the distance from $\\mathbf{x}$ to the prototype $\\mathbf{v}_k$ is significantly higher than $\\min_{j\\in\\{1,\\dots,K\\}}\\, d(\\mathbf{x}, \\mathbf{v}_i)$, and close to 1 when applied to the minimum value.\n",
    "\n",
    "\"Mapping from X to Z\" in the paper means mapping the vector $\\mathbf{x}$ to a probability vector $\\mathbf{z}$ of length $K$ via the softmax function. These probability vectors are then used directly for training the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In code** this means that $\\mathbb{P}(Z=k \\mid \\mathbf{x})$ is represented by a function taking\n",
    "- the data point $\\mathbf{x}$ which is a `Vector` of length $D$\n",
    "- $(D \\times K)$ `Matrix` of prototypes $\\mathbf{V}$ containing all $K$ prototypes $\\mathbf{v}_k$ i.e. each prototype is a `Vector` (remember, Column-Major order on matrices)\n",
    "- a distance measure on $\\mathbf{X}$\n",
    "\n",
    "and returning\n",
    "- a `Vector` $\\mathbf{z}$ of length $K$ representing a [probability vector](https://en.wikipedia.org/wiki/Probability_vector), where each value $z_k$ of the probability vector $\\mathbf{z}$ tells how probable it is that $\\mathbf{x}$ maps to $\\mathbf{v}_k$. Since $\\mathbf{z}$ is a probability vector, $\\sum_{i=k}^K z_k = 1$.\n",
    "\n",
    "We will name this function `softmax` and define it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_alpha (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: clean up duplicate code\n",
    "\n",
    "### FOR VECTORS; give in 𝐱, get 𝐳\n",
    "\n",
    "# This is same as Eq (2) in paper.\n",
    "function softmax_dist{T<:Number}(𝐱::Vector{T}, 𝐕::Matrix{T}, distanceMeasure::Function)\n",
    "    K = size(𝐕, 2)\n",
    "    res = Vector{Float64}(K)\n",
    "    denominator = Float64(0.0)\n",
    "    # Use one loop to calculate both numerator and denominator\n",
    "    @inbounds for k in 1:K\n",
    "        res[k] = exp(- distanceMeasure(𝐱, 𝐕[:,k]) )\n",
    "        denominator += res[k]\n",
    "    end\n",
    "    denom = inv(denominator)\n",
    "    res .* denom\n",
    "end\n",
    "\n",
    "function softmax_euclidean{T<:Number}(𝐱::Vector{T}, 𝐕::Matrix{T})\n",
    "    K = size(𝐕, 2)\n",
    "    res = Vector{Float64}(K)\n",
    "    denominator = Float64(0.0)\n",
    "    # Use one loop to calculate both numerator and denominator\n",
    "    @inbounds for k in 1:K\n",
    "        res[k] = exp(- vecnorm(𝐱 - 𝐕[:,k]) )\n",
    "        denominator += res[k]\n",
    "    end\n",
    "    denom = inv(denominator)\n",
    "    res .* denom\n",
    "end\n",
    "\n",
    "### FOR MATRICES; give in 𝐗, get 𝐙\n",
    "\n",
    "function softmax_dist{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T}, distanceMeasure::Function)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = Matrix{Float64}(K, N)\n",
    "    @fastmath @inbounds for n in 1:N\n",
    "        res[:,n] = softmax_dist(𝐗[:,n], 𝐕, distanceMeasure)\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "# Parallel version\n",
    "function softmax_dist_par{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T}, distanceMeasure::Function)\n",
    "    #nprocs()==CPU_CORES || addprocs(CPU_CORES-1)    \n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = SharedArray(Float64, (K, N))\n",
    "    @fastmath @sync @parallel for n in 1:N\n",
    "        for k in 1:K\n",
    "            res[k,n] = exp(- distanceMeasure(𝐗[:,n], 𝐕[:,k]) )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "function softmax_euclidean{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = Matrix{Float64}(K, N)\n",
    "    @fastmath @inbounds for n in 1:N\n",
    "        denominator = Float64(0.0)\n",
    "        # Use one loop to calculate both numerator and denominator\n",
    "        for k in 1:K\n",
    "            res[k,n] = exp(- vecnorm(𝐗[:,n] - 𝐕[:,k]) )\n",
    "            denominator += res[k,n]\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(denominator)\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "# Parallel version\n",
    "function softmax_euclidean_par{T<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "    #nprocs()==CPU_CORES || addprocs(CPU_CORES-1)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = SharedArray(Float64, (K, N))\n",
    "    @fastmath @sync @parallel for n in 1:N\n",
    "        for k in 1:K\n",
    "            res[k,n] = exp(- vecnorm(𝐗[:,n] - 𝐕[:,k]) )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "end\n",
    "\n",
    "# Version that accepts alphas, uses the distance function defined in the paper as Eq (12)\n",
    "function softmax_alpha(𝐗::Matrix{Float64}, 𝐕::Matrix{Float64}, 𝛂::Vector{Float64})\n",
    "    localD = length(𝛂)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    # Preallocate result matrix, no need to zero it\n",
    "    res = Matrix{Float64}(K, N)\n",
    "    @fastmath @inbounds @simd for n in 1:N\n",
    "        for k in 1:K\n",
    "            dm::Float64 = zero(Float64)\n",
    "            for i in 1:localD # Eq (12)\n",
    "                dm += 𝛂[i]*((𝐗[i,n]-𝐕[i,k])^2)\n",
    "            end\n",
    "            res[k,n] = exp(- dm )\n",
    "        end\n",
    "        res[:,n] = res[:,n] .* inv(sum(res[:,n]))\n",
    "    end\n",
    "    res\n",
    "    # Slower alternatives for distance calculation, replacing the 1:localD loop:\n",
    "    # return 𝛂 ⋅ ((𝐚-𝐛).^2) # Eq (12)\n",
    "    # return sum(i -> 𝛂[i]*(𝐚[i]-𝐛[i])^2, 1:localD) # Eq (12)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.413046 seconds (8.10 M allocations: 577.593 MB, 14.66% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time for i in 1:25\n",
    "    softmax_alpha(𝐗train, 𝐕, 𝛂⁺);\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For development\n",
    "𝐙pre = softmax_euclidean_par(𝐗train, 𝐕)\n",
    "𝐙pre⁺ = softmax_euclidean_par(𝐗⁺train, 𝐕)\n",
    "𝐙pre⁻ = softmax_euclidean_par(𝐗⁻train, 𝐕);\n",
    "\n",
    "𝐙preu = sdata(𝐙pre)\n",
    "𝐙preu⁺ = sdata(𝐙pre⁺)\n",
    "𝐙preu⁻ = sdata(𝐙pre⁻);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is akin to using a funky [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) to \"predict the prototype\" (category) where a data point $\\mathbf{x}$ maps to.\n",
    "Wikipedia:\n",
    "> These are all statistical classification problems. They all have in common a dependent variable to be predicted that comes from one of a limited set of items which cannot be meaningfully ordered, as well as a set of independent variables (also known as features, explanators, etc.), which are used to predict the dependent variable. Multinomial logit regression is a particular solution to the classification problem that assumes that a linear combination of the observed features and some problem-specific parameters can be used to determine the probability of each particular outcome of the dependent variable. The best values of the parameters for a given problem are usually determined from some training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of the optimization objective\n",
    "\n",
    "### $L_z$ &mdash; statistical parity\n",
    "In code, we will denote $L_z$ with function `Lz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lz (generic function with 4 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### \"NAIVE\" VERSION FOR UNDERSTANDING THE IMPLEMENTATION\n",
    "\n",
    "function LzNaive{T<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T}, dist::Function)\n",
    "    # Operate on matrices and take mean from sample dimension N\n",
    "    meanp = mean( softmax_dist_par(𝐗⁺, 𝐕, dist), 2 ) # Eq (6)\n",
    "    meann = mean( softmax_dist_par(𝐗⁻, 𝐕, dist), 2 ) # Similarly for M_k^-\n",
    "    sum(abs(meanp - meann)) # Eq (7), sum is from k=1 to K\n",
    "end\n",
    "\n",
    "### VERSIONS TAKING IN THE DATA SET AND PROTOTYPES\n",
    "# Optionally a distance measure function can be passed as an argument\n",
    "\n",
    "function Lz{T<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T})\n",
    "    return Lz(sdata(softmax_euclidean_par(𝐗⁺, 𝐕)), sdata(softmax_euclidean_par(𝐗⁻, 𝐕)))\n",
    "end\n",
    "\n",
    "function Lz{T<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T}, dist::Function)\n",
    "    return Lz(sdata(softmax_dist_par(𝐗⁺, 𝐕, dist)), sdata(softmax_dist_par(𝐗⁻, 𝐕, dist)))\n",
    "end\n",
    "\n",
    "# # TODO: use parallel version of softmax_dist_alpha?\n",
    "# function Lz{T<:Number,U<:Number}(𝐗⁺::Matrix{T}, 𝐗⁻::Matrix{T}, 𝐕::Matrix{T}, dist::Function, 𝛂::Vector{U})\n",
    "#     return Lz(softmax_dist_alpha(𝐗⁺, 𝐕, dist, 𝛂), softmax_dist_alpha(𝐗⁻, 𝐕, dist, 𝛂))\n",
    "# end\n",
    "\n",
    "### VERSION FOR PRECALCULATED 𝐙⁺ and 𝐙⁻\n",
    "# Note we have only a version for matrices. This is because during performance\n",
    "# testing I noticed that\n",
    "#\n",
    "#   ZZZp = sdata(𝐙shared⁺)\n",
    "#   ZZZn = sdata(𝐙shared⁻)\n",
    "#   Lz(ZZZp, ZZZn)\n",
    "#\n",
    "# is faster than\n",
    "#\n",
    "#   Lz(𝐙shared⁺, 𝐙shared⁻)\n",
    "#\n",
    "# So use the Matrix version always and if necessary lift the matrices out\n",
    "# of the SharedArray with sdata().\n",
    "#\n",
    "# TODO: is there way to make a faster parallel version?\n",
    "\n",
    "function Lz{T<:Number}(𝐙⁺::SharedArray{T,2}, 𝐙⁻::SharedArray{T,2})\n",
    "    Lz(sdata(𝐙⁺), sdata(𝐙⁻))\n",
    "end\n",
    "\n",
    "function Lz{T<:Number}(𝐙⁺::Matrix{T}, 𝐙⁻::Matrix{T})\n",
    "    # Operate on matrices and take mean from sample dimension N\n",
    "    meanp = mean( 𝐙⁺, 2 )[:] # Eq (6)\n",
    "    meann = mean( 𝐙⁻, 2 )[:] # Similarly for M_k^-\n",
    "    sum(abs(meanp - meann)) # Eq (7), sum is from k=1 to K\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06155303297814012,0.061122689374052214,0.061122689374052214)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test (e.g. somewhere between 0.02 and 0.07, depending on 𝐕 randomization)\n",
    "LZtrain1 = Lz(𝐗⁺test, 𝐗⁻test, 𝐕)\n",
    "LZtrain2 = Lz(𝐙pre⁺, 𝐙pre⁻)\n",
    "LZtrain3 = Lz(𝐙preu⁺, 𝐙preu⁻)\n",
    "(LZtrain1, LZtrain2, LZtrain3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.295667 seconds (120.00 k allocations: 9.308 MB)\n"
     ]
    }
   ],
   "source": [
    "@time for i in 1:5000\n",
    "    Lz(𝐙pre⁺, 𝐙pre⁻)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_x$ &mdash; information loss\n",
    "In code, we will denote $L_x$ with function `Lx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Symbols: 𝐗 ⁺ ⁻ ∑ 𝐕 𝐱 𝐲 𝐙 𝐳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note .* elementwise multiplication of softmax_dist() and V, there is no \\cdot in the paper in Eq (9), dot product would return a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lx (generic function with 4 methods)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### NAIVE VERSION FOR UNDERSTANDING THE IMPLEMENTATION\n",
    "\n",
    "function LxNaive{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{U}, dist::Function)\n",
    "    D = size(𝐗, 1)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    𝐗hat = zeros(Float64, (D,N))\n",
    "    sum = Float64(0.0)\n",
    "    for n in 1:N\n",
    "        𝐳_n = softmax_dist(𝐗[:,n], 𝐕, dist) # prob that x_n maps to protos (Array{13,1})\n",
    "        for k in 1:K # Eq (9)\n",
    "            𝐗hat[:,n] = 𝐗hat[:,n] + (𝐳_n[k] * 𝐕[:,k])\n",
    "        end\n",
    "        sum += (𝐗[:,n] - 𝐗hat[:,n]) ⋅ (𝐗[:,n] - 𝐗hat[:,n]) # Eq (8)\n",
    "    end\n",
    "    return sum, 𝐗hat\n",
    "end\n",
    "\n",
    "### VERSIONS TAKING IN THE DATA SET AND PROTOTYPES\n",
    "# Optionally a distance measure function can be passed as an argument\n",
    "\n",
    "function Lx{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{U})\n",
    "    return Lx(softmax_euclidean_par(𝐗, 𝐕), 𝐗, 𝐕)\n",
    "end\n",
    "\n",
    "function Lx{T<:Number,U<:Number}(𝐗::Matrix{T}, 𝐕::Matrix{U}, dist::Function)\n",
    "    return Lx(softmax_dist_par(𝐗, 𝐕, dist), 𝐗, 𝐕)\n",
    "end\n",
    "\n",
    "### VERSION FOR PRECALCULATED 𝐙\n",
    "\n",
    "function Lx{T<:Number}(𝐙::SharedArray{T,2}, 𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "    Lx(sdata(𝐙), 𝐗, 𝐕)\n",
    "end\n",
    "\n",
    "function Lx{T<:Number}(𝐙::Matrix{T}, 𝐗::Matrix{T}, 𝐕::Matrix{T})\n",
    "    𝐗minus𝐗hat = 𝐗 - (𝐕 * 𝐙) # Eq (8) and (9) combined\n",
    "    return vecdot(𝐗minus𝐗hat, 𝐗minus𝐗hat) # \"simple squared error\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(483529.9819632077,483529.9819632077,483529.9819632077)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test (e.g. 423475.8479283692)\n",
    "LXtrain1 = Lx(𝐗train, 𝐕, d)\n",
    "LXtrain2 = Lx(𝐙pre, 𝐗train, 𝐕)\n",
    "LXtrain3 = Lx(𝐙preu, 𝐗train, 𝐕)\n",
    "(LXtrain1, LXtrain2, LXtrain3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.080333 seconds (450 allocations: 2.402 GB, 24.92% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time for i in 1:50\n",
    "    Lx(𝐙pre, 𝐗train, 𝐕)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L_y$ &mdash; prediction accuracy\n",
    "In code, we will denote $L_y$ with function `Ly`.\n",
    "\n",
    "Essentially here we are letting the optimization pick both the prototypes (i.e. feature vectors) and their predictions (i.e. labels), and the predictions don't have to be discrete 0 and 1, but can be from the range $[0,1]$ and thus themselves can be viewed as probabilities. E.g. let's say that for prototype $v_k$ it's prediction $w_k = 0.82$, then \"there is a 82% chance prototype $\\mathbf{v}_k$ gets label 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ly (generic function with 4 methods)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### NAIVE VERSION TO HELP UNDERSTAND THE IMPLEMENTATION\n",
    "\n",
    "function LyNaive{T1<:Number,T2<:Number,T3<:Number}(\n",
    "        𝐗::Matrix{T1}, 𝐕::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}, dist::Function\n",
    "    )\n",
    "    D = size(𝐗, 1)\n",
    "    N = size(𝐗, 2)\n",
    "    K = size(𝐕, 2)\n",
    "    𝐲hat = zeros(Float64, N)\n",
    "    sum = Float64(0.0)\n",
    "    # Replace 𝐲hat in Eq (10) with Eq (11), then you get this for loop\n",
    "    for n in 1:N\n",
    "        𝐙_n = softmax_dist(𝐗[:,n], 𝐕, dist) # Vector of length K\n",
    "        for k in 1:K\n",
    "            𝐲hat[n] = 𝐲hat[n] + (𝐙_n[k] * 𝐰[k])\n",
    "        end\n",
    "        # The following line could be replaced with\n",
    "        # if 𝐲[n] == 1\n",
    "        #    sum -= log(𝐲hat[n])\n",
    "        # else # 𝐲[n] == 0\n",
    "        #    sum -= log(1 - 𝐲hat[n])\n",
    "        # end\n",
    "        sum += -𝐲[n] * log(𝐲hat[n])  -  (1 - 𝐲[n]) * log(1 - 𝐲hat[n])\n",
    "    end\n",
    "    #return sum, 𝐲hat\n",
    "    return sum\n",
    "end\n",
    "\n",
    "### VERSIONS TAKING IN THE DATA SET AND PROTOTYPES\n",
    "# Optionally a distance measure function can be passed as an argument\n",
    "\n",
    "function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "        𝐗::Matrix{T1}, 𝐕::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}\n",
    "    )\n",
    "    # 𝐙 = softmax_euclidean_par(𝐗, 𝐕)\n",
    "    return Ly(softmax_euclidean_par(𝐗, 𝐕), 𝐲, 𝐰)\n",
    "end\n",
    "\n",
    "function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "        𝐗::Matrix{T1}, 𝐕::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}, dist::Function\n",
    "    )\n",
    "    # 𝐙 = softmax_dist_par(𝐗, 𝐕, dist)\n",
    "    return Ly(softmax_dist_par(𝐗, 𝐕, dist), 𝐲, 𝐰)\n",
    "end\n",
    "\n",
    "### VERSION FOR PRECALCULATED 𝐙\n",
    "\n",
    "# function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "#         𝐙::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3}\n",
    "#     )\n",
    "#     # Copy to shared memory\n",
    "#     𝐙shared = convert(SharedArray{T1, 2}, 𝐙)\n",
    "#     return Ly(𝐙shared, 𝐲, 𝐰)\n",
    "# end\n",
    "\n",
    "# Slower:\n",
    "# function Ly{T1<:Number,T2<:Number,T3<:Number}(\n",
    "#         𝐙::SharedArray{T1,2}, 𝐲::Vector{T2}, 𝐰::Vector{T3}\n",
    "#     )\n",
    "#     N = size(𝐙, 2)\n",
    "#     # Keep 𝐙 as SharedArray, will be faster than taking sdata() when fed to the following @parallel loop\n",
    "#     sum = @parallel (+) for n in 1:N # Eq (10)\n",
    "#         yhat_n = 𝐙[:,n] ⋅ 𝐰 # Eq (11)\n",
    "#         - 𝐲[n] * log(yhat_n) - (1 - 𝐲[n]) * log(1 - yhat_n)\n",
    "#     end\n",
    "#     return sum\n",
    "# end\n",
    "\n",
    "function Ly{T1<:Number,T2<:Number,T3<:Number}(𝐙::SharedArray{T1,2}, 𝐲::Vector{T2}, 𝐰::Vector{T3})\n",
    "    Ly(sdata(𝐙), 𝐲, 𝐰)\n",
    "end\n",
    "\n",
    "function Ly{T1<:Number,T2<:Number,T3<:Number}(𝐙::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3})\n",
    "    N = size(𝐙, 2)\n",
    "    sum = zero(Float64)\n",
    "    𝐲hat = 𝐰' * 𝐙 # Eq (11)\n",
    "    𝐲hat = reshape(𝐲hat, length(𝐲hat))\n",
    "    for n in 1:N # Eq (10)\n",
    "        sum += - 𝐲[n] * log(𝐲hat[n]) - (1 - 𝐲[n]) * log(1 - 𝐲hat[n])\n",
    "    end\n",
    "    return sum\n",
    "end\n",
    "\n",
    "# Slower:\n",
    "# function Ly2{T1<:Number,T2<:Number,T3<:Number}(𝐙::Matrix{T1}, 𝐲::Vector{T2}, 𝐰::Vector{T3})\n",
    "#     tsup = (𝐰' * 𝐙) # # Eq (10) and (11)\n",
    "#     tsup = reshape(tsup, length(tsup))\n",
    "#     return -dot(𝐲, log(tsup)) + -dot(1 - 𝐲, log(1 - tsup))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26532.30050221933,26532.30050221933,26532.30050221933)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test (e.g. 20572.03833866735)\n",
    "LYtrain1 = Ly(𝐗train, 𝐕, 𝐲train, 𝐰)\n",
    "LYtrain2 = Ly(𝐙pre, 𝐲train, 𝐰)\n",
    "LYtrain3 = Ly(𝐙preu, 𝐲train, 𝐰)\n",
    "(LYtrain1, LYtrain2, LYtrain3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.372801 seconds (39.07 M allocations: 645.966 MB, 12.61% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time for i in 1:200\n",
    "    Ly(𝐙pre, 𝐲train, 𝐰)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective_pre_alpha (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall objective function\n",
    "objective_euclidean(𝐗, 𝐗⁺, 𝐗⁻, 𝐕, 𝐲, 𝐰, A) = A[:z]*Lz(𝐗⁺, 𝐗⁻, 𝐕) + A[:x]*Lx(𝐗, 𝐕) + A[:y]*Ly(𝐗, 𝐕, 𝐲, 𝐰)\n",
    "objective_dist(𝐗, 𝐗⁺, 𝐗⁻, 𝐕, 𝐲, 𝐰, A, dist) = A[:z]*Lz(𝐗⁺, 𝐗⁻, 𝐕, dist) + A[:x]*Lx(𝐗, 𝐕, dist) + A[:y]*Ly(𝐗, 𝐕, 𝐲, 𝐰, dist)\n",
    "function objective_pre(𝐗::Matrix, S::Vector{Bool}, 𝐕::Matrix, 𝐲::Vector, 𝐰::Vector, A::Dict)\n",
    "    # Calculate 𝐙 and partitions once\n",
    "    𝐙 = softmax_euclidean_par(𝐗, 𝐕)\n",
    "    (𝐙⁺, 𝐙⁻) = partition(𝐙, S)\n",
    "    # Use functions that accept precalculated 𝐙\n",
    "    return A[:z]*Lz(𝐙⁺, 𝐙⁻) + A[:x]*Lx(𝐙, 𝐗, 𝐕) + A[:y]*Ly(𝐙, 𝐲, 𝐰)\n",
    "end\n",
    "function objective_pre_alpha(𝐗::Matrix, S::Vector{Bool}, 𝐕::Matrix, 𝐲::Vector, 𝐰::Vector, A::Dict, 𝛂⁺::Vector, 𝛂⁻::Vector)\n",
    "    # Calculate 𝐙 and partitions\n",
    "    (𝐗⁺, 𝐗⁻) = partition(𝐗, S)\n",
    "    (𝐲⁺, 𝐲⁻) = partition(𝐲, S)\n",
    "    𝐙⁺ = softmax_alpha(𝐗⁺, 𝐕, 𝛂⁺)\n",
    "    𝐙⁻ = softmax_alpha(𝐗⁻, 𝐕, 𝛂⁻)\n",
    "    # Use functions that accept precalculated 𝐙\n",
    "    return A[:z]*Lz(𝐙⁺, 𝐙⁻) + A[:x]*Lx(𝐙⁺, 𝐗⁺, 𝐕) + A[:x]*Lx(𝐙⁻, 𝐗⁻, 𝐕) + A[:y]*Ly(𝐙⁺, 𝐲⁺, 𝐰) + A[:y]*Ly(𝐙⁻, 𝐲⁻, 𝐰)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2762.705737792306"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_euclidean(𝐗train, 𝐗⁺train, 𝐗⁻train, 𝐕, 𝐲train, 𝐰, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2762.705737792306"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_pre(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3168.8216791570753"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_pre_alpha(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1415.5589625439623"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_euclidean(𝐗test, 𝐗⁺test, 𝐗⁻test, 𝐕, 𝐲test, 𝐰, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.815536 seconds (2.79 M allocations: 588.613 MB, 2.29% gc time)\n",
      "  2.133098 seconds (2.17 M allocations: 587.456 MB, 5.32% gc time)\n",
      "  1.370461 seconds (5.16 M allocations: 1.008 GB, 36.67% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time for i in 1:10\n",
    "    objective_euclidean(𝐗train, 𝐗⁺train, 𝐗⁻train, 𝐕, 𝐲train, 𝐰, A)\n",
    "end\n",
    "@time for i in 1:10\n",
    "    objective_pre(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A)\n",
    "end\n",
    "@time for i in 1:10\n",
    "    objective_pre_alpha(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function fun_to_profile(n::Int)\n",
    "   for i in 1:n\n",
    "        objective_pre_alpha(𝐗train, S_𝐗train, 𝐕, 𝐲train, 𝐰, A, 𝛂⁺, 𝛂⁻)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Profiling\n",
    "profiling_logfile = \"profile.bin\"\n",
    "function benchmark()\n",
    "    # Any setup code goes here.\n",
    "\n",
    "    # Run once, to force compilation.\n",
    "    println(\"======================= First run:\")\n",
    "    srand(666)\n",
    "    @time fun_to_profile(1)\n",
    "\n",
    "    # Run a second time, with profiling.\n",
    "    println(\"\\n\\n======================= Second run:\")\n",
    "    srand(666)\n",
    "    Profile.init(delay=0.01)\n",
    "    Profile.clear()\n",
    "    Profile.clear_malloc_data()\n",
    "    @profile @time fun_to_profile(50)\n",
    "    \n",
    "    Profile.print()\n",
    "\n",
    "#     # Write profile results to profile.bin.\n",
    "#     r = Profile.retrieve()\n",
    "#     f = open(profiling_logfile, \"w\")\n",
    "#     serialize(f, r)\n",
    "#     close(f)\n",
    "end\n",
    "\n",
    "# function show_profiling()\n",
    "#     f = open(profiling_logfile)\n",
    "#     r = deserialize(f);\n",
    "#     ProfileView.view(r[1], lidict=r[2])\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show_profiling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test optimization run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obj_func (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0 # keep track of # function evaluations\n",
    "#function obj_func(x::Vector, grad::Vector)\n",
    "function obj_func(x::Vector)\n",
    "    # Increase count\n",
    "    global count\n",
    "    count::Int += 1\n",
    "    \n",
    "    # Print progress\n",
    "    if count % 50 == 0\n",
    "        @printf(\"Round %d\\n\", count)\n",
    "    end\n",
    "    \n",
    "    # Get 𝐕, 𝐰, 𝛂⁺, 𝛂⁻\n",
    "    loc𝐕 = reshape(x[1:length(𝐕)], size(𝐕)) # Uses the global 𝐕 for size\n",
    "    cursor = length(𝐕)\n",
    "    loc𝛂⁺ = x[cursor+1:cursor+length(𝛂⁺)]\n",
    "    cursor += length(𝛂⁺)\n",
    "    loc𝛂⁻ = x[cursor+1:cursor+length(𝛂⁻)]\n",
    "    cursor += length(𝛂⁻)\n",
    "    loc𝐰 = x[cursor+1:cursor+length(𝐰)]\n",
    "    #return size(loc𝐕), size(loc𝛂⁺), size(loc𝛂⁻), size(loc𝐰)\n",
    "    return objective_pre_alpha(𝐗train, S_𝐗train, loc𝐕, 𝐲train, loc𝐰, A, loc𝛂⁺, loc𝛂⁻)\n",
    "#     if length(grad) > 0:\n",
    "#         ...set grad to gradient, in-place...\n",
    "#     return ...value of f(x)...\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: "
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "reshape(A, dims)\n",
       "\\end{verbatim}\n",
       "Create an array with the same data as the given array, but with different dimensions. An implementation for a particular type of array may choose whether the data is copied or shared.\n"
      ],
      "text/markdown": [
       "```\n",
       "reshape(A, dims)\n",
       "```\n",
       "\n",
       "Create an array with the same data as the given array, but with different dimensions. An implementation for a particular type of array may choose whether the data is copied or shared.\n"
      ],
      "text/plain": [
       "```\n",
       "reshape(A, dims)\n",
       "```\n",
       "\n",
       "Create an array with the same data as the given array, but with different dimensions. An implementation for a particular type of array may choose whether the data is copied or shared.\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshape promote_shape\n",
      "\n"
     ]
    }
   ],
   "source": [
    "?reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99,13),(1,13),(99,),(99,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(𝐕),size(𝐰'),size(𝛂⁺),size(𝛂⁻)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optVarInit = vcat(reshape(𝐕, length(𝐕)), reshape(𝛂⁺, length(𝛂⁺)), reshape(𝛂⁻, length(𝛂⁻)), reshape(𝐰, length(𝐰)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1498,1498,1498)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds_lower = vcat(fill(-Inf, length(𝐕)), zeros(length(𝛂⁺) + length(𝛂⁻) + length(𝐰)));\n",
    "bounds_upper = vcat(fill(+Inf, length(𝐕)), ones(length(𝛂⁺) + length(𝛂⁻) + length(𝐰)));\n",
    "length(optVarInit), length(bounds_lower), length(bounds_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 9000\n",
      "Round 9050\n",
      "Round 9100\n",
      "Round 9150\n",
      "Round 9200\n",
      "Round 9250\n",
      "Round 9300\n",
      "Round 9350\n",
      "Round 9400\n",
      "Round 9450\n",
      "Round 9500\n",
      "Round 9550\n",
      "Round 9600\n",
      "Round 9650\n",
      "Round 9700\n",
      "Round 9750\n",
      "Round 9800\n",
      "Round 9850\n",
      "Round 9900\n",
      "Round 9950\n",
      "Round 10000\n",
      "Round 10050\n",
      "Round 10100\n",
      "Round 10150\n",
      "Round 10200\n",
      "Round 10250\n",
      "Round 10300\n",
      "Round 10350\n",
      "Round 10400\n",
      "Round 10450\n",
      "Round 10500\n",
      "Round 10550\n",
      "Round 10600\n",
      "Round 10650\n",
      "Round 10700\n",
      "Round 10750\n",
      "Round 10800\n",
      "Round 10850\n",
      "Round 10900\n",
      "Round 10950\n",
      "Round 11000\n",
      "Round 11050\n",
      "Round 11100\n",
      "Round 11150\n",
      "Round 11200\n",
      "Round 11250\n",
      "Round 11300\n",
      "Round 11350\n",
      "Round 11400\n",
      "Round 11450\n",
      "Round 11500\n",
      "Round 11550\n",
      "Round 11600\n",
      "Round 11650\n",
      "Round 11700\n",
      "Round 11750\n",
      "Round 11800\n",
      "Round 11850\n",
      "Round 11900\n",
      "Round 11950\n",
      "Round 12000\n",
      "Round 12050\n",
      "Round 12100\n",
      "Round 12150\n",
      "Round 12200\n",
      "Round 12250\n",
      "Round 12300\n",
      "Round 12350\n",
      "Round 12400\n",
      "Round 12450\n",
      "Round 12500\n",
      "Round 12550\n",
      "Round 12600\n",
      "Round 12650\n",
      "Round 12700\n",
      "Round 12750\n",
      "Round 12800\n",
      "Round 12850\n",
      "Round 12900\n",
      "Round 12950\n",
      "Round 13000\n",
      "Round 13050\n",
      "Round 13100\n",
      "Round 13150\n",
      "Round 13200\n",
      "Round 13250\n",
      "Round 13300\n",
      "Round 13350\n",
      "Round 13400\n",
      "Round 13450\n",
      "Round 13500\n",
      "Round 13550\n",
      "Round 13600\n",
      "Round 13650\n",
      "Round 13700\n",
      "Round 13750\n",
      "Round 13800\n",
      "Round 13850\n",
      "Round 13900\n",
      "Round 13950\n",
      "Round 14000\n",
      "Round 14050\n",
      "Round 14100\n",
      "Round 14150\n",
      "Round 14200\n",
      "Round 14250\n",
      "Round 14300\n",
      "Round 14350\n",
      "Round 14400\n",
      "Round 14450\n",
      "Round 14500\n",
      "Round 14550\n",
      "Round 14600\n",
      "Round 14650\n",
      "Round 14700\n",
      "Round 14750\n",
      "Round 14800\n",
      "Round 14850\n",
      "Round 14900\n",
      "Round 14950\n",
      "Round 15000\n",
      "Round 15050\n",
      "Round 15100\n",
      "Round 15150\n",
      "Round 15200\n",
      "Round 15250\n",
      "Round 15300\n",
      "Round 15350\n",
      "Round 15400\n",
      "Round 15450\n",
      "Round 15500\n",
      "Round 15550\n",
      "Round 15600\n",
      "Round 15650\n",
      "Round 15700\n",
      "Round 15750\n",
      "Round 15800\n",
      "Round 15850\n",
      "Round 15900\n",
      "Round 15950\n",
      "Round 16000\n",
      "Round 16050\n",
      "Round 16100\n",
      "Round 16150\n",
      "Round 16200\n",
      "Round 16250\n",
      "Round 16300\n",
      "Round 16350\n",
      "Round 16400\n",
      "Round 16450\n",
      "Round 16500\n",
      "Round 16550\n",
      "Round 16600\n",
      "Round 16650\n",
      "Round 16700\n",
      "Round 16750\n",
      "Round 16800\n",
      "Round 16850\n",
      "Round 16900\n",
      "Round 16950\n",
      "Round 17000\n",
      "Round 17050\n",
      "Round 17100\n",
      "Round 17150\n",
      "Round 17200\n",
      "Round 17250\n",
      "Round 17300\n",
      "Round 17350\n",
      "Round 17400\n",
      "Round 17450\n",
      "Round 17500\n",
      "Round 17550\n",
      "Round 17600\n",
      "Round 17650\n",
      "Round 17700\n",
      "Round 17750\n",
      "Round 17800\n",
      "Round 17850\n",
      "Round 17900\n",
      "Round 17950\n",
      "Round 18000\n",
      "Round 18050\n",
      "Round 18100\n",
      "Round 18150\n",
      "Round 18200\n",
      "Round 18250\n",
      "Round 18300\n",
      "Round 18350\n",
      "Round 18400\n",
      "Round 18450\n",
      "Round 18500\n",
      "Round 18550\n",
      "Round 18600\n",
      "Round 18650\n",
      "Round 18700\n",
      "Round 18750\n",
      "Round 18800\n",
      "Round 18850\n",
      "Round 18900\n",
      "Round 18950\n",
      "Round 19000\n",
      "Round 19050\n",
      "Round 19100\n",
      "Round 19150\n",
      "Round 19200\n",
      "Round 19250\n",
      "Round 19300\n",
      "Round 19350\n",
      "Round 19400\n",
      "Round 19450\n",
      "Round 19500\n",
      "Round 19550\n",
      "Round 19600\n",
      "Round 19650\n",
      "Round 19700\n",
      "Round 19750\n",
      "Round 19800\n",
      "Round 19850\n",
      "Round 19900\n",
      "Round 19950\n",
      "Round 20000\n",
      "Round 20050\n",
      "Round 20100\n",
      "Round 20150\n",
      "Round 20200\n",
      "Round 20250\n",
      "Round 20300\n",
      "Round 20350\n",
      "Round 20400\n",
      "Round 20450\n",
      "Round 20500\n",
      "Round 20550\n",
      "Round 20600\n",
      "Round 20650\n",
      "Round 20700\n",
      "Round 20750\n",
      "Round 20800\n",
      "Round 20850\n",
      "Round 20900\n",
      "Round 20950\n",
      "Round 21000\n",
      "Round 21050\n",
      "Round 21100\n",
      "Round 21150\n",
      "Round 21200\n",
      "Round 21250\n",
      "Round 21300\n",
      "Round 21350\n",
      "Round 21400\n",
      "Round 21450\n",
      "Round 21500\n",
      "Round 21550\n",
      "Round 21600\n",
      "Round 21650\n",
      "Round 21700\n",
      "Round 21750\n",
      "Round 21800\n",
      "Round 21850\n",
      "Round 21900\n",
      "Round 21950\n",
      "Round 22000\n",
      "Round 22050\n",
      "Round 22100\n",
      "Round 22150\n",
      "Round 22200\n",
      "Round 22250\n",
      "Round 22300\n",
      "Round 22350\n",
      "Round 22400\n",
      "Round 22450\n",
      "Round 22500\n",
      "Round 22550\n",
      "Round 22600\n",
      "Round 22650\n",
      "Round 22700\n",
      "Round 22750\n",
      "Round 22800\n",
      "Round 22850\n",
      "Round 22900\n",
      "Round 22950\n",
      "Round 23000\n",
      "Round 23050\n",
      "Round 23100\n",
      "Round 23150\n",
      "Round 23200\n",
      "Round 23250\n",
      "Round 23300\n",
      "Round 23350\n",
      "Round 23400\n",
      "Round 23450\n",
      "Round 23500\n",
      "Round 23550\n",
      "Round 23600\n",
      "Round 23650\n",
      "Round 23700\n",
      "Round 23750\n",
      "Round 23800\n",
      "Round 23850\n",
      "Round 23900\n",
      "Round 23950\n",
      "Round 24000\n",
      "Round 24050\n",
      "Round 24100\n",
      "Round 24150\n",
      "Round 24200\n",
      "Round 24250\n",
      "Round 24300\n",
      "Round 24350\n",
      "Round 24400\n",
      "Round 24450\n",
      "Round 24500\n",
      "Round 24550\n",
      "Round 24600\n",
      "Round 24650\n",
      "Round 24700\n",
      "Round 24750\n",
      "Round 24800\n",
      "Round 24850\n",
      "Round 24900\n",
      "Round 24950\n",
      "Round 25000\n",
      "Round 25050\n",
      "Round 25100\n",
      "Round 25150\n",
      "Round 25200\n",
      "Round 25250\n",
      "Round 25300\n",
      "Round 25350\n",
      "Round 25400\n",
      "Round 25450\n",
      "Round 25500\n",
      "Round 25550\n",
      "Round 25600\n",
      "Round 25650\n",
      "Round 25700\n",
      "Round 25750\n",
      "Round 25800\n",
      "Round 25850\n",
      "Round 25900\n",
      "Round 25950\n",
      "Round 26000\n",
      "Round 26050\n",
      "Round 26100\n",
      "Round 26150\n",
      "Round 26200\n",
      "Round 26250\n",
      "Round 26300\n",
      "Round 26350\n",
      "Round 26400\n",
      "Round 26450\n",
      "Round 26500\n",
      "Round 26550\n",
      "Round 26600\n",
      "Round 26650\n",
      "Round 26700\n",
      "Round 26750\n",
      "Round 26800\n",
      "Round 26850\n",
      "Round 26900\n",
      "Round 26950\n",
      "Round 27000\n",
      "Round 27050\n",
      "Round 27100\n",
      "Round 27150\n",
      "Round 27200\n",
      "Round 27250\n",
      "Round 27300\n",
      "Round 27350\n",
      "Round 27400\n",
      "Round 27450\n",
      "Round 27500\n",
      "Round 27550\n",
      "Round 27600\n",
      "Round 27650\n",
      "Round 27700\n",
      "Round 27750\n",
      "Round 27800\n",
      "Round 27850\n",
      "Round 27900\n",
      "Round 27950\n",
      "Round 28000\n",
      "Round 28050\n",
      "Round 28100\n",
      "Round 28150\n",
      "Round 28200\n",
      "Round 28250\n",
      "Round 28300\n",
      "Round 28350\n",
      "Round 28400\n",
      "Round 28450\n",
      "Round 28500\n",
      "Round 28550\n",
      "Round 28600\n",
      "Round 28650\n",
      "Round 28700\n",
      "Round 28750\n",
      "Round 28800\n",
      "Round 28850\n",
      "Round 28900\n",
      "Round 28950\n",
      "Round 29000\n",
      "Round 29050\n",
      "Round 29100\n",
      "Round 29150\n",
      "Round 29200\n",
      "Round 29250\n",
      "Round 29300\n",
      "Round 29350\n",
      "Round 29400\n",
      "Round 29450\n",
      "Round 29500\n",
      "Round 29550\n",
      "Round 29600\n",
      "Round 29650\n",
      "Round 29700\n",
      "Round 29750\n",
      "Round 29800\n",
      "Round 29850\n",
      "Round 29900\n",
      "Round 29950\n",
      "Round 30000\n",
      "Round 30050\n",
      "Round 30100\n",
      "Round 30150\n",
      "Round 30200\n",
      "Round 30250\n",
      "Round 30300\n",
      "Round 30350\n",
      "Round 30400\n",
      "Round 30450\n",
      "Round 30500\n",
      "Round 30550\n",
      "Round 30600\n",
      "Round 30650\n",
      "Round 30700\n",
      "Round 30750\n",
      "Round 30800\n",
      "Round 30850\n",
      "Round 30900\n",
      "Round 30950\n",
      "Round 31000\n",
      "Round 31050\n",
      "Round 31100\n",
      "Round 31150\n",
      "Round 31200\n",
      "Round 31250\n",
      "Round 31300\n",
      "Round 31350\n",
      "Round 31400\n",
      "Round 31450\n",
      "Round 31500\n",
      "Round 31550\n",
      "Round 31600\n",
      "Round 31650\n",
      "Round 31700\n",
      "Round 31750\n",
      "Round 31800\n",
      "Round 31850\n",
      "Round 31900\n",
      "Round 31950\n",
      "Round 32000\n",
      "Round 32050\n",
      "Round 32100\n",
      "Round 32150\n",
      "Round 32200\n",
      "Round 32250\n",
      "Round 32300\n",
      "Round 32350\n",
      "Round 32400\n",
      "Round 32450\n",
      "Round 32500\n",
      "Round 32550\n",
      "Round 32600\n",
      "Round 32650\n",
      "Round 32700\n",
      "Round 32750\n",
      "Round 32800\n",
      "Round 32850\n",
      "Round 32900\n",
      "Round 32950\n",
      "Round 33000\n",
      "Round 33050\n",
      "Round 33100\n",
      "Round 33150\n",
      "Round 33200\n",
      "Round 33250\n",
      "Round 33300\n",
      "Round 33350\n",
      "Round 33400\n",
      "Round 33450\n",
      "Round 33500\n",
      "Round 33550\n",
      "Round 33600\n",
      "Round 33650\n",
      "Round 33700\n",
      "Round 33750\n",
      "Round 33800\n",
      "Round 33850\n",
      "Round 33900\n",
      "Round 33950\n",
      "Round 34000\n",
      "Round 34050\n",
      "Round 34100\n",
      "Round 34150\n",
      "Round 34200\n",
      "Round 34250\n",
      "Round 34300\n",
      "Round 34350\n",
      "Round 34400\n",
      "Round 34450\n",
      "Round 34500\n",
      "Round 34550\n",
      "Round 34600\n",
      "Round 34650\n",
      "Round 34700\n",
      "Round 34750\n",
      "Round 34800\n",
      "Round 34850\n",
      "Round 34900\n",
      "Round 34950\n",
      "Round 35000\n",
      "Round 35050\n",
      "Round 35100\n",
      "Round 35150\n",
      "Round 35200\n",
      "Round 35250\n",
      "Round 35300\n",
      "Round 35350\n",
      "Round 35400\n",
      "Round 35450\n",
      "Round 35500\n",
      "Round 35550\n",
      "Round 35600\n",
      "Round 35650\n",
      "Round 35700\n",
      "Round 35750\n",
      "Round 35800\n",
      "Round 35850\n",
      "Round 35900\n",
      "Round 35950\n",
      "Round 36000\n",
      "Round 36050\n",
      "Round 36100\n",
      "Round 36150\n",
      "Round 36200\n",
      "Round 36250\n",
      "Round 36300\n",
      "Round 36350\n",
      "Round 36400\n",
      "Round 36450\n",
      "Round 36500\n",
      "Round 36550\n",
      "Round 36600\n",
      "Round 36650\n",
      "Round 36700\n",
      "Round 36750\n",
      "Round 36800\n",
      "Round 36850\n",
      "Round 36900\n",
      "Round 36950\n",
      "Round 37000\n",
      "Round 37050\n",
      "Round 37100\n",
      "Round 37150\n",
      "Round 37200\n",
      "Round 37250\n",
      "Round 37300\n",
      "Round 37350\n",
      "Round 37400\n",
      "Round 37450\n",
      "Round 37500\n",
      "Round 37550\n",
      "Round 37600\n",
      "Round 37650\n",
      "Round 37700\n",
      "Round 37750\n",
      "Round 37800\n",
      "Round 37850\n",
      "Round 37900\n",
      "Round 37950\n",
      "Round 38000\n",
      "Round 38050\n",
      "Round 38100\n",
      "Round 38150\n",
      "Round 38200\n",
      "Round 38250\n",
      "Round 38300\n",
      "Round 38350\n",
      "Round 38400\n",
      "Round 38450\n",
      "Round 38500\n",
      "Round 38550\n",
      "Round 38600\n",
      "Round 38650\n",
      "Round 38700\n",
      "Round 38750\n",
      "Round 38800\n",
      "Round 38850\n",
      "Round 38900\n",
      "Round 38950\n",
      "Round 39000\n",
      "Round 39050\n",
      "Round 39100\n",
      "Round 39150\n",
      "Round 39200\n",
      "Round 39250\n",
      "Round 39300\n",
      "Round 39350\n",
      "Round 39400\n",
      "Round 39450\n",
      "Round 39500\n",
      "Round 39550\n",
      "Round 39600\n",
      "Round 39650\n",
      "Round 39700\n",
      "Round 39750\n",
      "Round 39800\n",
      "Round 39850\n",
      "Round 39900\n",
      "Round 39950\n",
      "Round 40000\n",
      "Round 40050\n",
      "Round 40100\n",
      "Round 40150\n",
      "Round 40200\n",
      "Round 40250\n",
      "Round 40300\n",
      "Round 40350\n",
      "Round 40400\n",
      "Round 40450\n",
      "Round 40500\n",
      "Round 40550\n",
      "Round 40600\n",
      "Round 40650\n",
      "Round 40700\n",
      "Round 40750\n",
      "Round 40800\n",
      "Round 40850\n",
      "Round 40900\n",
      "Round 40950\n",
      "Round 41000\n",
      "Round 41050\n",
      "Round 41100\n",
      "Round 41150\n",
      "Round 41200\n",
      "Round 41250\n",
      "Round 41300\n",
      "Round 41350\n",
      "Round 41400\n",
      "Round 41450\n",
      "Round 41500\n",
      "Round 41550\n",
      "Round 41600\n",
      "Round 41650\n",
      "Round 41700\n",
      "Round 41750\n",
      "Round 41800\n",
      "Round 41850\n",
      "Round 41900\n",
      "Round 41950\n",
      "Round 42000\n",
      "Round 42050\n",
      "Round 42100\n",
      "Round 42150\n",
      "Round 42200\n",
      "Round 42250\n",
      "Round 42300\n",
      "Round 42350\n",
      "Round 42400\n",
      "Round 42450\n",
      "Round 42500\n",
      "Round 42550\n",
      "Round 42600\n",
      "Round 42650\n",
      "Round 42700\n",
      "Round 42750\n",
      "Round 42800\n",
      "Round 42850\n",
      "Round 42900\n",
      "Round 42950\n",
      "Round 43000\n",
      "Round 43050\n",
      "Round 43100\n",
      "Round 43150\n",
      "Round 43200\n",
      "Round 43250\n",
      "Round 43300\n",
      "Round 43350\n",
      "Round 43400\n",
      "Round 43450\n",
      "Round 43500\n",
      "Round 43550\n",
      "Round 43600\n",
      "Round 43650\n",
      "Round 43700\n",
      "Round 43750\n",
      "Round 43800\n",
      "Round 43850\n",
      "Round 43900\n",
      "Round 43950\n",
      "Round 44000\n",
      "Round 44050\n",
      "Round 44100\n",
      "Round 44150\n",
      "Round 44200\n",
      "Round 44250\n",
      "Round 44300\n",
      "Round 44350\n",
      "Round 44400\n",
      "Round 44450\n",
      "Round 44500\n",
      "Round 44550\n",
      "Round 44600\n",
      "Round 44650\n",
      "Round 44700\n",
      "Round 44750\n",
      "Round 44800\n",
      "Round 44850\n",
      "Round 44900\n",
      "Round 44950\n",
      "Round 45000\n",
      "Round 45050\n",
      "Round 45100\n",
      "Round 45150\n",
      "Round 45200\n",
      "Round 45250\n",
      "Round 45300\n",
      "Round 45350\n",
      "Round 45400\n",
      "Round 45450\n",
      "Round 45500\n",
      "Round 45550\n",
      "Round 45600\n",
      "Round 45650\n",
      "Round 45700\n",
      "Round 45750\n",
      "Round 45800\n",
      "Round 45850\n",
      "Round 45900\n",
      "Round 45950\n",
      "Round 46000\n",
      "Round 46050\n",
      "Round 46100\n",
      "Round 46150\n",
      "Round 46200\n",
      "Round 46250\n",
      "Round 46300\n",
      "Round 46350\n",
      "Round 46400\n",
      "Round 46450\n",
      "Round 46500\n",
      "Round 46550\n",
      "Round 46600\n",
      "Round 46650\n",
      "Round 46700\n",
      "Round 46750\n",
      "Round 46800\n",
      "Round 46850\n",
      "Round 46900\n",
      "Round 46950\n",
      "Round 47000\n",
      "Round 47050\n",
      "Round 47100\n",
      "Round 47150\n",
      "Round 47200\n",
      "Round 47250\n",
      "Round 47300\n",
      "Round 47350\n",
      "Round 47400\n",
      "Round 47450\n",
      "Round 47500\n",
      "Round 47550\n",
      "Round 47600\n",
      "Round 47650\n",
      "Round 47700\n",
      "Round 47750\n",
      "Round 47800\n",
      "Round 47850\n",
      "Round 47900\n",
      "Round 47950\n",
      "Round 48000\n",
      "Round 48050\n",
      "Round 48100\n",
      "Round 48150\n",
      "Round 48200\n",
      "Round 48250\n",
      "Round 48300\n",
      "Round 48350\n",
      "Round 48400\n",
      "Round 48450\n",
      "Round 48500\n",
      "Round 48550\n",
      "Round 48600\n",
      "Round 48650\n",
      "Round 48700\n",
      "Round 48750\n",
      "Round 48800\n",
      "Round 48850\n",
      "Round 48900\n",
      "Round 48950\n",
      "Round 49000\n",
      "Round 49050\n",
      "Round 49100\n",
      "Round 49150\n",
      "Round 49200\n",
      "Round 49250\n",
      "Round 49300\n",
      "Round 49350\n",
      "Round 49400\n",
      "Round 49450\n",
      "Round 49500\n",
      "Round 49550\n",
      "Round 49600\n",
      "Round 49650\n",
      "Round 49700\n",
      "Round 49750\n",
      "Round 49800\n",
      "Round 49850\n",
      "Round 49900\n",
      "Round 49950\n",
      "Round 50000\n",
      "Round 50050\n",
      "Round 50100\n",
      "Round 50150\n",
      "Round 50200\n",
      "Round 50250\n",
      "Round 50300\n",
      "Round 50350\n",
      "Round 50400\n",
      "Round 50450\n",
      "Round 50500\n",
      "Round 50550\n",
      "Round 50600\n",
      "Round 50650\n",
      "Round 50700\n",
      "Round 50750\n",
      "Round 50800\n",
      "Round 50850\n",
      "Round 50900\n",
      "Round 50950\n",
      "Round 51000\n",
      "Round 51050\n",
      "Round 51100\n",
      "Round 51150\n",
      "Round 51200\n",
      "Round 51250\n",
      "Round 51300\n",
      "Round 51350\n",
      "Round 51400\n",
      "Round 51450\n",
      "Round 51500\n",
      "Round 51550\n",
      "Round 51600\n",
      "Round 51650\n",
      "Round 51700\n",
      "Round 51750\n",
      "Round 51800\n",
      "Round 51850\n",
      "Round 51900\n",
      "Round 51950\n",
      "Round 52000\n",
      "Round 52050\n",
      "Round 52100\n",
      "Round 52150\n",
      "Round 52200\n",
      "Round 52250\n",
      "Round 52300\n",
      "Round 52350\n",
      "Round 52400\n",
      "Round 52450\n",
      "Round 52500\n",
      "Round 52550\n",
      "Round 52600\n",
      "Round 52650\n",
      "Round 52700\n",
      "Round 52750\n",
      "Round 52800\n",
      "Round 52850\n",
      "Round 52900\n",
      "Round 52950\n",
      "Round 53000\n",
      "Round 53050\n",
      "Round 53100\n",
      "Round 53150\n",
      "Round 53200\n",
      "Round 53250\n",
      "Round 53300\n",
      "Round 53350\n",
      "Round 53400\n",
      "Round 53450\n",
      "Round 53500\n",
      "Round 53550\n",
      "Round 53600\n",
      "Round 53650\n",
      "Round 53700\n",
      "Round 53750\n",
      "Round 53800\n",
      "Round 53850\n",
      "Round 53900\n",
      "Round 53950\n",
      "Round 54000\n",
      "Round 54050\n",
      "Round 54100\n",
      "Round 54150\n",
      "Round 54200\n",
      "Round 54250\n",
      "Round 54300\n",
      "Round 54350\n",
      "Round 54400\n",
      "Round 54450\n",
      "Round 54500\n",
      "Round 54550\n",
      "Round 54600\n",
      "Round 54650\n",
      "Round 54700\n",
      "Round 54750\n",
      "Round 54800\n",
      "Round 54850\n",
      "Round 54900\n",
      "Round 54950\n",
      "Round 55000\n",
      "Round 55050\n",
      "Round 55100\n",
      "Round 55150\n",
      "Round 55200\n",
      "Round 55250\n",
      "Round 55300\n",
      "Round 55350\n",
      "Round 55400\n",
      "Round 55450\n",
      "Round 55500\n",
      "Round 55550\n",
      "Round 55600\n",
      "Round 55650\n",
      "Round 55700\n",
      "Round 55750\n",
      "Round 55800\n",
      "Round 55850\n",
      "Round 55900\n",
      "Round 55950\n",
      "Round 56000\n",
      "Round 56050\n",
      "Round 56100\n",
      "Round 56150\n",
      "Round 56200\n",
      "Round 56250\n",
      "Round 56300\n",
      "Round 56350\n",
      "Round 56400\n",
      "Round 56450\n",
      "Round 56500\n",
      "Round 56550\n",
      "Round 56600\n",
      "Round 56650\n",
      "Round 56700\n",
      "Round 56750\n",
      "Round 56800\n",
      "Round 56850\n",
      "Round 56900\n",
      "Round 56950\n",
      "Round 57000\n",
      "Round 57050\n",
      "Round 57100\n",
      "Round 57150\n",
      "Round 57200\n",
      "Round 57250\n",
      "Round 57300\n",
      "Round 57350\n",
      "Round 57400\n",
      "Round 57450\n",
      "Round 57500\n",
      "Round 57550\n",
      "Round 57600\n",
      "Round 57650\n",
      "Round 57700\n",
      "Round 57750\n",
      "Round 57800\n",
      "Round 57850\n",
      "Round 57900\n",
      "Round 57950\n",
      "Round 58000\n",
      "Round 58050\n",
      "Round 58100\n",
      "Round 58150\n",
      "Round 58200\n",
      "Round 58250\n",
      "Round 58300\n",
      "Round 58350\n",
      "Round 58400\n",
      "Round 58450\n",
      "Round 58500\n",
      "Round 58550\n",
      "Round 58600\n",
      "Round 58650\n",
      "Round 58700\n",
      "Round 58750\n",
      "Round 58800\n",
      "Round 58850\n",
      "Round 58900\n",
      "Round 58950\n",
      "Round 59000\n",
      "Round 59050\n",
      "Round 59100\n",
      "Round 59150\n",
      "Round 59200\n",
      "Round 59250\n",
      "Round 59300\n",
      "Round 59350\n",
      "Round 59400\n",
      "Round 59450\n",
      "Round 59500\n",
      "Round 59550\n",
      "Round 59600\n",
      "Round 59650\n",
      "Round 59700\n",
      "Round 59750\n",
      "Round 59800\n",
      "Round 59850\n",
      "Round 59900\n",
      "Round 59950\n",
      "Round 60000\n",
      "Round 60050\n",
      "Round 60100\n",
      "Round 60150\n",
      "Round 60200\n",
      "Round 60250\n",
      "Round 60300\n",
      "Round 60350\n",
      "Round 60400\n",
      "Round 60450\n",
      "Round 60500\n",
      "Round 60550\n",
      "Round 60600\n",
      "Round 60650\n",
      "Round 60700\n",
      "Round 60750\n",
      "Round 60800\n",
      "Round 60850\n",
      "Round 60900\n",
      "Round 60950\n",
      "Round 61000\n",
      "Round 61050\n",
      "Round 61100\n",
      "Round 61150\n",
      "Round 61200\n",
      "Round 61250\n",
      "Round 61300\n",
      "Round 61350\n",
      "Round 61400\n",
      "Round 61450\n",
      "Round 61500\n",
      "Round 61550\n",
      "Round 61600\n",
      "Round 61650\n",
      "Round 61700\n",
      "Round 61750\n",
      "Round 61800\n",
      "Round 61850\n",
      "Round 61900\n",
      "Round 61950\n",
      "Round 62000\n",
      "Round 62050\n",
      "Round 62100\n",
      "Round 62150\n",
      "Round 62200\n",
      "Round 62250\n",
      "Round 62300\n",
      "Round 62350\n",
      "Round 62400\n",
      "Round 62450\n",
      "Round 62500\n",
      "Round 62550\n",
      "Round 62600\n",
      "Round 62650\n",
      "Round 62700\n",
      "Round 62750\n",
      "Round 62800\n",
      "Round 62850\n",
      "Round 62900\n",
      "Round 62950\n",
      "Round 63000\n",
      "Round 63050\n",
      "Round 63100\n",
      "Round 63150\n",
      "Round 63200\n",
      "Round 63250\n",
      "Round 63300\n",
      "Round 63350\n",
      "Round 63400\n",
      "Round 63450\n",
      "Round 63500\n",
      "Round 63550\n",
      "Round 63600\n",
      "Round 63650\n",
      "Round 63700\n",
      "Round 63750\n",
      "Round 63800\n",
      "Round 63850\n",
      "Round 63900\n",
      "Round 63950\n",
      "Round 64000\n",
      "Round 64050\n",
      "Round 64100\n",
      "Round 64150\n",
      "Round 64200\n",
      "Round 64250\n",
      "Round 64300\n",
      "Round 64350\n",
      "Round 64400\n",
      "Round 64450\n",
      "Round 64500\n",
      "Round 64550\n",
      "Round 64600\n",
      "Round 64650\n",
      "Round 64700\n",
      "Round 64750\n",
      "Round 64800\n",
      "Round 64850\n",
      "Round 64900\n",
      "Round 64950\n",
      "Round 65000\n",
      "Round 65050\n",
      "Round 65100\n",
      "Round 65150\n",
      "Round 65200\n",
      "Round 65250\n",
      "Round 65300\n",
      "Round 65350\n",
      "Round 65400\n",
      "Round 65450\n",
      "Round 65500\n",
      "Round 65550\n",
      "Round 65600\n",
      "Round 65650\n",
      "Round 65700\n",
      "Round 65750\n",
      "Round 65800\n",
      "Round 65850\n",
      "Round 65900\n",
      "Round 65950\n",
      "Round 66000\n",
      "Round 66050\n",
      "Round 66100\n",
      "Round 66150\n",
      "Round 66200\n",
      "Round 66250\n",
      "Round 66300\n",
      "Round 66350\n",
      "Round 66400\n",
      "Round 66450\n",
      "Round 66500\n",
      "Round 66550\n",
      "Round 66600\n",
      "Round 66650\n",
      "Round 66700\n",
      "Round 66750\n",
      "Round 66800\n",
      "Round 66850\n",
      "Round 66900\n",
      "Round 66950\n",
      "Round 67000\n",
      "Round 67050\n",
      "Round 67100\n",
      "Round 67150\n",
      "Round 67200\n",
      "Round 67250\n",
      "Round 67300\n",
      "Round 67350\n",
      "Round 67400\n",
      "Round 67450\n",
      "Round 67500\n",
      "Round 67550\n",
      "Round 67600\n",
      "Round 67650\n",
      "Round 67700\n",
      "Round 67750\n",
      "Round 67800\n",
      "Round 67850\n",
      "Round 67900\n",
      "Round 67950\n",
      "Round 68000\n",
      "Round 68050\n",
      "Round 68100\n",
      "Round 68150\n",
      "Round 68200\n",
      "Round 68250\n",
      "Round 68300\n",
      "Round 68350\n",
      "Round 68400\n",
      "Round 68450\n",
      "Round 68500\n",
      "Round 68550\n",
      "Round 68600\n",
      "Round 68650\n",
      "Round 68700\n",
      "Round 68750\n",
      "Round 68800\n",
      "Round 68850\n",
      "Round 68900\n",
      "Round 68950\n",
      "Round 69000\n",
      "Round 69050\n",
      "Round 69100\n",
      "Round 69150\n",
      "Round 69200\n",
      "Round 69250\n",
      "Round 69300\n",
      "Round 69350\n",
      "Round 69400\n",
      "Round 69450\n",
      "Round 69500\n",
      "Round 69550\n",
      "Round 69600\n",
      "Round 69650\n",
      "Round 69700\n",
      "Round 69750\n",
      "Round 69800\n",
      "Round 69850\n",
      "Round 69900\n",
      "Round 69950\n",
      "Round 70000\n",
      "Round 70050\n",
      "Round 70100\n",
      "Round 70150\n",
      "Round 70200\n",
      "Round 70250\n",
      "Round 70300\n",
      "Round 70350\n",
      "Round 70400\n",
      "Round 70450\n",
      "Round 70500\n",
      "Round 70550\n",
      "Round 70600\n",
      "Round 70650\n",
      "Round 70700\n",
      "Round 70750\n",
      "Round 70800\n",
      "Round 70850\n",
      "Round 70900\n",
      "Round 70950\n",
      "Round 71000\n",
      "Round 71050\n",
      "Round 71100\n",
      "Round 71150\n",
      "Round 71200\n",
      "Round 71250\n",
      "Round 71300\n",
      "Round 71350\n",
      "Round 71400\n",
      "Round 71450\n",
      "Round 71500\n",
      "Round 71550\n",
      "Round 71600\n",
      "Round 71650\n",
      "Round 71700\n",
      "Round 71750\n",
      "Round 71800\n",
      "Round 71850\n",
      "Round 71900\n",
      "Round 71950\n",
      "Round 72000\n",
      "Round 72050\n",
      "Round 72100\n",
      "Round 72150\n",
      "Round 72200\n",
      "Round 72250\n",
      "Round 72300\n",
      "Round 72350\n",
      "Round 72400\n",
      "Round 72450\n",
      "Round 72500\n",
      "Round 72550\n",
      "Round 72600\n",
      "Round 72650\n",
      "Round 72700\n",
      "Round 72750\n",
      "Round 72800\n",
      "Round 72850\n",
      "Round 72900\n",
      "Round 72950\n",
      "Round 73000\n",
      "Round 73050\n",
      "Round 73100\n",
      "Round 73150\n",
      "Round 73200\n",
      "Round 73250\n",
      "Round 73300\n",
      "Round 73350\n",
      "Round 73400\n",
      "Round 73450\n",
      "Round 73500\n",
      "Round 73550\n",
      "Round 73600\n",
      "Round 73650\n",
      "Round 73700\n",
      "Round 73750\n",
      "Round 73800\n",
      "Round 73850\n",
      "Round 73900\n",
      "Round 73950\n",
      "Round 74000\n",
      "Round 74050\n",
      "Round 74100\n",
      "Round 74150\n",
      "Round 74200\n",
      "Round 74250\n",
      "Round 74300\n",
      "Round 74350\n",
      "Round 74400\n",
      "Round 74450\n",
      "Round 74500\n",
      "Round 74550\n",
      "Round 74600\n",
      "Round 74650\n",
      "Round 74700\n",
      "Round 74750\n",
      "Round 74800\n",
      "Round 74850\n",
      "Round 74900\n",
      "Round 74950\n",
      "Round 75000\n",
      "Round 75050\n",
      "Round 75100\n",
      "Round 75150\n",
      "Round 75200\n",
      "Round 75250\n",
      "Round 75300\n",
      "Round 75350\n",
      "Round 75400\n",
      "Round 75450\n",
      "Round 75500\n",
      "Round 75550\n",
      "Round 75600\n",
      "Round 75650\n",
      "Round 75700\n",
      "Round 75750\n",
      "Round 75800\n",
      "Round 75850\n",
      "Round 75900\n",
      "Round 75950\n",
      "Round 76000\n",
      "Round 76050\n",
      "Round 76100\n",
      "Round 76150\n",
      "Round 76200\n",
      "Round 76250\n",
      "Round 76300\n",
      "Round 76350\n",
      "Round 76400\n",
      "Round 76450\n",
      "Round 76500\n",
      "Round 76550\n",
      "Round 76600\n",
      "Round 76650\n",
      "Round 76700\n",
      "Round 76750\n",
      "Round 76800\n",
      "Round 76850\n",
      "Round 76900\n",
      "Round 76950\n",
      "Round 77000\n",
      "Round 77050\n",
      "Round 77100\n",
      "Round 77150\n",
      "Round 77200\n",
      "Round 77250\n",
      "Round 77300\n",
      "Round 77350\n",
      "Round 77400\n",
      "Round 77450\n",
      "Round 77500\n",
      "Round 77550\n",
      "Round 77600\n",
      "Round 77650\n",
      "Round 77700\n",
      "Round 77750\n",
      "Round 77800\n",
      "Round 77850\n",
      "Round 77900\n",
      "Round 77950\n",
      "Round 78000\n",
      "Round 78050\n",
      "Round 78100\n",
      "Round 78150\n",
      "Round 78200\n",
      "Round 78250\n",
      "Round 78300\n",
      "Round 78350\n",
      "Round 78400\n",
      "Round 78450\n",
      "Round 78500\n",
      "Round 78550\n",
      "Round 78600\n",
      "Round 78650\n",
      "Round 78700\n",
      "Round 78750\n",
      "Round 78800\n",
      "Round 78850\n",
      "Round 78900\n",
      "Round 78950\n",
      "Round 79000\n",
      "Round 79050\n",
      "Round 79100\n",
      "Round 79150\n",
      "Round 79200\n",
      "Round 79250\n",
      "Round 79300\n",
      "Round 79350\n",
      "Round 79400\n",
      "Round 79450\n",
      "Round 79500\n",
      "Round 79550\n",
      "Round 79600\n",
      "Round 79650\n",
      "Round 79700\n",
      "Round 79750\n",
      "Round 79800\n",
      "Round 79850\n",
      "Round 79900\n",
      "Round 79950\n",
      "Round 80000\n",
      "Round 80050\n",
      "Round 80100\n",
      "Round 80150\n",
      "Round 80200\n",
      "Round 80250\n",
      "Round 80300\n",
      "Round 80350\n",
      "Round 80400\n",
      "Round 80450\n",
      "Round 80500\n",
      "Round 80550\n",
      "Round 80600\n",
      "Round 80650\n",
      "Round 80700\n",
      "Round 80750\n",
      "Round 80800\n",
      "Round 80850\n",
      "Round 80900\n",
      "Round 80950\n",
      "Round 81000\n",
      "Round 81050\n",
      "Round 81100\n",
      "Round 81150\n",
      "Round 81200\n",
      "Round 81250\n",
      "Round 81300\n",
      "Round 81350\n",
      "Round 81400\n",
      "Round 81450\n",
      "Round 81500\n",
      "Round 81550\n",
      "Round 81600\n",
      "Round 81650\n",
      "Round 81700\n",
      "Round 81750\n",
      "Round 81800\n",
      "Round 81850\n",
      "Round 81900\n",
      "Round 81950\n",
      "Round 82000\n",
      "Round 82050\n",
      "Round 82100\n",
      "Round 82150\n",
      "Round 82200\n",
      "Round 82250\n",
      "Round 82300\n",
      "Round 82350\n",
      "Round 82400\n",
      "Round 82450\n",
      "Round 82500\n",
      "Round 82550\n",
      "Round 82600\n",
      "Round 82650\n",
      "Round 82700\n",
      "Round 82750\n",
      "Round 82800\n",
      "Round 82850\n",
      "Round 82900\n",
      "Round 82950\n",
      "Round 83000\n",
      "Round 83050\n",
      "Round 83100\n",
      "Round 83150\n",
      "Round 83200\n",
      "Round 83250\n",
      "Round 83300\n",
      "Round 83350\n",
      "Round 83400\n",
      "Round 83450\n",
      "Round 83500\n",
      "Round 83550\n",
      "Round 83600\n",
      "Round 83650\n",
      "Round 83700\n",
      "Round 83750\n",
      "Round 83800\n",
      "Round 83850\n",
      "Round 83900\n",
      "Round 83950\n",
      "Round 84000\n",
      "Round 84050\n",
      "Round 84100\n",
      "Round 84150\n",
      "Round 84200\n",
      "Round 84250\n",
      "Round 84300\n",
      "Round 84350\n",
      "Round 84400\n",
      "Round 84450\n",
      "Round 84500\n",
      "Round 84550\n",
      "Round 84600\n",
      "Round 84650\n",
      "Round 84700\n",
      "Round 84750\n",
      "Round 84800\n",
      "Round 84850\n",
      "Round 84900\n",
      "Round 84950\n",
      "Round 85000\n",
      "Round 85050\n",
      "Round 85100\n",
      "Round 85150\n",
      "Round 85200\n",
      "Round 85250\n",
      "Round 85300\n",
      "Round 85350\n",
      "Round 85400\n",
      "Round 85450\n",
      "Round 85500\n",
      "Round 85550\n",
      "Round 85600\n",
      "Round 85650\n",
      "Round 85700\n",
      "Round 85750\n",
      "Round 85800\n",
      "Round 85850\n",
      "Round 85900\n",
      "Round 85950\n",
      "Round 86000\n",
      "Round 86050\n",
      "Round 86100\n",
      "Round 86150\n",
      "Round 86200\n",
      "Round 86250\n",
      "Round 86300\n",
      "Round 86350\n",
      "Round 86400\n",
      "Round 86450\n",
      "Round 86500\n",
      "Round 86550\n",
      "Round 86600\n",
      "Round 86650\n",
      "Round 86700\n",
      "Round 86750\n",
      "Round 86800\n",
      "Round 86850\n",
      "Round 86900\n",
      "Round 86950\n",
      "Round 87000\n",
      "Round 87050\n",
      "Round 87100\n",
      "Round 87150\n",
      "Round 87200\n",
      "Round 87250\n",
      "Round 87300\n",
      "Round 87350\n",
      "Round 87400\n",
      "Round 87450\n",
      "Round 87500\n",
      "Round 87550\n",
      "Round 87600\n",
      "Round 87650\n",
      "Round 87700\n",
      "Round 87750\n",
      "Round 87800\n",
      "Round 87850\n",
      "Round 87900\n",
      "Round 87950\n",
      "Round 88000\n",
      "Round 88050\n",
      "Round 88100\n",
      "Round 88150\n",
      "Round 88200\n",
      "Round 88250\n",
      "Round 88300\n",
      "Round 88350\n",
      "Round 88400\n",
      "Round 88450\n",
      "Round 88500\n",
      "Round 88550\n",
      "Round 88600\n",
      "Round 88650\n",
      "Round 88700\n",
      "Round 88750\n",
      "Round 88800\n",
      "Round 88850\n",
      "Round 88900\n",
      "Round 88950\n",
      "Round 89000\n",
      "Round 89050\n",
      "Round 89100\n",
      "Round 89150\n",
      "Round 89200\n",
      "Round 89250\n",
      "Round 89300\n",
      "Round 89350\n",
      "Round 89400\n",
      "Round 89450\n",
      "Round 89500\n",
      "Round 89550\n",
      "Round 89600\n",
      "Round 89650\n",
      "Round 89700\n",
      "Round 89750\n",
      "Round 89800\n",
      "Round 89850\n",
      "Round 89900\n",
      "Round 89950\n",
      "Round 90000\n",
      "Round 90050\n",
      "Round 90100\n",
      "Round 90150\n",
      "Round 90200\n",
      "Round 90250\n",
      "Round 90300\n",
      "Round 90350\n",
      "Round 90400\n",
      "Round 90450\n",
      "Round 90500\n",
      "Round 90550\n",
      "Round 90600\n",
      "Round 90650\n",
      "Round 90700\n",
      "Round 90750\n",
      "Round 90800\n",
      "Round 90850\n",
      "Round 90900\n",
      "Round 90950\n",
      "Round 91000\n",
      "Round 91050\n",
      "Round 91100\n",
      "Round 91150\n",
      "Round 91200\n",
      "Round 91250\n",
      "Round 91300\n",
      "Round 91350\n",
      "Round 91400\n",
      "Round 91450\n",
      "Round 91500\n",
      "Round 91550\n",
      "Round 91600\n",
      "Round 91650\n",
      "Round 91700\n",
      "Round 91750\n",
      "Round 91800\n",
      "Round 91850\n",
      "Round 91900\n",
      "Round 91950\n",
      "Round 92000\n",
      "Round 92050\n",
      "Round 92100\n",
      "Round 92150\n",
      "Round 92200\n",
      "Round 92250\n",
      "Round 92300\n",
      "Round 92350\n",
      "Round 92400\n",
      "Round 92450\n",
      "Round 92500\n",
      "Round 92550\n",
      "Round 92600\n",
      "Round 92650\n",
      "Round 92700\n",
      "Round 92750\n",
      "Round 92800\n",
      "Round 92850\n",
      "Round 92900\n",
      "Round 92950\n",
      "Round 93000\n",
      "Round 93050\n",
      "Round 93100\n",
      "Round 93150\n",
      "Round 93200\n",
      "Round 93250\n",
      "Round 93300\n",
      "Round 93350\n",
      "Round 93400\n",
      "Round 93450\n",
      "Round 93500\n",
      "Round 93550\n",
      "Round 93600\n",
      "Round 93650\n",
      "Round 93700\n",
      "Round 93750\n",
      "Round 93800\n",
      "Round 93850\n",
      "Round 93900\n",
      "Round 93950\n",
      "Round 94000\n",
      "Round 94050\n",
      "Round 94100\n",
      "Round 94150\n",
      "Round 94200\n",
      "Round 94250\n",
      "Round 94300\n",
      "Round 94350\n",
      "Round 94400\n",
      "Round 94450\n",
      "Round 94500\n",
      "Round 94550\n",
      "Round 94600\n",
      "Round 94650\n",
      "Round 94700\n",
      "Round 94750\n",
      "Round 94800\n",
      "Round 94850\n",
      "Round 94900\n",
      "Round 94950\n",
      "Round 95000\n",
      "Round 95050\n",
      "Round 95100\n",
      "Round 95150\n",
      "Round 95200\n",
      "Round 95250\n",
      "Round 95300\n",
      "Round 95350\n",
      "Round 95400\n",
      "Round 95450\n",
      "Round 95500\n",
      "Round 95550\n",
      "Round 95600\n",
      "Round 95650\n",
      "Round 95700\n",
      "Round 95750\n",
      "Round 95800\n",
      "Round 95850\n",
      "Round 95900\n",
      "Round 95950\n",
      "Round 96000\n",
      "Round 96050\n",
      "Round 96100\n",
      "Round 96150\n",
      "Round 96200\n",
      "Round 96250\n",
      "Round 96300\n",
      "Round 96350\n",
      "Round 96400\n",
      "Round 96450\n",
      "Round 96500\n",
      "Round 96550\n",
      "Round 96600\n",
      "Round 96650\n",
      "Round 96700\n",
      "Round 96750\n",
      "Round 96800\n",
      "Round 96850\n",
      "Round 96900\n",
      "Round 96950\n",
      "Round 97000\n",
      "Round 97050\n",
      "Round 97100\n",
      "Round 97150\n",
      "Round 97200\n",
      "Round 97250\n",
      "Round 97300\n",
      "Round 97350\n",
      "Round 97400\n",
      "Round 97450\n",
      "Round 97500\n",
      "Round 97550\n",
      "Round 97600\n",
      "Round 97650\n",
      "Round 97700\n",
      "Round 97750\n",
      "Round 97800\n",
      "Round 97850\n",
      "Round 97900\n",
      "Round 97950\n",
      "Round 98000\n",
      "Round 98050\n",
      "Round 98100\n",
      "Round 98150\n",
      "Round 98200\n",
      "Round 98250\n",
      "Round 98300\n",
      "Round 98350\n",
      "Round 98400\n",
      "Round 98450\n",
      "Round 98500\n",
      "Round 98550\n",
      "Round 98600\n",
      "Round 98650\n",
      "Round 98700\n",
      "Round 98750\n",
      "Round 98800\n",
      "Round 98850\n",
      "Round 98900\n",
      "Round 98950\n",
      "Round 99000\n",
      "Round 99050\n",
      "Round 99100\n",
      "Round 99150\n",
      "Round 99200\n",
      "Round 99250\n",
      "Round 99300\n",
      "Round 99350\n",
      "Round 99400\n",
      "Round 99450\n",
      "Round 99500\n",
      "Round 99550\n",
      "Round 99600\n",
      "Round 99650\n",
      "Round 99700\n",
      "Round 99750\n",
      "Round 99800\n",
      "Round 99850\n",
      "Round 99900\n",
      "Round 99950\n",
      "Round 100000\n",
      "Round 100050\n",
      "Round 100100\n",
      "Round 100150\n",
      "Round 100200\n",
      "Round 100250\n",
      "Round 100300\n",
      "Round 100350\n",
      "Round 100400\n",
      "Round 100450\n",
      "Round 100500\n",
      "Round 100550\n",
      "Round 100600\n",
      "Round 100650\n",
      "Round 100700\n",
      "Round 100750\n",
      "Round 100800\n",
      "Round 100850\n",
      "Round 100900\n",
      "Round 100950\n",
      "Round 101000\n",
      "Round 101050\n",
      "Round 101100\n",
      "Round 101150\n",
      "Round 101200\n",
      "Round 101250\n",
      "Round 101300\n",
      "Round 101350\n",
      "Round 101400\n",
      "Round 101450\n",
      "Round 101500\n",
      "Round 101550\n",
      "Round 101600\n",
      "Round 101650\n",
      "Round 101700\n",
      "Round 101750\n",
      "Round 101800\n",
      "Round 101850\n",
      "Round 101900\n",
      "Round 101950\n",
      "Round 102000\n",
      "Round 102050\n",
      "Round 102100\n",
      "Round 102150\n",
      "Round 102200\n",
      "Round 102250\n",
      "Round 102300\n",
      "Round 102350\n",
      "Round 102400\n",
      "Round 102450\n",
      "Round 102500\n",
      "Round 102550\n",
      "Round 102600\n",
      "Round 102650\n",
      "Round 102700\n",
      "Round 102750\n",
      "Round 102800\n",
      "Round 102850\n",
      "Round 102900\n",
      "Round 102950\n",
      "Round 103000"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: InterruptException:\nwhile loading In[40], in expression starting on line 179",
     "output_type": "error",
     "traceback": [
      "LoadError: InterruptException:\nwhile loading In[40], in expression starting on line 179",
      "",
      " in find at array.jl:778",
      " in objective_pre_alpha at In[26]:14",
      " in obj_func at In[38]:22",
      " in finite_difference! at /Users/kuuranne/.julia/v0.4/Calculus/src/finite_difference.jl:128",
      " in g! at /Users/kuuranne/.julia/v0.4/Optim/src/types.jl:159",
      " in fg! at /Users/kuuranne/.julia/v0.4/Optim/src/types.jl:163",
      " in function_barrier at /Users/kuuranne/.julia/v0.4/Optim/src/fminbox.jl:58",
      " in anonymous at /Users/kuuranne/.julia/v0.4/Optim/src/fminbox.jl:130",
      " in barrier_combined at /Users/kuuranne/.julia/v0.4/Optim/src/fminbox.jl:67",
      " in anonymous at /Users/kuuranne/.julia/v0.4/Optim/src/fminbox.jl:167",
      " in linefunc! at /Users/kuuranne/.julia/v0.4/Optim/src/linesearch/hz_linesearch.jl:597",
      " in hz_linesearch! at /Users/kuuranne/.julia/v0.4/Optim/src/linesearch/hz_linesearch.jl:280",
      " in hz_linesearch! at /Users/kuuranne/.julia/v0.4/Optim/src/linesearch/hz_linesearch.jl:176 (repeats 10 times)",
      " in __cg#6__ at /Users/kuuranne/.julia/v0.4/Optim/src/cg.jl:221",
      " in fminbox at /Users/kuuranne/.julia/v0.4/Optim/src/fminbox.jl:174"
     ]
    }
   ],
   "source": [
    "count::Int = 0\n",
    "# # Call L-BFGS\n",
    "# res = optimize(obj_func,\n",
    "#     optVarInit,\n",
    "#     method = :l_bfgs,\n",
    "#     xtol = 1e-4,\n",
    "#     grtol = 1e-12,\n",
    "#     iterations = 1000,\n",
    "#     store_trace = true,\n",
    "#     show_trace = false)\n",
    "\n",
    "# TODO: Can we use upper and lower limits for 𝐕? Will it speed up the optimizer?\n",
    "#       The prototypes need to lie inside the smallest hypercube that contains all original datapoints, yes?\n",
    "\n",
    "d1 = DifferentiableFunction(obj_func)\n",
    "# # Note that d1 above will use central finite differencing to approximate the gradient.\n",
    "\n",
    "res = fminbox(d1,\n",
    "    optVarInit,\n",
    "    bounds_lower,\n",
    "    bounds_upper,\n",
    "    xtol = 1e-4,\n",
    "    grtol = 1e-12,\n",
    "    iterations = 100,\n",
    "    store_trace = true,\n",
    "    show_trace = false)\n",
    "# In tutorial: @elapsed fminbox(d4, x0, l, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#opt = Opt(:LD_MMA, 2)\n",
    "opt = Opt(:LN_SBPLX, length(optVarInit)) # Use a derivative-free optimization algorithm (instead of L-BFGS)\n",
    "\n",
    "# Lower and upper bounds for alphas and 𝐰\n",
    "lower_bounds!(opt, bounds_lower)\n",
    "upper_bounds!(opt, bounds_upper)\n",
    "\n",
    "# Tolerance\n",
    "xtol_rel!(opt, 1e-4)\n",
    "\n",
    "# Stop when the number of function evaluations exceeds the second argument.\n",
    "#(0 or negative for no limit.)\n",
    "maxeval!(opt, 200)\n",
    "\n",
    "# Stop when the optimization time (in seconds) exceeds the second argument.\n",
    "#(0 or negative for no limit.)\n",
    "maxtime!(opt, 60*2)\n",
    "\n",
    "# Minimize\n",
    "min_objective!(opt, obj_func)\n",
    "\n",
    "(minf,minx,ret) = optimize(opt, optVarInit)\n",
    "println(\"got $minf at $minx after $count iterations (returned $ret)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization, running the algorithm\n",
    "Hyperparameters for the objective function.\n",
    "In the paper they use grid search to find the parameters. The sets defined here are the same as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sets of hyperparameters as in paper, for grid search\n",
    "gridA = Dict(:z => Set([0.1, 0.5, 1.0, 5.0, 10.0]), :x => Set([0, 0.01]), :y => Set([0.1, 0.5, 1.0, 5.0, 10.0]))\n",
    "# An example of selected hyperparameters, for development\n",
    "A = Dict(:z => 0.01, :x => 0.5, :y => 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Overall process with pictures\n",
    "- Try Lx Ly Lz with Float64 matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems/Cons/Notes:\n",
    "- Nothing said in the paper about choosing the number of prototypes $K$\n",
    "- Let's say that there is a column/feature \"Religion\" in the dataset.\n",
    "- Now this paper says we can only say that \"Is a member of protected group\" or \"Is not a member of protected group\".\n",
    "- You have to decide what is the \"protected\" group, and what is the \"normal/non protected\" group. You have to decide based on some external criteria who are discriminated against and who are not.\n",
    "- Let's say we have a dataset with a feature \"Religion\" and we have 5 different religions represented.\n",
    "- Now we have to choose which ones are protected and which ones are not.\n",
    "- The problem of course is that some might be in general discriminated against more than others. There is not necessary even split between the different groups that are discriminated against.\n",
    "\n",
    "- What we would like to say is that \"Religion\" is a sensitive feature, and we should not infer _anything_ from it, regardless what it is.\n",
    "\n",
    "- Does running the algo multiple times help, changing the binary classification each time? Can we extend it so that $S \\in {1,...,C}$ where $C$ is the number of categories in the sensitive column.\n",
    "  - We can extend, just split $L_z$ to multiple cases and the optimization is done to all of them. There will be $c = \\frac{(C-1)C}{2} \\approx O(C^2)$ pairs. Whether this is computationally still feasible is another question. In the objective function $L_z$ is replaced by $A_{z_1} \\cdot L_{z_1} + A_{z_2} \\cdot L_{z_2} + \\dots + A_{z_c} \\cdot L_{z_c}$.\n",
    "\n",
    "- On the current case where $S \\in \\left\\{0,1\\right\\}$ once we have set for which rows $S=1$ and $S=0$, we can flip them around without changing anything. This is because we are using statistical parity. This means that from the algorithm's perspective saying that group0 is non-protected and groups 1..4 are protected is the same as saying group1 is protected and other non-protected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.2",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
